{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1RvQvUKyziRV0N6BaYZrxMUe2MT8faYJL",
      "authorship_tag": "ABX9TyPD+Gu3sMdX8UhKRY/37g4y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Victor-Manach/numerai/blob/main/numerai_mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q numerapi"
      ],
      "metadata": {
        "id": "_jyKOP6w9uJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --no-deps numerai-tools"
      ],
      "metadata": {
        "id": "p7QFmq9M9oMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Any, Literal, Optional, Tuple, Callable\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "import math\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from numerai_tools.scoring import numerai_corr, correlation_contribution\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import h5py\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 5)"
      ],
      "metadata": {
        "id": "967lWY0-Wj05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_drive = Path('/content/drive/My Drive')"
      ],
      "metadata": {
        "id": "A0WsbcEFWjdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"{device=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v63MVMfibQDK",
        "outputId": "cfca2b03-8e0b-4233-983b-83e919a5f730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device='cuda'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQQNsJzFWcG7"
      },
      "outputs": [],
      "source": [
        "DATA_VERSION = \"v5.0\"\n",
        "MODEL_NAME = \"MLP_WITH_INFERENCE_ADAPTER\"\n",
        "PATH_TO_DATA = path_to_drive / f\"numerai/data/{DATA_VERSION}\"\n",
        "SKIP_TRAINING = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_path = PATH_TO_DATA / \"features.json\"\n",
        "TRAIN_PATH = PATH_TO_DATA / \"train/train_v2.h5\"\n",
        "VALID_PATH = PATH_TO_DATA / \"valid/valid_v2.h5\"\n",
        "\n",
        "MODEL_PATH = path_to_drive / f\"numerai/models\""
      ],
      "metadata": {
        "id": "puFZg1nlWtsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_metadata = json.load(open(features_path))\n",
        "all_target_cols = feature_metadata[\"targets\"]\n",
        "feature_sets = feature_metadata[\"feature_sets\"]\n",
        "\n",
        "sm_feature_set = feature_sets[\"small\"]\n",
        "med_feature_set = feature_sets[\"medium\"]\n",
        "all_feature_set = feature_sets[\"all\"]\n",
        "\n",
        "ERA_COL = \"era\"\n",
        "MAIN_TARGET = \"target\"\n",
        "AUXILIARY_TARGETS = [\n",
        "  \"target_victor_20\",\n",
        "  \"target_xerxes_20\",\n",
        "  \"target_teager2b_20\",\n",
        "  \"target_caroline_20\",\n",
        "  \"target_sam_20\",\n",
        "  \"target_echo_20\",\n",
        "  \"target_alpha_20\",\n",
        "  \"target_jeremy_20\"\n",
        "]\n",
        "\n",
        "feature_set = med_feature_set"
      ],
      "metadata": {
        "id": "UqWtdRgZWz7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InfiniteDataIterator:\n",
        "    def __init__(self, dataloader, split):\n",
        "        self.dataloader = dataloader\n",
        "        self._iterator = iter(dataloader)\n",
        "        self.split = split\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        try:\n",
        "            return next(self._iterator)\n",
        "        except StopIteration:\n",
        "            print(f\"Restarting data loader ({self.split} split)...\")\n",
        "            self._iterator = iter(self.dataloader)\n",
        "            return next(self._iterator)\n",
        "\n",
        "    def reset(self):\n",
        "        print(f\"Resetting data loader ({self.split} split)...\")\n",
        "        self._iterator = iter(self.dataloader)"
      ],
      "metadata": {
        "id": "3QGbvvH1VFjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_features_id(features_to_use, h5_file):\n",
        "    feature_to_id_file = h5_file.parent / \"feature_to_id.json\"\n",
        "    with open(feature_to_id_file, 'r') as f:\n",
        "        feature_to_id = json.load(f)\n",
        "    features_id = [feature_to_id[f] for f in features_to_use]\n",
        "\n",
        "    return features_id\n",
        "\n",
        "def load_targets_id(targets_to_use, h5_file):\n",
        "    target_to_id_file = h5_file.parent / \"target_to_id.json\"\n",
        "    with open(target_to_id_file, 'r') as f:\n",
        "        target_to_id = json.load(f)\n",
        "    targets_id = [target_to_id[t] for t in targets_to_use]\n",
        "\n",
        "    return targets_id\n",
        "\n",
        "def load_target_id(target_to_use, h5_file):\n",
        "    target_to_id_file = h5_file.parent / \"target_to_id.json\"\n",
        "    with open(target_to_id_file, 'r') as f:\n",
        "        target_to_id = json.load(f)\n",
        "    target_id = target_to_id[target_to_use]\n",
        "\n",
        "    return target_id\n",
        "\n",
        "def load_eras_data(keep_eras, h5_file, target_to_use=None):\n",
        "    if target_to_use is not None:\n",
        "        target_id = load_target_id(target_to_use, h5_file)\n",
        "    else:\n",
        "        target_id = None\n",
        "    era_vector = []\n",
        "    with h5py.File(h5_file, 'r') as f:\n",
        "        era_ix = f['era_index']\n",
        "        idx_list = []\n",
        "        if keep_eras is not None:\n",
        "            for era in keep_eras:\n",
        "                era_key = str(era).zfill(4)\n",
        "                if era_key in era_ix:\n",
        "                    idx_list.append(era_ix[era_key][:])\n",
        "                    era_vector.append([era]*len(era_ix[era_key]))\n",
        "            if idx_list:\n",
        "                indices = np.concatenate(idx_list)\n",
        "            else:\n",
        "                indices = np.array([], dtype=np.int64)\n",
        "            features = f['features'][indices, :]\n",
        "        else:\n",
        "            features = f['features'][:]\n",
        "\n",
        "        if target_id is not None:\n",
        "            if keep_eras is not None:\n",
        "                targets = f['targets'][indices, target_id]\n",
        "            else:\n",
        "                targets = f['targets'][:, target_id]\n",
        "        else:\n",
        "            if keep_eras is not None:\n",
        "                targets = f['targets'][indices, :]\n",
        "            else:\n",
        "                targets = f['targets'][:]\n",
        "\n",
        "    era_vector = np.concatenate(era_vector)\n",
        "    return features, targets, era_vector\n",
        "\n",
        "def load_first_n_eras_data(n, h5_file, target_to_use=None):\n",
        "    if target_to_use is not None:\n",
        "        target_id = load_target_id(target_to_use, h5_file)\n",
        "    else:\n",
        "        target_id = None\n",
        "\n",
        "    era_vector = []\n",
        "    with h5py.File(h5_file, 'r') as f:\n",
        "        era_ix = f['era_index']\n",
        "        era_keys = sorted(list(era_ix.keys()))\n",
        "        first_n_keys = era_keys[:n]\n",
        "        idx_list = []\n",
        "        for era_key in first_n_keys:\n",
        "            idx_list.append(era_ix[era_key][:])\n",
        "            era_vector.append([int(era_key)]*len(era_ix[era_key]))\n",
        "        if idx_list:\n",
        "            indices = np.concatenate(idx_list)\n",
        "        else:\n",
        "            indices = np.array([], dtype=np.int64)\n",
        "        features = f['features'][indices, :]\n",
        "        if target_id is not None:\n",
        "            targets = f['targets'][indices, target_id]\n",
        "        else:\n",
        "            targets = f['targets'][indices, :]\n",
        "\n",
        "    era_vector = np.concatenate(era_vector)\n",
        "    return features, targets, era_vector, [int(e) for e in first_n_keys]\n",
        "\n",
        "\n",
        "def load_eras_data_and_filter_columns(keep_eras, h5_file, features_to_use, target_to_use=None):\n",
        "    features, targets, era_vector = load_eras_data(keep_eras, h5_file, target_to_use)\n",
        "    features_id = load_features_id(features_to_use, h5_file)\n",
        "\n",
        "    return features[:, features_id], targets, era_vector\n",
        "\n",
        "def load_first_n_eras_data_and_filter_columns(n, h5_file, features_to_use, target_to_use=None):\n",
        "    features, targets, era_vector, first_n_eras = load_first_n_eras_data(n, h5_file, target_to_use)\n",
        "    features_id = load_features_id(features_to_use, h5_file)\n",
        "\n",
        "    return features[:, features_id], targets, era_vector, first_n_eras"
      ],
      "metadata": {
        "id": "H9s3DqTG98OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_eras = [i for i in range(500, 600)]\n",
        "# train_eras = [i for i in range(500, 950)]\n",
        "x_train_np, y_train_np, train_era_vector_np = load_eras_data_and_filter_columns(train_eras, TRAIN_PATH, feature_set, None)"
      ],
      "metadata": {
        "id": "lBdEXYOEbKJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_target_id = load_target_id(MAIN_TARGET, TRAIN_PATH)\n",
        "aux_target_ids = load_targets_id(AUXILIARY_TARGETS, TRAIN_PATH)\n",
        "\n",
        "y_train_main_np = y_train_np[:, main_target_id]\n",
        "y_train_aux_np = y_train_np[:, aux_target_ids]"
      ],
      "metadata": {
        "id": "eXD4ybC6q5e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.tensor(x_train_np, dtype=torch.float32)\n",
        "y_train_main = torch.tensor(y_train_main_np, dtype=torch.float32)\n",
        "y_train_aux = torch.tensor(y_train_aux_np, dtype=torch.float32)\n",
        "train_era_vector_pt = torch.tensor(train_era_vector_np, dtype=torch.int32)\n",
        "\n",
        "x_train.shape, y_train_main.shape, y_train_aux.shape, train_era_vector_pt.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsQZ25oyXYAW",
        "outputId": "a2b92ff8-d40a-4c23-fdc0-48a9b5f92935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([536631, 705]),\n",
              " torch.Size([536631]),\n",
              " torch.Size([536631, 8]),\n",
              " torch.Size([536631]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_era_vector_pt.min(), train_era_vector_pt.max()"
      ],
      "metadata": {
        "id": "YUVjA9EbyDAm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5eb8b19-5b3a-4df6-eddd-d4e785d25cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(500, dtype=torch.int32), tensor(599, dtype=torch.int32))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = (x_train - 2.0) / 2.0"
      ],
      "metadata": {
        "id": "FXKCUuFvfggZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_main.unique(), y_train_aux.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ql7fZorjnugn",
        "outputId": "9c8ae6bb-2b7c-4e10-9a40-eac912ddd417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000]),\n",
              " tensor([0.0000, 0.2500, 0.5000,  ...,    nan,    nan,    nan]))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.float()\n",
        "y_train_main = y_train_main.float()\n",
        "y_train_aux = y_train_aux.float()"
      ],
      "metadata": {
        "id": "fjRC3n_9jaQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n = 150\n",
        "n = 50\n",
        "x_valid_np, y_valid_np, valid_era_vector_np, valid_eras = load_first_n_eras_data_and_filter_columns(n, VALID_PATH, feature_set, None)"
      ],
      "metadata": {
        "id": "lI8VQ3FcFwkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_target_id = load_target_id(MAIN_TARGET, VALID_PATH)\n",
        "aux_target_ids = load_targets_id(AUXILIARY_TARGETS, VALID_PATH)\n",
        "\n",
        "y_valid_main_np = y_valid_np[:, main_target_id]\n",
        "y_valid_aux_np = y_valid_np[:, aux_target_ids]"
      ],
      "metadata": {
        "id": "yvfthHJcrStf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_valid = torch.tensor(x_valid_np, dtype=torch.float32)\n",
        "y_valid_main = torch.tensor(y_valid_main_np, dtype=torch.float32)\n",
        "y_valid_aux = torch.tensor(y_valid_aux_np, dtype=torch.float32)\n",
        "valid_era_vector_pt = torch.tensor(valid_era_vector_np, dtype=torch.int32)\n",
        "\n",
        "x_valid.shape, y_valid_main.shape, y_valid_aux.shape, valid_era_vector_pt.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXFblsSvfGl4",
        "outputId": "5c240d65-0e8f-400c-9f43-d0a9e968e150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([340746, 705]),\n",
              " torch.Size([340746]),\n",
              " torch.Size([340746, 8]),\n",
              " torch.Size([340746]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_era_vector_pt.min(), valid_era_vector_pt.max()"
      ],
      "metadata": {
        "id": "qihADFF8yOVb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f17acdb-8df4-46ab-ef96-54f0a0b61ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(975, dtype=torch.int32), tensor(1024, dtype=torch.int32))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_valid = (x_valid - 2.0) / 2.0"
      ],
      "metadata": {
        "id": "oPed6Toxfmd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_valid_main.unique(), y_valid_aux.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmd8qK3FfHzQ",
        "outputId": "b531b547-46e3-4aae-8d27-e26060b41c40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000]),\n",
              " tensor([0.0000, 0.2500, 0.5000,  ...,    nan,    nan,    nan]))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_valid = x_valid.float()\n",
        "y_valid_main = y_valid_main.float()\n",
        "y_valid_aux = y_valid_aux.float()"
      ],
      "metadata": {
        "id": "_fG_qz48fJsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "9rkRxXF_yQol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdapterBlock(nn.Module):\n",
        "    def __init__(self, config, device=None, dtype=None):\n",
        "        super().__init__()\n",
        "\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        self.ln = nn.LayerNorm(config.n_embd, **factory_kwargs)\n",
        "        self.down_proj = nn.Linear(config.n_embd, config.adapter_dim, bias=config.bias, **factory_kwargs)\n",
        "        self.up_proj = nn.Linear(config.adapter_dim, config.n_embd, bias=config.bias, **factory_kwargs)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.ln(x)\n",
        "        x = self.down_proj(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.up_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MlpBlock(nn.Module):\n",
        "    def __init__(self, config, device=None, dtype=None):\n",
        "        super().__init__()\n",
        "\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        self.ln = nn.LayerNorm(config.n_embd, **factory_kwargs)\n",
        "        self.c_fc = nn.Linear(config.n_embd, config.n_embd, bias=config.bias, **factory_kwargs)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias, **factory_kwargs)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ln(x)\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MlpBackbone(nn.Module):\n",
        "    def __init__(self, config, device=None, dtype=None):\n",
        "        super().__init__()\n",
        "\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        self.embedding = nn.Linear(config.input_dim, config.n_embd, bias=True, **factory_kwargs)\n",
        "        self.layers = nn.ModuleList([MlpBlock(config, **factory_kwargs) for _ in range(config.n_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        for layer in self.layers:\n",
        "            x_ffn = layer(x)\n",
        "            x = x + x_ffn\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class MlpBlockWithAdapter(nn.Module):\n",
        "    def __init__(self, mlp_block:MlpBlock, config, device=None, dtype=None):\n",
        "        super().__init__()\n",
        "\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        self.base_block = mlp_block\n",
        "        self.adapter_block = AdapterBlock(config, **factory_kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.base_block(x)\n",
        "        x = x + self.adapter_block(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class MlpBackboneWithAdapter(nn.Module):\n",
        "    def __init__(self, mlp_backbone:MlpBackbone, config, device=None, dtype=None):\n",
        "        super().__init__()\n",
        "\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        for block in mlp_backbone.layers:\n",
        "            for name, param in block.named_parameters():\n",
        "                if \"ln\" not in name:\n",
        "                    param.requires_grad = False\n",
        "\n",
        "        self.embedding = mlp_backbone.embedding\n",
        "        # for param in self.embedding.parameters():\n",
        "        #     param.requires_grad = False\n",
        "\n",
        "        self.mlp_blocks_with_adapter = nn.ModuleList(\n",
        "            [MlpBlockWithAdapter(mlp_block, config, **factory_kwargs) for mlp_block in mlp_backbone.layers])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        for layer in self.mlp_blocks_with_adapter:\n",
        "            x = layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class ReconstructionHead(nn.Module):\n",
        "    def __init__(self, config, device=None, dtype=None):\n",
        "        super().__init__()\n",
        "\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        self.proj = nn.Linear(config.n_embd, config.input_dim, bias=True, **factory_kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.proj(x)\n",
        "\n",
        "class RegressionHead(nn.Module):\n",
        "    def __init__(self, config, device=None, dtype=None):\n",
        "        super().__init__()\n",
        "\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        self.main_proj = nn.Linear(config.n_embd, config.main_output_dim, bias=True, **factory_kwargs)\n",
        "        self.aux_proj = nn.Linear(config.n_embd, config.aux_output_dim, bias=True, **factory_kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main_proj(x), self.aux_proj(x)"
      ],
      "metadata": {
        "id": "cKEp06usg7su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MlpReconstructModel(nn.Module):\n",
        "    def __init__(self, config, device=None, dtype=None):\n",
        "        super().__init__()\n",
        "\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        self.backbone = MlpBackbone(config, **factory_kwargs)\n",
        "        self.head = ReconstructionHead(config, **factory_kwargs)\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, mask = self.apply_mask(x)\n",
        "        x = self.backbone(x)\n",
        "\n",
        "        return self.head(x), mask\n",
        "\n",
        "    def apply_mask(self, x):\n",
        "        mask = torch.rand_like(x) < self.config.mask_ratio\n",
        "        noise = torch.randn_like(x) * self.config.noise_std\n",
        "        x_masked = x.clone()\n",
        "        x_masked[mask] = noise[mask]\n",
        "        return x_masked, mask"
      ],
      "metadata": {
        "id": "bPuMIkyzFyO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MlpRegressionModel(nn.Module):\n",
        "    def __init__(self, config, device=None, dtype=None):\n",
        "        super().__init__()\n",
        "\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        self.backbone = MlpBackbone(config, **factory_kwargs)\n",
        "        self.head = RegressionHead(config, **factory_kwargs)\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "\n",
        "        return self.head(x)"
      ],
      "metadata": {
        "id": "DiVcf67vGL9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MlpReconstructWithAdapterModel(nn.Module):\n",
        "    def __init__(self, mlp_backbone:MlpBackbone, config, device=None, dtype=None):\n",
        "        super().__init__()\n",
        "\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        self.backbone_with_adapter = MlpBackboneWithAdapter(mlp_backbone, config, **factory_kwargs)\n",
        "        self.head = ReconstructionHead(config, **factory_kwargs)\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, mask = self.apply_mask(x)\n",
        "        x = self.backbone_with_adapter(x)\n",
        "\n",
        "        return self.head(x), mask\n",
        "\n",
        "    def apply_mask(self, x):\n",
        "        mask = torch.rand_like(x) < self.config.mask_ratio\n",
        "        noise = torch.randn_like(x) * self.config.noise_std\n",
        "        x_masked = x.clone()\n",
        "        x_masked[mask] = noise[mask]\n",
        "        return x_masked, mask"
      ],
      "metadata": {
        "id": "P909B6ReHyBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MlpRegressionWithAdapterModel(nn.Module):\n",
        "    def __init__(self, mlp_backbone:MlpBackboneWithAdapter, head, device=None, dtype=None):\n",
        "        super().__init__()\n",
        "\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        self.backbone_with_adapter = mlp_backbone\n",
        "        self.head = head\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone_with_adapter(x)\n",
        "\n",
        "        return self.head(x)"
      ],
      "metadata": {
        "id": "FKDTvx4HHoYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MlpReconstructWithAdapterAndConsistency(nn.Module):\n",
        "    def __init__(self,\n",
        "                 frozen_backbone: MlpBackbone,\n",
        "                 backbone_with_adapter: MlpBackboneWithAdapter,\n",
        "                 config,\n",
        "                 device=None,\n",
        "                 dtype=None):\n",
        "        super().__init__()\n",
        "\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        self.config = config\n",
        "\n",
        "        self.frozen_backbone = frozen_backbone\n",
        "        for param in self.frozen_backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.backbone_with_adapter = backbone_with_adapter\n",
        "        self.head = ReconstructionHead(config, **factory_kwargs)\n",
        "\n",
        "    def apply_mask(self, x):\n",
        "        mask = torch.rand_like(x) < self.config.mask_ratio\n",
        "        noise = torch.randn_like(x) * self.config.noise_std\n",
        "        x_masked = x.clone()\n",
        "        x_masked[mask] = noise[mask]\n",
        "        return x_masked, mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_masked, mask = self.apply_mask(x)\n",
        "        h_orig = self.frozen_backbone(x_masked)\n",
        "        h_adapt = self.backbone_with_adapter(x_masked)\n",
        "        recon = self.head(h_adapt)\n",
        "\n",
        "        return recon, mask, h_orig, h_adapt"
      ],
      "metadata": {
        "id": "E1ACxf3_Sfod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    input_dim: int\n",
        "    main_output_dim: int\n",
        "    aux_output_dim: int\n",
        "    n_layers: int\n",
        "    n_embd: int\n",
        "    adapter_dim: int\n",
        "    dropout: float\n",
        "    bias: bool\n",
        "    mask_ratio: float\n",
        "    noise_std: float\n",
        "\n",
        "@dataclass\n",
        "class MlpReconstructConfig:\n",
        "    input_dim: int\n",
        "    main_output_dim: int\n",
        "    aux_output_dim: int\n",
        "    n_layers: int\n",
        "    n_embd: int\n",
        "    dropout: float\n",
        "    bias: bool\n",
        "    mask_ratio: float\n",
        "    noise_std: float"
      ],
      "metadata": {
        "id": "T-Xdr4n9aPhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(MODEL_PATH/f\"MLP_RECONSTRUCT.pt\", map_location=device, weights_only=False)"
      ],
      "metadata": {
        "id": "CmKW9muLOIUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config(\n",
        "    input_dim=len(feature_set),\n",
        "    main_output_dim=1,\n",
        "    aux_output_dim=len(AUXILIARY_TARGETS),\n",
        "    n_layers=4,\n",
        "    n_embd=256,\n",
        "    adapter_dim=32,\n",
        "    dropout=0.0,\n",
        "    bias=True,\n",
        "    mask_ratio=0.4,\n",
        "    noise_std=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "1Nj_wBrUalqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "mjGb8tn1z98k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def configure_optimizer(model, weight_decay, learning_rate, betas, device_type, print_):\n",
        "    param_dict = {pn: p for pn, p in model.named_parameters() if p.requires_grad}\n",
        "    decay_params = [p for n, p in param_dict.items() if p.dim() >= 2] # all weight tensors in matmuls + embeddings decay\n",
        "    nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2] # all biases and layernorms\n",
        "\n",
        "    optim_groups = [\n",
        "        {'params': decay_params, 'weight_decay': weight_decay},\n",
        "        {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "    ]\n",
        "    num_decay_params = sum(p.numel() for p in decay_params)\n",
        "    num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "    if print_:\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "\n",
        "    use_fused = (device_type == 'cuda')\n",
        "    extra_args = dict(fused=True) if use_fused else dict()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "    if print_:\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "    return optimizer"
      ],
      "metadata": {
        "id": "FpNmK4E7G8Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 512\n",
        "total_batch_size = 721920\n",
        "assert total_batch_size % (batch_size*config.input_dim) == 0\n",
        "grad_accum_steps = total_batch_size // (batch_size*config.input_dim)\n",
        "print(f\"Total desired batch size: {total_batch_size}\")\n",
        "print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srEo7sJKcVO2",
        "outputId": "87191179-e9ee-4526-c5b9-95f74387142a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total desired batch size: 721920\n",
            "=> calculated gradient accumulation steps: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "backbone_ckpt = {k:v for (k,v) in checkpoint[\"model\"].items() if \"main_proj\" not in k}\n",
        "backbone_for_reg_model_ckpt = {f\"backbone.{k}\":v for (k,v) in backbone_ckpt.items()}\n",
        "\n",
        "reconstruct_head_ckpt = {k:v for (k,v) in checkpoint[\"model\"].items() if \"main_proj\" in k}\n",
        "reconstruct_head_ckpt = {\"head.\"+k.replace(\"main_proj\", \"proj\"): v for (k,v) in reconstruct_head_ckpt.items()}"
      ],
      "metadata": {
        "id": "oRrCVwCdOO8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_backbone = MlpBackbone(config).to(device)\n",
        "missing_keys, unexpected_keys = mlp_backbone.load_state_dict(backbone_ckpt, strict=False)\n",
        "\n",
        "print(\"Missing keys:\", missing_keys)\n",
        "print(\"Unexpected keys:\", unexpected_keys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVzStBuriTUE",
        "outputId": "755d008f-1274-4137-e69d-9ce3b62b7963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing keys: []\n",
            "Unexpected keys: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_model = MlpRegressionModel(config).to(device)\n",
        "missing_keys, unexpected_keys = reg_model.load_state_dict(backbone_for_reg_model_ckpt, strict=False)\n",
        "\n",
        "print(\"Missing keys:\", missing_keys)\n",
        "print(\"Unexpected keys:\", unexpected_keys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyYagCYbku65",
        "outputId": "a6d4ce13-b701-44ff-be07-30b44fec6441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing keys: ['head.main_proj.weight', 'head.main_proj.bias', 'head.aux_proj.weight', 'head.aux_proj.bias']\n",
            "Unexpected keys: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    n_params = sum(p.numel() for p in model.parameters())\n",
        "    n_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"[{model.__class__.__name__}] Number of trainable parameters: {n_params_trainable:,} out of {n_params:,} total parameters.\")"
      ],
      "metadata": {
        "id": "vi0WTnQ3cxzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_parameters(reg_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LiWR_NBvfhZ",
        "outputId": "abd50681-c33a-49d8-ee9f-f8957d316999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MlpRegressionModel] Number of trainable parameters: 711,433 out of 711,433 total parameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def display_pt_model_params(module: nn.Module):\n",
        "    print(\"\\nModel Parameters:\\n\")\n",
        "    print(f\"{'Layer':<100} {'# Parameters':>15} {'Shape':>20}\")\n",
        "    print(\"-\" * 150)\n",
        "    total_params = 0\n",
        "\n",
        "    for name, param in module.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            num_params = param.numel()\n",
        "            total_params += num_params\n",
        "            print(f\"{name:<100} {num_params:>15,} {str(tuple(param.shape)):>20}\")\n",
        "\n",
        "    print(\"-\" * 150)\n",
        "    print(f\"{'Total':<100} {total_params:>15,} params\\n\")"
      ],
      "metadata": {
        "id": "YiPXTUP7cUit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_pt_model_params(reg_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ8oqJGYcYaE",
        "outputId": "c365711c-5748-4e16-c0c5-4dbf44e56f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Parameters:\n",
            "\n",
            "Layer                                                                                                   # Parameters                Shape\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "backbone.embedding.weight                                                                                    180,480           (256, 705)\n",
            "backbone.embedding.bias                                                                                          256               (256,)\n",
            "backbone.layers.0.ln.weight                                                                                      256               (256,)\n",
            "backbone.layers.0.ln.bias                                                                                        256               (256,)\n",
            "backbone.layers.0.c_fc.weight                                                                                 65,536           (256, 256)\n",
            "backbone.layers.0.c_fc.bias                                                                                      256               (256,)\n",
            "backbone.layers.0.c_proj.weight                                                                               65,536           (256, 256)\n",
            "backbone.layers.0.c_proj.bias                                                                                    256               (256,)\n",
            "backbone.layers.1.ln.weight                                                                                      256               (256,)\n",
            "backbone.layers.1.ln.bias                                                                                        256               (256,)\n",
            "backbone.layers.1.c_fc.weight                                                                                 65,536           (256, 256)\n",
            "backbone.layers.1.c_fc.bias                                                                                      256               (256,)\n",
            "backbone.layers.1.c_proj.weight                                                                               65,536           (256, 256)\n",
            "backbone.layers.1.c_proj.bias                                                                                    256               (256,)\n",
            "backbone.layers.2.ln.weight                                                                                      256               (256,)\n",
            "backbone.layers.2.ln.bias                                                                                        256               (256,)\n",
            "backbone.layers.2.c_fc.weight                                                                                 65,536           (256, 256)\n",
            "backbone.layers.2.c_fc.bias                                                                                      256               (256,)\n",
            "backbone.layers.2.c_proj.weight                                                                               65,536           (256, 256)\n",
            "backbone.layers.2.c_proj.bias                                                                                    256               (256,)\n",
            "backbone.layers.3.ln.weight                                                                                      256               (256,)\n",
            "backbone.layers.3.ln.bias                                                                                        256               (256,)\n",
            "backbone.layers.3.c_fc.weight                                                                                 65,536           (256, 256)\n",
            "backbone.layers.3.c_fc.bias                                                                                      256               (256,)\n",
            "backbone.layers.3.c_proj.weight                                                                               65,536           (256, 256)\n",
            "backbone.layers.3.c_proj.bias                                                                                    256               (256,)\n",
            "head.main_proj.weight                                                                                            256             (1, 256)\n",
            "head.main_proj.bias                                                                                                1                 (1,)\n",
            "head.aux_proj.weight                                                                                           2,048             (8, 256)\n",
            "head.aux_proj.bias                                                                                                 8                 (8,)\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Total                                                                                                        711,433 params\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(x_train, y_train_main, y_train_aux, train_era_vector_pt)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)"
      ],
      "metadata": {
        "id": "7KtNMxNLb0rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataset = TensorDataset(x_valid, y_valid_main, y_valid_aux, valid_era_vector_pt)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)"
      ],
      "metadata": {
        "id": "tdifFg43k0fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader), len(valid_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDDTIy_gzZI5",
        "outputId": "139aa6b7-c352-4137-e996-4e9df8c93552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1049, 666)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 4000 # 9200\n",
        "val_loss_n_steps = 200\n",
        "learning_rate = 6e-4\n",
        "decay_lr = True\n",
        "warmup_iters = 400 # 400\n",
        "lr_decay_iters = max_steps # should be ~= max_iters\n",
        "min_lr = 6e-5 # should be ~= learning_rate/10\n",
        "weight_decay = 1e-1\n",
        "betas = (0.9, 0.999)\n",
        "epsilon = 1e-8\n",
        "gamma = 3.0\n",
        "n_steps_for_adapter = 600\n",
        "\n",
        "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "torch.cuda.manual_seed_all(1337)"
      ],
      "metadata": {
        "id": "5lBDjdCgcdU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_optimizer = configure_optimizer(reg_model, weight_decay, learning_rate, betas, device, print_=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWQ1-uAFcslH",
        "outputId": "9aaae530-b789-4a10-98c7-5c2bd5f1680a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 11, with 707,072 parameters\n",
            "num non-decayed parameter tensors: 19, with 4,361 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr(it, warmup_iters, learning_rate, lr_decay_iters, min_lr):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * (it + 1) / (warmup_iters + 1)\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "get_lr_with_default_values = lambda it: get_lr(it, warmup_iters, learning_rate, lr_decay_iters, min_lr)"
      ],
      "metadata": {
        "id": "YXrkMSHJcxIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_mse_loss(main_pred, aux_pred, y_main_true, y_aux_true):\n",
        "    main_pred = main_pred.view(-1)\n",
        "    y_main_true = y_main_true.view(-1)\n",
        "\n",
        "    w_main = 1.0 + gamma * torch.abs(y_main_true - 0.5) / 0.5\n",
        "    loss = w_main * (main_pred - y_main_true) ** 2\n",
        "\n",
        "    mask_aux = ~torch.isnan(y_aux_true)\n",
        "    y_aux_true = torch.where(mask_aux, y_aux_true, torch.tensor(0.0, dtype=y_aux_true.dtype, device=y_aux_true.device))\n",
        "    aux_loss = (((aux_pred - y_aux_true) ** 2) * mask_aux.float()).sum() / (mask_aux.float().sum() + epsilon)\n",
        "\n",
        "    return torch.mean(loss) + 0.5 * aux_loss"
      ],
      "metadata": {
        "id": "7Ru1XSWrm7tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_mse_loss(pred, target, mask):\n",
        "    return ((pred - target)**2 * mask.float()).sum() / mask.float().sum().clamp_min(epsilon)\n",
        "\n",
        "def masked_mse_loss_with_consistency(pred, target, mask, h_orig, h_adapt, lambda_):\n",
        "    consistency_loss = F.mse_loss(h_adapt, h_orig)\n",
        "    return masked_mse_loss(pred, target, mask) + lambda_ * consistency_loss"
      ],
      "metadata": {
        "id": "HzMIT38H4re8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def run_regression_eval_loop(model, valid_loader_iter, n_valid_steps, loss_fn):\n",
        "    model.eval()\n",
        "    valid_loader_iter.reset()\n",
        "    val_loss_accum = 0.0\n",
        "    for _ in range(n_valid_steps):\n",
        "        xb, yb_main, yb_aux, _ = next(valid_loader_iter)\n",
        "        xb, yb_main, yb_aux = xb.to(device), yb_main.to(device), yb_aux.to(device)\n",
        "        with torch.autocast(device_type=device, dtype=dtype):\n",
        "            main_logits, aux_logits = model(xb)\n",
        "            loss = loss_fn(main_logits, aux_logits, yb_main, yb_aux)\n",
        "        loss = loss / n_valid_steps\n",
        "        val_loss_accum += loss.detach()\n",
        "\n",
        "    return val_loss_accum"
      ],
      "metadata": {
        "id": "ONtipOYUwaQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_adapter_on_dataset(mlp_backbone, config, loader_iter, n_steps, grad_accum_steps, loss_fn):\n",
        "    backbone = MlpBackbone(config).to(device)\n",
        "    backbone.load_state_dict(mlp_backbone.state_dict())\n",
        "\n",
        "    backbone_with_adapter = MlpBackboneWithAdapter(backbone, config)\n",
        "    model = MlpReconstructWithAdapterAndConsistency(backbone, backbone_with_adapter, config).to(device)\n",
        "\n",
        "    # display_pt_model_params(model)\n",
        "\n",
        "    optimizer = configure_optimizer(model, weight_decay, learning_rate, betas, device, print_=False)\n",
        "\n",
        "    model.train(True)\n",
        "    loader_iter.reset()\n",
        "\n",
        "    for step in range(n_steps):\n",
        "        last_step = (step == n_steps - 1)\n",
        "        t0 = time.time()\n",
        "        optimizer.zero_grad()\n",
        "        loss_accum = 0.0\n",
        "        for micro_step in range(grad_accum_steps):\n",
        "            xb, _, _, _ = next(loader_iter)\n",
        "            xb = xb.to(device)\n",
        "\n",
        "            with torch.autocast(device_type=device, dtype=dtype):\n",
        "                xb_hat, mask, h_orig, h_adapt = model(xb)\n",
        "                loss = loss_fn(xb_hat, xb, mask, h_orig, h_adapt, lambda_=0.4)\n",
        "\n",
        "            loss = loss / grad_accum_steps\n",
        "            loss_accum += loss.detach()\n",
        "            loss.backward()\n",
        "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        t1 = time.time()\n",
        "        dt = t1 - t0\n",
        "        if step % 10 == 0 or last_step:\n",
        "            print(f\"[{model.__class__.__name__}] step {step+1:3d}/{n_steps:3d} | loss={loss_accum.item():.6f} | lr={learning_rate:.6e} | norm={norm:.4f} | dt={dt*1000:.2f}ms\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "QmJEgIhR3kFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_eval_loop_with_adapter(model, valid_loader_iter, config, n_steps, n_valid_steps):\n",
        "    adapter_model = train_adapter_on_dataset(model.backbone, config, valid_loader_iter, n_steps, 1, masked_mse_loss_with_consistency)\n",
        "    reg_model_with_adapter = MlpRegressionWithAdapterModel(adapter_model.backbone_with_adapter, model.head).to(device)\n",
        "    val_loss = run_regression_eval_loop(reg_model_with_adapter, valid_loader_iter, n_valid_steps, weighted_mse_loss)\n",
        "    return reg_model_with_adapter, val_loss"
      ],
      "metadata": {
        "id": "Sk-zhGk76TG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_train_step(model, optimizer, train_loader_iter, step, loss_fn):\n",
        "    model.train(True)\n",
        "    lr = get_lr_with_default_values(step) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss_accum = 0.0\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "        xb, yb_main, yb_aux, _ = next(train_loader_iter)\n",
        "        xb, yb_main, yb_aux = xb.to(device), yb_main.to(device), yb_aux.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=dtype):\n",
        "            main_logits, aux_logits = model(xb)\n",
        "            loss = loss_fn(main_logits, aux_logits, yb_main, yb_aux)\n",
        "\n",
        "        loss = loss / grad_accum_steps\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss_accum, norm, lr"
      ],
      "metadata": {
        "id": "LBY9AT0kw5xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, valid_losses, valid_losses_with_adapter = [], [], []\n",
        "\n",
        "train_loader_iter = InfiniteDataIterator(train_loader, \"train\")\n",
        "valid_loader_iter = InfiniteDataIterator(valid_loader, \"valid\")\n",
        "n_valid_steps = len(valid_loader)\n",
        "for step in range(max_steps):\n",
        "    t0 = time.time()\n",
        "    last_step = (step == max_steps - 1)\n",
        "\n",
        "    if step % val_loss_n_steps == 0 or last_step:\n",
        "        val_loss = run_regression_eval_loop(reg_model, valid_loader_iter, n_valid_steps, weighted_mse_loss)\n",
        "        print(f\"[{reg_model.__class__.__name__}] step {step+1:3d}/{max_steps:3d} | val_loss={val_loss.item():.4f}\")\n",
        "        valid_losses.append(val_loss.item())\n",
        "\n",
        "        reg_model_with_adapter, val_loss_with_adapter = run_eval_loop_with_adapter(reg_model, valid_loader_iter, config, n_steps_for_adapter, n_valid_steps)\n",
        "        print(f\"[{reg_model_with_adapter.__class__.__name__}] step {step+1:3d}/{max_steps:3d} | val_loss={val_loss_with_adapter.item():.4f}\")\n",
        "        valid_losses_with_adapter.append(val_loss_with_adapter.item())\n",
        "\n",
        "    loss_accum, norm, lr = run_train_step(reg_model, reg_optimizer, train_loader_iter, step, weighted_mse_loss)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    if step % 50 == 0 or last_step:\n",
        "        print(f\"[{reg_model.__class__.__name__}] step {step+1:3d}/{max_steps:3d} | loss={loss_accum.item():.6f} | lr={lr:.6e} | norm={norm:.4f} | dt={dt*1000:.2f}ms\")\n",
        "\n",
        "    train_losses.append(loss_accum.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJuzpUlKuZCL",
        "outputId": "7c909206-e263-4784-a432-4d93f7e0b3ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionModel] step   1/4000 | val_loss=1.1463\n",
            "Resetting data loader (valid split)...\n",
            "[MlpReconstructWithAdapterAndConsistency] step   1/600 | loss=0.637228 | lr=6.000000e-04 | norm=0.1308 | dt=482.41ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  11/600 | loss=0.512301 | lr=6.000000e-04 | norm=0.0632 | dt=11.80ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  21/600 | loss=0.437016 | lr=6.000000e-04 | norm=0.0520 | dt=9.14ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  31/600 | loss=0.389662 | lr=6.000000e-04 | norm=0.0431 | dt=8.92ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  41/600 | loss=0.356932 | lr=6.000000e-04 | norm=0.0376 | dt=7.53ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  51/600 | loss=0.328232 | lr=6.000000e-04 | norm=0.0340 | dt=10.76ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  61/600 | loss=0.313615 | lr=6.000000e-04 | norm=0.0324 | dt=9.40ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  71/600 | loss=0.301137 | lr=6.000000e-04 | norm=0.0305 | dt=8.97ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  81/600 | loss=0.287033 | lr=6.000000e-04 | norm=0.0293 | dt=10.17ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  91/600 | loss=0.277644 | lr=6.000000e-04 | norm=0.0285 | dt=10.24ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 101/600 | loss=0.271537 | lr=6.000000e-04 | norm=0.0278 | dt=9.62ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 111/600 | loss=0.265319 | lr=6.000000e-04 | norm=0.0271 | dt=11.56ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 121/600 | loss=0.254112 | lr=6.000000e-04 | norm=0.0267 | dt=10.36ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 131/600 | loss=0.252478 | lr=6.000000e-04 | norm=0.0261 | dt=11.58ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 141/600 | loss=0.247566 | lr=6.000000e-04 | norm=0.0256 | dt=9.05ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 151/600 | loss=0.242241 | lr=6.000000e-04 | norm=0.0263 | dt=9.41ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 161/600 | loss=0.237261 | lr=6.000000e-04 | norm=0.0254 | dt=10.35ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 171/600 | loss=0.232296 | lr=6.000000e-04 | norm=0.0249 | dt=8.28ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 181/600 | loss=0.229422 | lr=6.000000e-04 | norm=0.0247 | dt=10.78ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 191/600 | loss=0.226807 | lr=6.000000e-04 | norm=0.0246 | dt=10.62ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 201/600 | loss=0.225749 | lr=6.000000e-04 | norm=0.0244 | dt=10.00ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 211/600 | loss=0.221826 | lr=6.000000e-04 | norm=0.0237 | dt=11.00ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 221/600 | loss=0.220038 | lr=6.000000e-04 | norm=0.0244 | dt=9.82ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 231/600 | loss=0.217619 | lr=6.000000e-04 | norm=0.0239 | dt=11.02ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 241/600 | loss=0.215396 | lr=6.000000e-04 | norm=0.0237 | dt=8.04ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 251/600 | loss=0.215175 | lr=6.000000e-04 | norm=0.0239 | dt=8.03ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 261/600 | loss=0.213728 | lr=6.000000e-04 | norm=0.0240 | dt=9.45ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 271/600 | loss=0.209375 | lr=6.000000e-04 | norm=0.0232 | dt=8.68ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 281/600 | loss=0.206013 | lr=6.000000e-04 | norm=0.0232 | dt=11.06ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 291/600 | loss=0.207941 | lr=6.000000e-04 | norm=0.0233 | dt=8.61ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 301/600 | loss=0.210044 | lr=6.000000e-04 | norm=0.0234 | dt=9.77ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 311/600 | loss=0.204567 | lr=6.000000e-04 | norm=0.0230 | dt=8.33ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 321/600 | loss=0.205245 | lr=6.000000e-04 | norm=0.0230 | dt=9.94ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 331/600 | loss=0.204971 | lr=6.000000e-04 | norm=0.0231 | dt=10.16ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 341/600 | loss=0.203071 | lr=6.000000e-04 | norm=0.0232 | dt=9.50ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 351/600 | loss=0.198955 | lr=6.000000e-04 | norm=0.0226 | dt=8.88ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 361/600 | loss=0.198211 | lr=6.000000e-04 | norm=0.0228 | dt=9.90ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 371/600 | loss=0.193139 | lr=6.000000e-04 | norm=0.0221 | dt=8.57ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 381/600 | loss=0.197961 | lr=6.000000e-04 | norm=0.0223 | dt=9.16ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 391/600 | loss=0.191738 | lr=6.000000e-04 | norm=0.0217 | dt=7.62ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 401/600 | loss=0.195810 | lr=6.000000e-04 | norm=0.0226 | dt=9.26ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 411/600 | loss=0.191531 | lr=6.000000e-04 | norm=0.0219 | dt=9.61ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 421/600 | loss=0.190968 | lr=6.000000e-04 | norm=0.0222 | dt=8.69ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 431/600 | loss=0.189327 | lr=6.000000e-04 | norm=0.0224 | dt=8.81ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 441/600 | loss=0.189968 | lr=6.000000e-04 | norm=0.0221 | dt=7.90ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 451/600 | loss=0.187370 | lr=6.000000e-04 | norm=0.0217 | dt=8.46ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 461/600 | loss=0.187112 | lr=6.000000e-04 | norm=0.0216 | dt=8.22ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 471/600 | loss=0.188630 | lr=6.000000e-04 | norm=0.0221 | dt=10.17ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 481/600 | loss=0.184348 | lr=6.000000e-04 | norm=0.0213 | dt=8.71ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 491/600 | loss=0.187381 | lr=6.000000e-04 | norm=0.0220 | dt=11.48ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 501/600 | loss=0.187844 | lr=6.000000e-04 | norm=0.0221 | dt=10.00ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 511/600 | loss=0.183544 | lr=6.000000e-04 | norm=0.0219 | dt=8.92ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 521/600 | loss=0.182838 | lr=6.000000e-04 | norm=0.0218 | dt=8.22ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 531/600 | loss=0.182882 | lr=6.000000e-04 | norm=0.0216 | dt=10.73ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 541/600 | loss=0.182173 | lr=6.000000e-04 | norm=0.0215 | dt=10.24ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 551/600 | loss=0.181586 | lr=6.000000e-04 | norm=0.0217 | dt=9.98ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 561/600 | loss=0.182608 | lr=6.000000e-04 | norm=0.0218 | dt=10.92ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 571/600 | loss=0.183818 | lr=6.000000e-04 | norm=0.0221 | dt=9.38ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 581/600 | loss=0.180275 | lr=6.000000e-04 | norm=0.0216 | dt=9.82ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 591/600 | loss=0.182381 | lr=6.000000e-04 | norm=0.0217 | dt=9.54ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 600/600 | loss=0.184003 | lr=6.000000e-04 | norm=0.0220 | dt=9.31ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionWithAdapterModel] step   1/4000 | val_loss=1.1544\n",
            "[MlpRegressionModel] step   1/4000 | loss=1.070074 | lr=1.496259e-06 | norm=6.6810 | dt=15116.80ms\n",
            "[MlpRegressionModel] step  51/4000 | loss=0.263114 | lr=7.630923e-05 | norm=0.8620 | dt=17.91ms\n",
            "[MlpRegressionModel] step 101/4000 | loss=0.200595 | lr=1.511222e-04 | norm=1.0775 | dt=15.37ms\n",
            "[MlpRegressionModel] step 151/4000 | loss=0.200571 | lr=2.259352e-04 | norm=1.1489 | dt=15.74ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionModel] step 201/4000 | val_loss=0.2009\n",
            "Resetting data loader (valid split)...\n",
            "[MlpReconstructWithAdapterAndConsistency] step   1/600 | loss=0.636718 | lr=6.000000e-04 | norm=0.1397 | dt=17.32ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  11/600 | loss=0.515190 | lr=6.000000e-04 | norm=0.0615 | dt=11.28ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  21/600 | loss=0.442954 | lr=6.000000e-04 | norm=0.0516 | dt=10.16ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  31/600 | loss=0.394959 | lr=6.000000e-04 | norm=0.0435 | dt=8.49ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  41/600 | loss=0.362023 | lr=6.000000e-04 | norm=0.0388 | dt=8.36ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  51/600 | loss=0.333756 | lr=6.000000e-04 | norm=0.0345 | dt=8.55ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  61/600 | loss=0.316258 | lr=6.000000e-04 | norm=0.0325 | dt=12.14ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  71/600 | loss=0.305894 | lr=6.000000e-04 | norm=0.0316 | dt=10.36ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  81/600 | loss=0.292077 | lr=6.000000e-04 | norm=0.0303 | dt=9.00ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  91/600 | loss=0.283964 | lr=6.000000e-04 | norm=0.0294 | dt=10.29ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 101/600 | loss=0.274094 | lr=6.000000e-04 | norm=0.0283 | dt=9.72ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 111/600 | loss=0.269253 | lr=6.000000e-04 | norm=0.0283 | dt=13.16ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 121/600 | loss=0.259012 | lr=6.000000e-04 | norm=0.0276 | dt=11.11ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 131/600 | loss=0.256832 | lr=6.000000e-04 | norm=0.0275 | dt=8.50ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 141/600 | loss=0.251827 | lr=6.000000e-04 | norm=0.0272 | dt=9.34ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 151/600 | loss=0.246271 | lr=6.000000e-04 | norm=0.0261 | dt=10.83ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 161/600 | loss=0.241482 | lr=6.000000e-04 | norm=0.0264 | dt=9.96ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 171/600 | loss=0.238390 | lr=6.000000e-04 | norm=0.0261 | dt=10.16ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 181/600 | loss=0.235731 | lr=6.000000e-04 | norm=0.0256 | dt=10.32ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 191/600 | loss=0.231781 | lr=6.000000e-04 | norm=0.0258 | dt=10.29ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 201/600 | loss=0.229605 | lr=6.000000e-04 | norm=0.0251 | dt=8.63ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 211/600 | loss=0.227582 | lr=6.000000e-04 | norm=0.0250 | dt=8.40ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 221/600 | loss=0.226393 | lr=6.000000e-04 | norm=0.0246 | dt=9.65ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 231/600 | loss=0.221430 | lr=6.000000e-04 | norm=0.0245 | dt=8.82ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 241/600 | loss=0.219387 | lr=6.000000e-04 | norm=0.0248 | dt=8.52ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 251/600 | loss=0.220521 | lr=6.000000e-04 | norm=0.0247 | dt=11.08ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 261/600 | loss=0.218894 | lr=6.000000e-04 | norm=0.0253 | dt=9.17ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 271/600 | loss=0.215343 | lr=6.000000e-04 | norm=0.0251 | dt=9.25ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 281/600 | loss=0.210923 | lr=6.000000e-04 | norm=0.0247 | dt=9.46ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 291/600 | loss=0.211061 | lr=6.000000e-04 | norm=0.0241 | dt=9.25ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 301/600 | loss=0.213035 | lr=6.000000e-04 | norm=0.0241 | dt=9.53ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 311/600 | loss=0.208187 | lr=6.000000e-04 | norm=0.0241 | dt=7.94ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 321/600 | loss=0.209987 | lr=6.000000e-04 | norm=0.0251 | dt=10.70ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 331/600 | loss=0.209828 | lr=6.000000e-04 | norm=0.0239 | dt=10.62ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 341/600 | loss=0.208221 | lr=6.000000e-04 | norm=0.0245 | dt=8.48ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 351/600 | loss=0.203304 | lr=6.000000e-04 | norm=0.0237 | dt=10.98ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 361/600 | loss=0.204330 | lr=6.000000e-04 | norm=0.0235 | dt=9.81ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 371/600 | loss=0.198230 | lr=6.000000e-04 | norm=0.0238 | dt=9.58ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 381/600 | loss=0.201626 | lr=6.000000e-04 | norm=0.0247 | dt=9.90ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 391/600 | loss=0.200729 | lr=6.000000e-04 | norm=0.0235 | dt=8.32ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 401/600 | loss=0.198580 | lr=6.000000e-04 | norm=0.0236 | dt=11.27ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 411/600 | loss=0.193836 | lr=6.000000e-04 | norm=0.0233 | dt=10.11ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 421/600 | loss=0.195711 | lr=6.000000e-04 | norm=0.0233 | dt=8.15ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 431/600 | loss=0.192126 | lr=6.000000e-04 | norm=0.0232 | dt=9.70ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 441/600 | loss=0.192256 | lr=6.000000e-04 | norm=0.0234 | dt=10.47ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 451/600 | loss=0.191500 | lr=6.000000e-04 | norm=0.0239 | dt=10.49ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 461/600 | loss=0.190935 | lr=6.000000e-04 | norm=0.0230 | dt=10.52ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 471/600 | loss=0.192233 | lr=6.000000e-04 | norm=0.0228 | dt=9.70ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 481/600 | loss=0.188131 | lr=6.000000e-04 | norm=0.0236 | dt=9.63ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 491/600 | loss=0.191890 | lr=6.000000e-04 | norm=0.0234 | dt=8.28ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 501/600 | loss=0.191084 | lr=6.000000e-04 | norm=0.0233 | dt=9.78ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 511/600 | loss=0.188242 | lr=6.000000e-04 | norm=0.0232 | dt=8.12ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 521/600 | loss=0.188570 | lr=6.000000e-04 | norm=0.0232 | dt=9.20ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 531/600 | loss=0.188069 | lr=6.000000e-04 | norm=0.0230 | dt=9.96ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 541/600 | loss=0.185233 | lr=6.000000e-04 | norm=0.0223 | dt=10.63ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 551/600 | loss=0.187006 | lr=6.000000e-04 | norm=0.0230 | dt=8.58ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 561/600 | loss=0.186571 | lr=6.000000e-04 | norm=0.0230 | dt=10.29ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 571/600 | loss=0.189053 | lr=6.000000e-04 | norm=0.0237 | dt=10.51ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 581/600 | loss=0.185674 | lr=6.000000e-04 | norm=0.0231 | dt=10.61ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 591/600 | loss=0.186761 | lr=6.000000e-04 | norm=0.0234 | dt=7.79ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 600/600 | loss=0.186830 | lr=6.000000e-04 | norm=0.0233 | dt=10.26ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionWithAdapterModel] step 201/4000 | val_loss=0.2028\n",
            "[MlpRegressionModel] step 201/4000 | loss=0.212911 | lr=3.007481e-04 | norm=0.9701 | dt=13200.79ms\n",
            "[MlpRegressionModel] step 251/4000 | loss=0.190931 | lr=3.755611e-04 | norm=0.8348 | dt=20.38ms\n",
            "[MlpRegressionModel] step 301/4000 | loss=0.196089 | lr=4.503741e-04 | norm=0.7504 | dt=18.07ms\n",
            "[MlpRegressionModel] step 351/4000 | loss=0.201215 | lr=5.251870e-04 | norm=0.7861 | dt=17.04ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionModel] step 401/4000 | val_loss=0.1984\n",
            "Resetting data loader (valid split)...\n",
            "[MlpReconstructWithAdapterAndConsistency] step   1/600 | loss=0.627981 | lr=6.000000e-04 | norm=0.1522 | dt=15.69ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  11/600 | loss=0.514230 | lr=6.000000e-04 | norm=0.0568 | dt=10.27ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  21/600 | loss=0.452268 | lr=6.000000e-04 | norm=0.0488 | dt=9.19ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  31/600 | loss=0.406173 | lr=6.000000e-04 | norm=0.0429 | dt=8.12ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  41/600 | loss=0.370199 | lr=6.000000e-04 | norm=0.0382 | dt=11.29ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  51/600 | loss=0.341804 | lr=6.000000e-04 | norm=0.0354 | dt=11.72ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  61/600 | loss=0.324979 | lr=6.000000e-04 | norm=0.0337 | dt=8.04ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  71/600 | loss=0.314354 | lr=6.000000e-04 | norm=0.0335 | dt=9.05ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  81/600 | loss=0.299972 | lr=6.000000e-04 | norm=0.0304 | dt=8.56ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  91/600 | loss=0.288427 | lr=6.000000e-04 | norm=0.0291 | dt=8.83ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 101/600 | loss=0.281845 | lr=6.000000e-04 | norm=0.0286 | dt=8.53ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 111/600 | loss=0.276896 | lr=6.000000e-04 | norm=0.0289 | dt=8.19ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 121/600 | loss=0.265706 | lr=6.000000e-04 | norm=0.0283 | dt=9.85ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 131/600 | loss=0.264490 | lr=6.000000e-04 | norm=0.0287 | dt=11.98ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 141/600 | loss=0.259897 | lr=6.000000e-04 | norm=0.0295 | dt=10.59ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 151/600 | loss=0.256011 | lr=6.000000e-04 | norm=0.0281 | dt=12.36ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 161/600 | loss=0.248214 | lr=6.000000e-04 | norm=0.0264 | dt=9.97ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 171/600 | loss=0.245589 | lr=6.000000e-04 | norm=0.0275 | dt=9.43ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 181/600 | loss=0.241840 | lr=6.000000e-04 | norm=0.0276 | dt=8.21ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 191/600 | loss=0.238406 | lr=6.000000e-04 | norm=0.0259 | dt=8.18ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 201/600 | loss=0.235198 | lr=6.000000e-04 | norm=0.0257 | dt=12.16ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 211/600 | loss=0.234388 | lr=6.000000e-04 | norm=0.0264 | dt=8.06ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 221/600 | loss=0.231516 | lr=6.000000e-04 | norm=0.0254 | dt=9.25ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 231/600 | loss=0.229534 | lr=6.000000e-04 | norm=0.0263 | dt=8.27ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 241/600 | loss=0.224487 | lr=6.000000e-04 | norm=0.0272 | dt=12.38ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 251/600 | loss=0.227304 | lr=6.000000e-04 | norm=0.0265 | dt=10.12ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 261/600 | loss=0.224616 | lr=6.000000e-04 | norm=0.0272 | dt=10.28ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 271/600 | loss=0.221701 | lr=6.000000e-04 | norm=0.0256 | dt=8.95ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 281/600 | loss=0.216496 | lr=6.000000e-04 | norm=0.0254 | dt=10.52ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 291/600 | loss=0.220314 | lr=6.000000e-04 | norm=0.0262 | dt=11.19ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 301/600 | loss=0.218886 | lr=6.000000e-04 | norm=0.0260 | dt=12.26ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 311/600 | loss=0.216501 | lr=6.000000e-04 | norm=0.0262 | dt=11.69ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 321/600 | loss=0.214810 | lr=6.000000e-04 | norm=0.0256 | dt=10.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 331/600 | loss=0.215891 | lr=6.000000e-04 | norm=0.0258 | dt=9.81ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 341/600 | loss=0.216133 | lr=6.000000e-04 | norm=0.0265 | dt=13.67ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 351/600 | loss=0.209717 | lr=6.000000e-04 | norm=0.0257 | dt=9.56ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 361/600 | loss=0.209136 | lr=6.000000e-04 | norm=0.0247 | dt=8.93ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 371/600 | loss=0.203693 | lr=6.000000e-04 | norm=0.0269 | dt=10.51ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 381/600 | loss=0.207613 | lr=6.000000e-04 | norm=0.0254 | dt=11.99ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 391/600 | loss=0.203689 | lr=6.000000e-04 | norm=0.0253 | dt=7.79ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 401/600 | loss=0.205205 | lr=6.000000e-04 | norm=0.0245 | dt=9.29ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 411/600 | loss=0.198957 | lr=6.000000e-04 | norm=0.0241 | dt=11.56ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 421/600 | loss=0.200638 | lr=6.000000e-04 | norm=0.0248 | dt=8.30ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 431/600 | loss=0.198828 | lr=6.000000e-04 | norm=0.0247 | dt=10.79ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 441/600 | loss=0.197730 | lr=6.000000e-04 | norm=0.0248 | dt=10.17ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 451/600 | loss=0.198073 | lr=6.000000e-04 | norm=0.0244 | dt=9.59ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 461/600 | loss=0.195580 | lr=6.000000e-04 | norm=0.0242 | dt=10.53ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 471/600 | loss=0.197228 | lr=6.000000e-04 | norm=0.0238 | dt=9.38ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 481/600 | loss=0.194052 | lr=6.000000e-04 | norm=0.0242 | dt=14.07ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 491/600 | loss=0.197611 | lr=6.000000e-04 | norm=0.0246 | dt=8.62ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 501/600 | loss=0.197687 | lr=6.000000e-04 | norm=0.0244 | dt=11.94ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 511/600 | loss=0.193635 | lr=6.000000e-04 | norm=0.0239 | dt=11.14ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 521/600 | loss=0.191989 | lr=6.000000e-04 | norm=0.0243 | dt=13.86ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 531/600 | loss=0.193433 | lr=6.000000e-04 | norm=0.0241 | dt=10.35ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 541/600 | loss=0.189022 | lr=6.000000e-04 | norm=0.0255 | dt=9.56ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 551/600 | loss=0.191813 | lr=6.000000e-04 | norm=0.0242 | dt=8.81ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 561/600 | loss=0.192112 | lr=6.000000e-04 | norm=0.0238 | dt=10.15ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 571/600 | loss=0.195082 | lr=6.000000e-04 | norm=0.0245 | dt=9.40ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 581/600 | loss=0.191877 | lr=6.000000e-04 | norm=0.0239 | dt=9.43ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 591/600 | loss=0.191710 | lr=6.000000e-04 | norm=0.0251 | dt=9.69ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 600/600 | loss=0.190844 | lr=6.000000e-04 | norm=0.0253 | dt=9.89ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionWithAdapterModel] step 401/4000 | val_loss=0.2009\n",
            "[MlpRegressionModel] step 401/4000 | loss=0.171507 | lr=6.000000e-04 | norm=0.6540 | dt=13419.76ms\n",
            "[MlpRegressionModel] step 451/4000 | loss=0.183117 | lr=5.997430e-04 | norm=0.6632 | dt=21.17ms\n",
            "[MlpRegressionModel] step 501/4000 | loss=0.192634 | lr=5.989726e-04 | norm=0.5987 | dt=18.48ms\n",
            "Restarting data loader (train split)...\n",
            "[MlpRegressionModel] step 551/4000 | loss=0.195679 | lr=5.976901e-04 | norm=0.7130 | dt=18.63ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionModel] step 601/4000 | val_loss=0.1949\n",
            "Resetting data loader (valid split)...\n",
            "[MlpReconstructWithAdapterAndConsistency] step   1/600 | loss=0.621991 | lr=6.000000e-04 | norm=0.1536 | dt=19.11ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  11/600 | loss=0.521373 | lr=6.000000e-04 | norm=0.0528 | dt=9.24ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  21/600 | loss=0.468725 | lr=6.000000e-04 | norm=0.0458 | dt=11.93ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  31/600 | loss=0.422306 | lr=6.000000e-04 | norm=0.0425 | dt=11.20ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  41/600 | loss=0.384854 | lr=6.000000e-04 | norm=0.0387 | dt=10.76ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  51/600 | loss=0.354936 | lr=6.000000e-04 | norm=0.0351 | dt=10.10ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  61/600 | loss=0.337531 | lr=6.000000e-04 | norm=0.0330 | dt=10.57ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  71/600 | loss=0.323279 | lr=6.000000e-04 | norm=0.0323 | dt=12.19ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  81/600 | loss=0.309242 | lr=6.000000e-04 | norm=0.0330 | dt=10.11ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  91/600 | loss=0.298137 | lr=6.000000e-04 | norm=0.0309 | dt=11.14ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 101/600 | loss=0.290508 | lr=6.000000e-04 | norm=0.0284 | dt=11.27ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 111/600 | loss=0.283129 | lr=6.000000e-04 | norm=0.0295 | dt=10.94ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 121/600 | loss=0.273007 | lr=6.000000e-04 | norm=0.0283 | dt=10.04ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 131/600 | loss=0.271097 | lr=6.000000e-04 | norm=0.0309 | dt=7.73ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 141/600 | loss=0.266192 | lr=6.000000e-04 | norm=0.0281 | dt=9.09ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 151/600 | loss=0.260717 | lr=6.000000e-04 | norm=0.0274 | dt=8.00ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 161/600 | loss=0.254223 | lr=6.000000e-04 | norm=0.0279 | dt=8.43ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 171/600 | loss=0.252524 | lr=6.000000e-04 | norm=0.0289 | dt=8.34ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 181/600 | loss=0.249604 | lr=6.000000e-04 | norm=0.0287 | dt=8.75ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 191/600 | loss=0.244305 | lr=6.000000e-04 | norm=0.0304 | dt=8.03ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 201/600 | loss=0.240557 | lr=6.000000e-04 | norm=0.0270 | dt=9.83ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 211/600 | loss=0.239106 | lr=6.000000e-04 | norm=0.0274 | dt=8.09ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 221/600 | loss=0.237755 | lr=6.000000e-04 | norm=0.0270 | dt=9.27ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 231/600 | loss=0.234237 | lr=6.000000e-04 | norm=0.0272 | dt=9.30ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 241/600 | loss=0.230399 | lr=6.000000e-04 | norm=0.0267 | dt=10.62ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 251/600 | loss=0.232246 | lr=6.000000e-04 | norm=0.0276 | dt=8.07ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 261/600 | loss=0.229345 | lr=6.000000e-04 | norm=0.0264 | dt=8.79ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 271/600 | loss=0.225294 | lr=6.000000e-04 | norm=0.0260 | dt=9.35ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 281/600 | loss=0.224339 | lr=6.000000e-04 | norm=0.0278 | dt=10.30ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 291/600 | loss=0.223570 | lr=6.000000e-04 | norm=0.0273 | dt=8.32ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 301/600 | loss=0.225229 | lr=6.000000e-04 | norm=0.0280 | dt=9.42ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 311/600 | loss=0.220774 | lr=6.000000e-04 | norm=0.0272 | dt=9.39ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 321/600 | loss=0.220817 | lr=6.000000e-04 | norm=0.0262 | dt=8.50ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 331/600 | loss=0.221584 | lr=6.000000e-04 | norm=0.0288 | dt=8.00ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 341/600 | loss=0.219049 | lr=6.000000e-04 | norm=0.0279 | dt=9.92ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 351/600 | loss=0.214521 | lr=6.000000e-04 | norm=0.0257 | dt=8.02ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 361/600 | loss=0.215522 | lr=6.000000e-04 | norm=0.0266 | dt=8.66ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 371/600 | loss=0.208001 | lr=6.000000e-04 | norm=0.0272 | dt=10.39ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 381/600 | loss=0.212550 | lr=6.000000e-04 | norm=0.0287 | dt=11.27ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 391/600 | loss=0.209093 | lr=6.000000e-04 | norm=0.0308 | dt=8.00ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 401/600 | loss=0.210246 | lr=6.000000e-04 | norm=0.0267 | dt=8.08ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 411/600 | loss=0.204690 | lr=6.000000e-04 | norm=0.0241 | dt=10.80ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 421/600 | loss=0.206561 | lr=6.000000e-04 | norm=0.0291 | dt=8.47ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 431/600 | loss=0.202119 | lr=6.000000e-04 | norm=0.0261 | dt=9.30ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 441/600 | loss=0.203180 | lr=6.000000e-04 | norm=0.0279 | dt=7.88ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 451/600 | loss=0.202077 | lr=6.000000e-04 | norm=0.0266 | dt=8.35ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 461/600 | loss=0.200746 | lr=6.000000e-04 | norm=0.0285 | dt=8.59ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 471/600 | loss=0.202890 | lr=6.000000e-04 | norm=0.0255 | dt=7.96ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 481/600 | loss=0.199058 | lr=6.000000e-04 | norm=0.0252 | dt=10.58ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 491/600 | loss=0.201461 | lr=6.000000e-04 | norm=0.0258 | dt=7.55ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 501/600 | loss=0.201360 | lr=6.000000e-04 | norm=0.0278 | dt=10.34ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 511/600 | loss=0.198017 | lr=6.000000e-04 | norm=0.0290 | dt=10.19ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 521/600 | loss=0.197062 | lr=6.000000e-04 | norm=0.0270 | dt=10.21ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 531/600 | loss=0.201412 | lr=6.000000e-04 | norm=0.0281 | dt=8.10ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 541/600 | loss=0.194156 | lr=6.000000e-04 | norm=0.0259 | dt=7.90ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 551/600 | loss=0.196649 | lr=6.000000e-04 | norm=0.0263 | dt=8.72ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 561/600 | loss=0.196157 | lr=6.000000e-04 | norm=0.0245 | dt=8.90ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 571/600 | loss=0.197936 | lr=6.000000e-04 | norm=0.0258 | dt=11.69ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 581/600 | loss=0.194712 | lr=6.000000e-04 | norm=0.0249 | dt=11.47ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 591/600 | loss=0.196631 | lr=6.000000e-04 | norm=0.0248 | dt=10.33ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 600/600 | loss=0.196918 | lr=6.000000e-04 | norm=0.0251 | dt=10.74ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionWithAdapterModel] step 601/4000 | val_loss=0.1979\n",
            "[MlpRegressionModel] step 601/4000 | loss=0.196366 | lr=5.958981e-04 | norm=0.5793 | dt=13066.20ms\n",
            "[MlpRegressionModel] step 651/4000 | loss=0.186165 | lr=5.935999e-04 | norm=0.4386 | dt=16.92ms\n",
            "[MlpRegressionModel] step 701/4000 | loss=0.180300 | lr=5.908000e-04 | norm=0.6226 | dt=17.75ms\n",
            "[MlpRegressionModel] step 751/4000 | loss=0.157259 | lr=5.875036e-04 | norm=0.4504 | dt=15.56ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionModel] step 801/4000 | val_loss=0.1963\n",
            "Resetting data loader (valid split)...\n",
            "[MlpReconstructWithAdapterAndConsistency] step   1/600 | loss=0.620591 | lr=6.000000e-04 | norm=0.1421 | dt=15.83ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  11/600 | loss=0.522164 | lr=6.000000e-04 | norm=0.0515 | dt=13.13ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  21/600 | loss=0.474863 | lr=6.000000e-04 | norm=0.0445 | dt=9.06ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  31/600 | loss=0.431811 | lr=6.000000e-04 | norm=0.0422 | dt=10.05ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  41/600 | loss=0.395168 | lr=6.000000e-04 | norm=0.0387 | dt=10.33ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  51/600 | loss=0.364162 | lr=6.000000e-04 | norm=0.0355 | dt=10.63ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  61/600 | loss=0.343965 | lr=6.000000e-04 | norm=0.0357 | dt=7.96ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  71/600 | loss=0.329960 | lr=6.000000e-04 | norm=0.0322 | dt=9.29ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  81/600 | loss=0.314774 | lr=6.000000e-04 | norm=0.0339 | dt=12.42ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  91/600 | loss=0.305016 | lr=6.000000e-04 | norm=0.0317 | dt=10.55ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 101/600 | loss=0.296995 | lr=6.000000e-04 | norm=0.0313 | dt=13.21ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 111/600 | loss=0.291464 | lr=6.000000e-04 | norm=0.0313 | dt=9.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 121/600 | loss=0.278581 | lr=6.000000e-04 | norm=0.0312 | dt=8.23ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 131/600 | loss=0.277826 | lr=6.000000e-04 | norm=0.0286 | dt=8.74ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 141/600 | loss=0.270240 | lr=6.000000e-04 | norm=0.0289 | dt=9.71ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 151/600 | loss=0.267585 | lr=6.000000e-04 | norm=0.0304 | dt=7.80ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 161/600 | loss=0.261489 | lr=6.000000e-04 | norm=0.0286 | dt=7.81ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 171/600 | loss=0.257864 | lr=6.000000e-04 | norm=0.0312 | dt=9.35ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 181/600 | loss=0.254677 | lr=6.000000e-04 | norm=0.0293 | dt=10.65ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 191/600 | loss=0.251214 | lr=6.000000e-04 | norm=0.0293 | dt=10.91ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 201/600 | loss=0.247397 | lr=6.000000e-04 | norm=0.0283 | dt=7.49ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 211/600 | loss=0.244940 | lr=6.000000e-04 | norm=0.0304 | dt=9.03ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 221/600 | loss=0.243154 | lr=6.000000e-04 | norm=0.0278 | dt=7.90ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 231/600 | loss=0.239588 | lr=6.000000e-04 | norm=0.0287 | dt=13.36ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 241/600 | loss=0.236898 | lr=6.000000e-04 | norm=0.0267 | dt=10.94ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 251/600 | loss=0.239190 | lr=6.000000e-04 | norm=0.0291 | dt=10.40ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 261/600 | loss=0.236761 | lr=6.000000e-04 | norm=0.0322 | dt=8.00ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 271/600 | loss=0.232993 | lr=6.000000e-04 | norm=0.0338 | dt=8.94ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 281/600 | loss=0.227296 | lr=6.000000e-04 | norm=0.0281 | dt=8.94ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 291/600 | loss=0.229318 | lr=6.000000e-04 | norm=0.0315 | dt=9.82ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 301/600 | loss=0.230695 | lr=6.000000e-04 | norm=0.0308 | dt=12.96ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 311/600 | loss=0.225953 | lr=6.000000e-04 | norm=0.0290 | dt=8.17ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 321/600 | loss=0.228557 | lr=6.000000e-04 | norm=0.0339 | dt=8.40ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 331/600 | loss=0.226797 | lr=6.000000e-04 | norm=0.0340 | dt=9.78ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 341/600 | loss=0.225588 | lr=6.000000e-04 | norm=0.0282 | dt=10.62ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 351/600 | loss=0.219214 | lr=6.000000e-04 | norm=0.0262 | dt=9.12ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 361/600 | loss=0.219253 | lr=6.000000e-04 | norm=0.0276 | dt=10.47ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 371/600 | loss=0.215448 | lr=6.000000e-04 | norm=0.0305 | dt=9.53ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 381/600 | loss=0.217572 | lr=6.000000e-04 | norm=0.0275 | dt=7.91ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 391/600 | loss=0.213541 | lr=6.000000e-04 | norm=0.0287 | dt=7.71ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 401/600 | loss=0.215010 | lr=6.000000e-04 | norm=0.0306 | dt=8.82ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 411/600 | loss=0.209448 | lr=6.000000e-04 | norm=0.0289 | dt=12.46ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 421/600 | loss=0.210138 | lr=6.000000e-04 | norm=0.0296 | dt=8.74ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 431/600 | loss=0.209817 | lr=6.000000e-04 | norm=0.0299 | dt=10.00ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 441/600 | loss=0.208883 | lr=6.000000e-04 | norm=0.0332 | dt=11.08ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 451/600 | loss=0.207919 | lr=6.000000e-04 | norm=0.0271 | dt=9.01ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 461/600 | loss=0.206474 | lr=6.000000e-04 | norm=0.0258 | dt=7.94ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 471/600 | loss=0.207718 | lr=6.000000e-04 | norm=0.0301 | dt=11.91ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 481/600 | loss=0.203449 | lr=6.000000e-04 | norm=0.0278 | dt=9.86ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 491/600 | loss=0.207314 | lr=6.000000e-04 | norm=0.0277 | dt=8.25ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 501/600 | loss=0.207183 | lr=6.000000e-04 | norm=0.0266 | dt=10.50ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 511/600 | loss=0.202336 | lr=6.000000e-04 | norm=0.0305 | dt=9.04ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 521/600 | loss=0.201790 | lr=6.000000e-04 | norm=0.0302 | dt=8.53ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 531/600 | loss=0.203446 | lr=6.000000e-04 | norm=0.0273 | dt=8.24ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 541/600 | loss=0.199987 | lr=6.000000e-04 | norm=0.0309 | dt=9.66ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 551/600 | loss=0.203319 | lr=6.000000e-04 | norm=0.0299 | dt=10.33ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 561/600 | loss=0.202238 | lr=6.000000e-04 | norm=0.0296 | dt=7.85ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 571/600 | loss=0.205288 | lr=6.000000e-04 | norm=0.0286 | dt=8.34ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 581/600 | loss=0.199777 | lr=6.000000e-04 | norm=0.0284 | dt=8.74ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 591/600 | loss=0.200784 | lr=6.000000e-04 | norm=0.0281 | dt=9.95ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 600/600 | loss=0.201663 | lr=6.000000e-04 | norm=0.0263 | dt=8.68ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionWithAdapterModel] step 801/4000 | val_loss=0.1984\n",
            "[MlpRegressionModel] step 801/4000 | loss=0.184793 | lr=5.837170e-04 | norm=0.4357 | dt=13119.68ms\n",
            "[MlpRegressionModel] step 851/4000 | loss=0.185705 | lr=5.794475e-04 | norm=0.5938 | dt=18.08ms\n",
            "[MlpRegressionModel] step 901/4000 | loss=0.181131 | lr=5.747031e-04 | norm=0.5307 | dt=18.35ms\n",
            "[MlpRegressionModel] step 951/4000 | loss=0.163573 | lr=5.694929e-04 | norm=0.5223 | dt=19.53ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionModel] step 1001/4000 | val_loss=0.1981\n",
            "Resetting data loader (valid split)...\n",
            "[MlpReconstructWithAdapterAndConsistency] step   1/600 | loss=0.608909 | lr=6.000000e-04 | norm=0.1284 | dt=19.97ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  11/600 | loss=0.523989 | lr=6.000000e-04 | norm=0.0478 | dt=10.63ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  21/600 | loss=0.479121 | lr=6.000000e-04 | norm=0.0419 | dt=8.96ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  31/600 | loss=0.439357 | lr=6.000000e-04 | norm=0.0408 | dt=8.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  41/600 | loss=0.400385 | lr=6.000000e-04 | norm=0.0384 | dt=10.44ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  51/600 | loss=0.374553 | lr=6.000000e-04 | norm=0.0355 | dt=7.83ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  61/600 | loss=0.352704 | lr=6.000000e-04 | norm=0.0339 | dt=8.34ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  71/600 | loss=0.338908 | lr=6.000000e-04 | norm=0.0349 | dt=8.41ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  81/600 | loss=0.324414 | lr=6.000000e-04 | norm=0.0330 | dt=10.51ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  91/600 | loss=0.312131 | lr=6.000000e-04 | norm=0.0318 | dt=9.23ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 101/600 | loss=0.301818 | lr=6.000000e-04 | norm=0.0319 | dt=8.18ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 111/600 | loss=0.295655 | lr=6.000000e-04 | norm=0.0381 | dt=7.93ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 121/600 | loss=0.284660 | lr=6.000000e-04 | norm=0.0322 | dt=10.49ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 131/600 | loss=0.282795 | lr=6.000000e-04 | norm=0.0315 | dt=9.97ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 141/600 | loss=0.277453 | lr=6.000000e-04 | norm=0.0287 | dt=7.83ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 151/600 | loss=0.271468 | lr=6.000000e-04 | norm=0.0317 | dt=10.02ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 161/600 | loss=0.265034 | lr=6.000000e-04 | norm=0.0356 | dt=10.12ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 171/600 | loss=0.262205 | lr=6.000000e-04 | norm=0.0395 | dt=9.42ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 181/600 | loss=0.258614 | lr=6.000000e-04 | norm=0.0406 | dt=7.64ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 191/600 | loss=0.255055 | lr=6.000000e-04 | norm=0.0396 | dt=7.63ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 201/600 | loss=0.252150 | lr=6.000000e-04 | norm=0.0346 | dt=10.48ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 211/600 | loss=0.250527 | lr=6.000000e-04 | norm=0.0367 | dt=9.51ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 221/600 | loss=0.247753 | lr=6.000000e-04 | norm=0.0318 | dt=7.84ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 231/600 | loss=0.242485 | lr=6.000000e-04 | norm=0.0313 | dt=8.65ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 241/600 | loss=0.242889 | lr=6.000000e-04 | norm=0.0306 | dt=7.99ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 251/600 | loss=0.243514 | lr=6.000000e-04 | norm=0.0314 | dt=8.17ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 261/600 | loss=0.241670 | lr=6.000000e-04 | norm=0.0360 | dt=8.38ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 271/600 | loss=0.237079 | lr=6.000000e-04 | norm=0.0279 | dt=8.39ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 281/600 | loss=0.232919 | lr=6.000000e-04 | norm=0.0319 | dt=9.97ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 291/600 | loss=0.234534 | lr=6.000000e-04 | norm=0.0309 | dt=8.74ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 301/600 | loss=0.236795 | lr=6.000000e-04 | norm=0.0342 | dt=11.19ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 311/600 | loss=0.230653 | lr=6.000000e-04 | norm=0.0332 | dt=12.51ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 321/600 | loss=0.231527 | lr=6.000000e-04 | norm=0.0294 | dt=7.64ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 331/600 | loss=0.231559 | lr=6.000000e-04 | norm=0.0301 | dt=9.14ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 341/600 | loss=0.228995 | lr=6.000000e-04 | norm=0.0290 | dt=9.22ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 351/600 | loss=0.225418 | lr=6.000000e-04 | norm=0.0317 | dt=9.14ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 361/600 | loss=0.222769 | lr=6.000000e-04 | norm=0.0300 | dt=9.35ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 371/600 | loss=0.220127 | lr=6.000000e-04 | norm=0.0280 | dt=10.07ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 381/600 | loss=0.222705 | lr=6.000000e-04 | norm=0.0286 | dt=7.69ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 391/600 | loss=0.218344 | lr=6.000000e-04 | norm=0.0293 | dt=10.01ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 401/600 | loss=0.220840 | lr=6.000000e-04 | norm=0.0298 | dt=10.11ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 411/600 | loss=0.215994 | lr=6.000000e-04 | norm=0.0311 | dt=12.31ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 421/600 | loss=0.216015 | lr=6.000000e-04 | norm=0.0382 | dt=9.28ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 431/600 | loss=0.213358 | lr=6.000000e-04 | norm=0.0303 | dt=10.50ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 441/600 | loss=0.213767 | lr=6.000000e-04 | norm=0.0348 | dt=9.19ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 451/600 | loss=0.213191 | lr=6.000000e-04 | norm=0.0346 | dt=10.95ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 461/600 | loss=0.211652 | lr=6.000000e-04 | norm=0.0344 | dt=8.94ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 471/600 | loss=0.211646 | lr=6.000000e-04 | norm=0.0287 | dt=10.01ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 481/600 | loss=0.209346 | lr=6.000000e-04 | norm=0.0307 | dt=7.88ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 491/600 | loss=0.211747 | lr=6.000000e-04 | norm=0.0358 | dt=10.97ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 501/600 | loss=0.213055 | lr=6.000000e-04 | norm=0.0320 | dt=9.09ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 511/600 | loss=0.207812 | lr=6.000000e-04 | norm=0.0328 | dt=10.47ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 521/600 | loss=0.208593 | lr=6.000000e-04 | norm=0.0287 | dt=8.63ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 531/600 | loss=0.208464 | lr=6.000000e-04 | norm=0.0319 | dt=9.75ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 541/600 | loss=0.206963 | lr=6.000000e-04 | norm=0.0325 | dt=8.12ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 551/600 | loss=0.207592 | lr=6.000000e-04 | norm=0.0359 | dt=10.67ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 561/600 | loss=0.206406 | lr=6.000000e-04 | norm=0.0304 | dt=8.42ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 571/600 | loss=0.208928 | lr=6.000000e-04 | norm=0.0326 | dt=10.28ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 581/600 | loss=0.205488 | lr=6.000000e-04 | norm=0.0335 | dt=8.79ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 591/600 | loss=0.207932 | lr=6.000000e-04 | norm=0.0333 | dt=8.17ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 600/600 | loss=0.205574 | lr=6.000000e-04 | norm=0.0357 | dt=9.90ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionWithAdapterModel] step 1001/4000 | val_loss=0.1994\n",
            "[MlpRegressionModel] step 1001/4000 | loss=0.183917 | lr=5.638269e-04 | norm=0.4395 | dt=12901.68ms\n",
            "Restarting data loader (train split)...\n",
            "[MlpRegressionModel] step 1051/4000 | loss=0.182994 | lr=5.577157e-04 | norm=0.8074 | dt=16.12ms\n",
            "[MlpRegressionModel] step 1101/4000 | loss=0.171724 | lr=5.511711e-04 | norm=0.8004 | dt=14.47ms\n",
            "[MlpRegressionModel] step 1151/4000 | loss=0.159310 | lr=5.442054e-04 | norm=0.4823 | dt=22.25ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionModel] step 1201/4000 | val_loss=0.2027\n",
            "Resetting data loader (valid split)...\n",
            "[MlpReconstructWithAdapterAndConsistency] step   1/600 | loss=0.613682 | lr=6.000000e-04 | norm=0.1294 | dt=18.95ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  11/600 | loss=0.524688 | lr=6.000000e-04 | norm=0.0479 | dt=14.78ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  21/600 | loss=0.482943 | lr=6.000000e-04 | norm=0.0426 | dt=10.70ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  31/600 | loss=0.440745 | lr=6.000000e-04 | norm=0.0397 | dt=8.30ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  41/600 | loss=0.408997 | lr=6.000000e-04 | norm=0.0378 | dt=10.50ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  51/600 | loss=0.379626 | lr=6.000000e-04 | norm=0.0360 | dt=10.45ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  61/600 | loss=0.362384 | lr=6.000000e-04 | norm=0.0340 | dt=10.15ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  71/600 | loss=0.346477 | lr=6.000000e-04 | norm=0.0335 | dt=8.71ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  81/600 | loss=0.331409 | lr=6.000000e-04 | norm=0.0327 | dt=8.41ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  91/600 | loss=0.319864 | lr=6.000000e-04 | norm=0.0318 | dt=9.32ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 101/600 | loss=0.308107 | lr=6.000000e-04 | norm=0.0345 | dt=8.15ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 111/600 | loss=0.304444 | lr=6.000000e-04 | norm=0.0310 | dt=10.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 121/600 | loss=0.291538 | lr=6.000000e-04 | norm=0.0306 | dt=10.16ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 131/600 | loss=0.289614 | lr=6.000000e-04 | norm=0.0328 | dt=9.28ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 141/600 | loss=0.283636 | lr=6.000000e-04 | norm=0.0341 | dt=11.69ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 151/600 | loss=0.279557 | lr=6.000000e-04 | norm=0.0310 | dt=10.68ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 161/600 | loss=0.272724 | lr=6.000000e-04 | norm=0.0331 | dt=9.37ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 171/600 | loss=0.269543 | lr=6.000000e-04 | norm=0.0398 | dt=11.91ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 181/600 | loss=0.265730 | lr=6.000000e-04 | norm=0.0365 | dt=8.23ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 191/600 | loss=0.263098 | lr=6.000000e-04 | norm=0.0365 | dt=9.15ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 201/600 | loss=0.258968 | lr=6.000000e-04 | norm=0.0378 | dt=7.92ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 211/600 | loss=0.258231 | lr=6.000000e-04 | norm=0.0319 | dt=7.92ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 221/600 | loss=0.255743 | lr=6.000000e-04 | norm=0.0337 | dt=9.36ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 231/600 | loss=0.251786 | lr=6.000000e-04 | norm=0.0320 | dt=10.56ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 241/600 | loss=0.248008 | lr=6.000000e-04 | norm=0.0314 | dt=10.15ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 251/600 | loss=0.250438 | lr=6.000000e-04 | norm=0.0320 | dt=11.67ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 261/600 | loss=0.249053 | lr=6.000000e-04 | norm=0.0330 | dt=9.55ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 271/600 | loss=0.244693 | lr=6.000000e-04 | norm=0.0333 | dt=9.74ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 281/600 | loss=0.239912 | lr=6.000000e-04 | norm=0.0315 | dt=8.88ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 291/600 | loss=0.240793 | lr=6.000000e-04 | norm=0.0302 | dt=9.41ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 301/600 | loss=0.242086 | lr=6.000000e-04 | norm=0.0312 | dt=9.90ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 311/600 | loss=0.237951 | lr=6.000000e-04 | norm=0.0328 | dt=9.70ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 321/600 | loss=0.239766 | lr=6.000000e-04 | norm=0.0339 | dt=9.66ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 331/600 | loss=0.238456 | lr=6.000000e-04 | norm=0.0296 | dt=7.99ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 341/600 | loss=0.234439 | lr=6.000000e-04 | norm=0.0363 | dt=9.67ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 351/600 | loss=0.233138 | lr=6.000000e-04 | norm=0.0315 | dt=10.47ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 361/600 | loss=0.231211 | lr=6.000000e-04 | norm=0.0360 | dt=8.13ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 371/600 | loss=0.224907 | lr=6.000000e-04 | norm=0.0353 | dt=9.45ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 381/600 | loss=0.229114 | lr=6.000000e-04 | norm=0.0293 | dt=11.65ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 391/600 | loss=0.224401 | lr=6.000000e-04 | norm=0.0304 | dt=9.87ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 401/600 | loss=0.226324 | lr=6.000000e-04 | norm=0.0305 | dt=9.67ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 411/600 | loss=0.222552 | lr=6.000000e-04 | norm=0.0362 | dt=9.06ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 421/600 | loss=0.220961 | lr=6.000000e-04 | norm=0.0359 | dt=10.39ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 431/600 | loss=0.219206 | lr=6.000000e-04 | norm=0.0361 | dt=8.59ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 441/600 | loss=0.220062 | lr=6.000000e-04 | norm=0.0351 | dt=9.42ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 451/600 | loss=0.219008 | lr=6.000000e-04 | norm=0.0323 | dt=9.41ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 461/600 | loss=0.218002 | lr=6.000000e-04 | norm=0.0353 | dt=8.57ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 471/600 | loss=0.219193 | lr=6.000000e-04 | norm=0.0348 | dt=7.66ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 481/600 | loss=0.215530 | lr=6.000000e-04 | norm=0.0285 | dt=13.71ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 491/600 | loss=0.218394 | lr=6.000000e-04 | norm=0.0365 | dt=9.49ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 501/600 | loss=0.220199 | lr=6.000000e-04 | norm=0.0301 | dt=8.68ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 511/600 | loss=0.213877 | lr=6.000000e-04 | norm=0.0314 | dt=8.16ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 521/600 | loss=0.214323 | lr=6.000000e-04 | norm=0.0327 | dt=9.82ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 531/600 | loss=0.216012 | lr=6.000000e-04 | norm=0.0316 | dt=11.00ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 541/600 | loss=0.210992 | lr=6.000000e-04 | norm=0.0391 | dt=10.07ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 551/600 | loss=0.213867 | lr=6.000000e-04 | norm=0.0465 | dt=10.37ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 561/600 | loss=0.212298 | lr=6.000000e-04 | norm=0.0343 | dt=10.83ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 571/600 | loss=0.215416 | lr=6.000000e-04 | norm=0.0286 | dt=7.91ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 581/600 | loss=0.212537 | lr=6.000000e-04 | norm=0.0345 | dt=8.34ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 591/600 | loss=0.212575 | lr=6.000000e-04 | norm=0.0321 | dt=9.49ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 600/600 | loss=0.213376 | lr=6.000000e-04 | norm=0.0333 | dt=12.43ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionWithAdapterModel] step 1201/4000 | val_loss=0.2014\n",
            "[MlpRegressionModel] step 1201/4000 | loss=0.190189 | lr=5.368320e-04 | norm=0.4579 | dt=13160.59ms\n",
            "[MlpRegressionModel] step 1251/4000 | loss=0.172771 | lr=5.290649e-04 | norm=0.4887 | dt=17.33ms\n",
            "[MlpRegressionModel] step 1301/4000 | loss=0.175211 | lr=5.209188e-04 | norm=0.5024 | dt=19.38ms\n",
            "[MlpRegressionModel] step 1351/4000 | loss=0.170530 | lr=5.124094e-04 | norm=0.6244 | dt=20.38ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionModel] step 1401/4000 | val_loss=0.2046\n",
            "Resetting data loader (valid split)...\n",
            "[MlpReconstructWithAdapterAndConsistency] step   1/600 | loss=0.613311 | lr=6.000000e-04 | norm=0.1312 | dt=14.62ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  11/600 | loss=0.526883 | lr=6.000000e-04 | norm=0.0467 | dt=9.73ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  21/600 | loss=0.483515 | lr=6.000000e-04 | norm=0.0411 | dt=8.22ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  31/600 | loss=0.448202 | lr=6.000000e-04 | norm=0.0400 | dt=9.80ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  41/600 | loss=0.413184 | lr=6.000000e-04 | norm=0.0388 | dt=9.38ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  51/600 | loss=0.383922 | lr=6.000000e-04 | norm=0.0358 | dt=9.28ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  61/600 | loss=0.365675 | lr=6.000000e-04 | norm=0.0337 | dt=8.52ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  71/600 | loss=0.351359 | lr=6.000000e-04 | norm=0.0353 | dt=7.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  81/600 | loss=0.334693 | lr=6.000000e-04 | norm=0.0340 | dt=11.95ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  91/600 | loss=0.327196 | lr=6.000000e-04 | norm=0.0325 | dt=9.52ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 101/600 | loss=0.315756 | lr=6.000000e-04 | norm=0.0310 | dt=9.94ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 111/600 | loss=0.313334 | lr=6.000000e-04 | norm=0.0401 | dt=9.10ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 121/600 | loss=0.298703 | lr=6.000000e-04 | norm=0.0334 | dt=9.43ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 131/600 | loss=0.296492 | lr=6.000000e-04 | norm=0.0384 | dt=10.32ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 141/600 | loss=0.289299 | lr=6.000000e-04 | norm=0.0394 | dt=7.78ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 151/600 | loss=0.284802 | lr=6.000000e-04 | norm=0.0369 | dt=8.96ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 161/600 | loss=0.280754 | lr=6.000000e-04 | norm=0.0353 | dt=8.49ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 171/600 | loss=0.277003 | lr=6.000000e-04 | norm=0.0335 | dt=9.67ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 181/600 | loss=0.273288 | lr=6.000000e-04 | norm=0.0361 | dt=11.06ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 191/600 | loss=0.269178 | lr=6.000000e-04 | norm=0.0358 | dt=10.89ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 201/600 | loss=0.265690 | lr=6.000000e-04 | norm=0.0356 | dt=11.56ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 211/600 | loss=0.262748 | lr=6.000000e-04 | norm=0.0311 | dt=8.74ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 221/600 | loss=0.261672 | lr=6.000000e-04 | norm=0.0314 | dt=9.05ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 231/600 | loss=0.257578 | lr=6.000000e-04 | norm=0.0310 | dt=10.16ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 241/600 | loss=0.253978 | lr=6.000000e-04 | norm=0.0291 | dt=10.24ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 251/600 | loss=0.257321 | lr=6.000000e-04 | norm=0.0319 | dt=9.69ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 261/600 | loss=0.253712 | lr=6.000000e-04 | norm=0.0319 | dt=10.78ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 271/600 | loss=0.247805 | lr=6.000000e-04 | norm=0.0353 | dt=7.99ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 281/600 | loss=0.246385 | lr=6.000000e-04 | norm=0.0422 | dt=8.79ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 291/600 | loss=0.246465 | lr=6.000000e-04 | norm=0.0327 | dt=8.23ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 301/600 | loss=0.247532 | lr=6.000000e-04 | norm=0.0360 | dt=7.75ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 311/600 | loss=0.243856 | lr=6.000000e-04 | norm=0.0355 | dt=7.68ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 321/600 | loss=0.244866 | lr=6.000000e-04 | norm=0.0331 | dt=9.03ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 331/600 | loss=0.243998 | lr=6.000000e-04 | norm=0.0333 | dt=8.93ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 341/600 | loss=0.241403 | lr=6.000000e-04 | norm=0.0348 | dt=9.49ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 351/600 | loss=0.238782 | lr=6.000000e-04 | norm=0.0345 | dt=8.36ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 361/600 | loss=0.235389 | lr=6.000000e-04 | norm=0.0366 | dt=9.22ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 371/600 | loss=0.230380 | lr=6.000000e-04 | norm=0.0348 | dt=8.74ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 381/600 | loss=0.234070 | lr=6.000000e-04 | norm=0.0383 | dt=8.01ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 391/600 | loss=0.231199 | lr=6.000000e-04 | norm=0.0380 | dt=8.26ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 401/600 | loss=0.233110 | lr=6.000000e-04 | norm=0.0349 | dt=8.23ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 411/600 | loss=0.228584 | lr=6.000000e-04 | norm=0.0312 | dt=8.49ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 421/600 | loss=0.226982 | lr=6.000000e-04 | norm=0.0345 | dt=10.08ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 431/600 | loss=0.226257 | lr=6.000000e-04 | norm=0.0317 | dt=10.42ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 441/600 | loss=0.228215 | lr=6.000000e-04 | norm=0.0411 | dt=8.14ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 451/600 | loss=0.223804 | lr=6.000000e-04 | norm=0.0416 | dt=9.47ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 461/600 | loss=0.224016 | lr=6.000000e-04 | norm=0.0395 | dt=8.99ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 471/600 | loss=0.224639 | lr=6.000000e-04 | norm=0.0353 | dt=8.67ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 481/600 | loss=0.222949 | lr=6.000000e-04 | norm=0.0343 | dt=8.15ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 491/600 | loss=0.225135 | lr=6.000000e-04 | norm=0.0339 | dt=8.19ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 501/600 | loss=0.225412 | lr=6.000000e-04 | norm=0.0317 | dt=8.24ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 511/600 | loss=0.218954 | lr=6.000000e-04 | norm=0.0317 | dt=10.25ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 521/600 | loss=0.219587 | lr=6.000000e-04 | norm=0.0384 | dt=8.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 531/600 | loss=0.220337 | lr=6.000000e-04 | norm=0.0318 | dt=9.45ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 541/600 | loss=0.216847 | lr=6.000000e-04 | norm=0.0342 | dt=9.01ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 551/600 | loss=0.218571 | lr=6.000000e-04 | norm=0.0495 | dt=8.99ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 561/600 | loss=0.219988 | lr=6.000000e-04 | norm=0.0428 | dt=8.46ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 571/600 | loss=0.223317 | lr=6.000000e-04 | norm=0.0334 | dt=8.94ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 581/600 | loss=0.216392 | lr=6.000000e-04 | norm=0.0349 | dt=9.44ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 591/600 | loss=0.219191 | lr=6.000000e-04 | norm=0.0333 | dt=8.20ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 600/600 | loss=0.219212 | lr=6.000000e-04 | norm=0.0368 | dt=8.31ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionWithAdapterModel] step 1401/4000 | val_loss=0.2021\n",
            "[MlpRegressionModel] step 1401/4000 | loss=0.164726 | lr=5.035527e-04 | norm=0.6436 | dt=12907.43ms\n",
            "[MlpRegressionModel] step 1451/4000 | loss=0.149872 | lr=4.943656e-04 | norm=0.3778 | dt=21.07ms\n",
            "[MlpRegressionModel] step 1501/4000 | loss=0.162382 | lr=4.848656e-04 | norm=0.5741 | dt=16.09ms\n",
            "[MlpRegressionModel] step 1551/4000 | loss=0.165875 | lr=4.750709e-04 | norm=0.4588 | dt=20.31ms\n",
            "Restarting data loader (train split)...\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionModel] step 1601/4000 | val_loss=0.2124\n",
            "Resetting data loader (valid split)...\n",
            "[MlpReconstructWithAdapterAndConsistency] step   1/600 | loss=0.606393 | lr=6.000000e-04 | norm=0.1142 | dt=15.58ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  11/600 | loss=0.528520 | lr=6.000000e-04 | norm=0.0466 | dt=9.03ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  21/600 | loss=0.485068 | lr=6.000000e-04 | norm=0.0410 | dt=11.64ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  31/600 | loss=0.448884 | lr=6.000000e-04 | norm=0.0392 | dt=9.31ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  41/600 | loss=0.414146 | lr=6.000000e-04 | norm=0.0375 | dt=10.24ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  51/600 | loss=0.388405 | lr=6.000000e-04 | norm=0.0360 | dt=9.56ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  61/600 | loss=0.368985 | lr=6.000000e-04 | norm=0.0344 | dt=11.14ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  71/600 | loss=0.355607 | lr=6.000000e-04 | norm=0.0384 | dt=8.21ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  81/600 | loss=0.339142 | lr=6.000000e-04 | norm=0.0336 | dt=8.11ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  91/600 | loss=0.328285 | lr=6.000000e-04 | norm=0.0350 | dt=10.49ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 101/600 | loss=0.319147 | lr=6.000000e-04 | norm=0.0330 | dt=10.20ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 111/600 | loss=0.315448 | lr=6.000000e-04 | norm=0.0354 | dt=10.61ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 121/600 | loss=0.302310 | lr=6.000000e-04 | norm=0.0359 | dt=10.61ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 131/600 | loss=0.299948 | lr=6.000000e-04 | norm=0.0418 | dt=8.88ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 141/600 | loss=0.293297 | lr=6.000000e-04 | norm=0.0392 | dt=7.64ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 151/600 | loss=0.288032 | lr=6.000000e-04 | norm=0.0352 | dt=10.13ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 161/600 | loss=0.283306 | lr=6.000000e-04 | norm=0.0358 | dt=10.24ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 171/600 | loss=0.279657 | lr=6.000000e-04 | norm=0.0357 | dt=11.17ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 181/600 | loss=0.277186 | lr=6.000000e-04 | norm=0.0310 | dt=10.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 191/600 | loss=0.272705 | lr=6.000000e-04 | norm=0.0412 | dt=8.20ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 201/600 | loss=0.270220 | lr=6.000000e-04 | norm=0.0370 | dt=11.67ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 211/600 | loss=0.267672 | lr=6.000000e-04 | norm=0.0332 | dt=9.65ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 221/600 | loss=0.264847 | lr=6.000000e-04 | norm=0.0316 | dt=10.65ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 231/600 | loss=0.260660 | lr=6.000000e-04 | norm=0.0363 | dt=9.27ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 241/600 | loss=0.258999 | lr=6.000000e-04 | norm=0.0397 | dt=11.31ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 251/600 | loss=0.259739 | lr=6.000000e-04 | norm=0.0363 | dt=10.81ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 261/600 | loss=0.258522 | lr=6.000000e-04 | norm=0.0405 | dt=11.57ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 271/600 | loss=0.254214 | lr=6.000000e-04 | norm=0.0339 | dt=10.32ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 281/600 | loss=0.250564 | lr=6.000000e-04 | norm=0.0316 | dt=9.33ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 291/600 | loss=0.251183 | lr=6.000000e-04 | norm=0.0376 | dt=9.59ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 301/600 | loss=0.253902 | lr=6.000000e-04 | norm=0.0412 | dt=9.43ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 311/600 | loss=0.249928 | lr=6.000000e-04 | norm=0.0376 | dt=9.34ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 321/600 | loss=0.248782 | lr=6.000000e-04 | norm=0.0325 | dt=8.34ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 331/600 | loss=0.249001 | lr=6.000000e-04 | norm=0.0349 | dt=8.08ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 341/600 | loss=0.245186 | lr=6.000000e-04 | norm=0.0364 | dt=9.80ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 351/600 | loss=0.243899 | lr=6.000000e-04 | norm=0.0337 | dt=10.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 361/600 | loss=0.240678 | lr=6.000000e-04 | norm=0.0351 | dt=11.62ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 371/600 | loss=0.235993 | lr=6.000000e-04 | norm=0.0334 | dt=9.56ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 381/600 | loss=0.241431 | lr=6.000000e-04 | norm=0.0387 | dt=10.34ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 391/600 | loss=0.237493 | lr=6.000000e-04 | norm=0.0343 | dt=9.81ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 401/600 | loss=0.239682 | lr=6.000000e-04 | norm=0.0340 | dt=11.01ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 411/600 | loss=0.233508 | lr=6.000000e-04 | norm=0.0391 | dt=10.86ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 421/600 | loss=0.231584 | lr=6.000000e-04 | norm=0.0402 | dt=8.10ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 431/600 | loss=0.230787 | lr=6.000000e-04 | norm=0.0345 | dt=9.77ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 441/600 | loss=0.233193 | lr=6.000000e-04 | norm=0.0372 | dt=8.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 451/600 | loss=0.229649 | lr=6.000000e-04 | norm=0.0349 | dt=9.62ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 461/600 | loss=0.229483 | lr=6.000000e-04 | norm=0.0410 | dt=12.48ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 471/600 | loss=0.229835 | lr=6.000000e-04 | norm=0.0356 | dt=9.33ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 481/600 | loss=0.226943 | lr=6.000000e-04 | norm=0.0451 | dt=9.68ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 491/600 | loss=0.231562 | lr=6.000000e-04 | norm=0.0342 | dt=10.03ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 501/600 | loss=0.230380 | lr=6.000000e-04 | norm=0.0330 | dt=9.89ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 511/600 | loss=0.225206 | lr=6.000000e-04 | norm=0.0408 | dt=8.64ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 521/600 | loss=0.226446 | lr=6.000000e-04 | norm=0.0314 | dt=8.24ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 531/600 | loss=0.227838 | lr=6.000000e-04 | norm=0.0350 | dt=8.78ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 541/600 | loss=0.221676 | lr=6.000000e-04 | norm=0.0344 | dt=8.24ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 551/600 | loss=0.222891 | lr=6.000000e-04 | norm=0.0365 | dt=10.09ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 561/600 | loss=0.224614 | lr=6.000000e-04 | norm=0.0377 | dt=7.32ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 571/600 | loss=0.227086 | lr=6.000000e-04 | norm=0.0331 | dt=8.51ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 581/600 | loss=0.221791 | lr=6.000000e-04 | norm=0.0343 | dt=8.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 591/600 | loss=0.224483 | lr=6.000000e-04 | norm=0.0344 | dt=8.64ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 600/600 | loss=0.225023 | lr=6.000000e-04 | norm=0.0355 | dt=9.87ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionWithAdapterModel] step 1601/4000 | val_loss=0.2068\n",
            "[MlpRegressionModel] step 1601/4000 | loss=0.163891 | lr=4.650000e-04 | norm=0.4480 | dt=13168.70ms\n",
            "[MlpRegressionModel] step 1651/4000 | loss=0.158614 | lr=4.546721e-04 | norm=0.4563 | dt=15.54ms\n",
            "[MlpRegressionModel] step 1701/4000 | loss=0.148728 | lr=4.441069e-04 | norm=0.4238 | dt=19.67ms\n",
            "[MlpRegressionModel] step 1751/4000 | loss=0.155079 | lr=4.333245e-04 | norm=0.4723 | dt=14.13ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionModel] step 1801/4000 | val_loss=0.2163\n",
            "Resetting data loader (valid split)...\n",
            "[MlpReconstructWithAdapterAndConsistency] step   1/600 | loss=0.610410 | lr=6.000000e-04 | norm=0.1282 | dt=18.25ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  11/600 | loss=0.531690 | lr=6.000000e-04 | norm=0.0456 | dt=11.65ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  21/600 | loss=0.491518 | lr=6.000000e-04 | norm=0.0404 | dt=11.22ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  31/600 | loss=0.453322 | lr=6.000000e-04 | norm=0.0388 | dt=8.77ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  41/600 | loss=0.422597 | lr=6.000000e-04 | norm=0.0368 | dt=8.30ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  51/600 | loss=0.394797 | lr=6.000000e-04 | norm=0.0357 | dt=10.12ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  61/600 | loss=0.375455 | lr=6.000000e-04 | norm=0.0337 | dt=10.68ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  71/600 | loss=0.362964 | lr=6.000000e-04 | norm=0.0358 | dt=8.14ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  81/600 | loss=0.346246 | lr=6.000000e-04 | norm=0.0348 | dt=9.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  91/600 | loss=0.335887 | lr=6.000000e-04 | norm=0.0338 | dt=10.26ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 101/600 | loss=0.323549 | lr=6.000000e-04 | norm=0.0353 | dt=8.09ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 111/600 | loss=0.320869 | lr=6.000000e-04 | norm=0.0311 | dt=10.68ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 121/600 | loss=0.309465 | lr=6.000000e-04 | norm=0.0352 | dt=8.25ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 131/600 | loss=0.307524 | lr=6.000000e-04 | norm=0.0313 | dt=10.48ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 141/600 | loss=0.300903 | lr=6.000000e-04 | norm=0.0336 | dt=9.34ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 151/600 | loss=0.295766 | lr=6.000000e-04 | norm=0.0317 | dt=9.90ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 161/600 | loss=0.288901 | lr=6.000000e-04 | norm=0.0331 | dt=8.35ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 171/600 | loss=0.288091 | lr=6.000000e-04 | norm=0.0363 | dt=10.16ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 181/600 | loss=0.282249 | lr=6.000000e-04 | norm=0.0398 | dt=8.18ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 191/600 | loss=0.280156 | lr=6.000000e-04 | norm=0.0343 | dt=8.42ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 201/600 | loss=0.275015 | lr=6.000000e-04 | norm=0.0389 | dt=8.34ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 211/600 | loss=0.273298 | lr=6.000000e-04 | norm=0.0316 | dt=8.21ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 221/600 | loss=0.272394 | lr=6.000000e-04 | norm=0.0337 | dt=10.13ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 231/600 | loss=0.266414 | lr=6.000000e-04 | norm=0.0360 | dt=8.71ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 241/600 | loss=0.266153 | lr=6.000000e-04 | norm=0.0385 | dt=8.74ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 251/600 | loss=0.268469 | lr=6.000000e-04 | norm=0.0420 | dt=9.27ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 261/600 | loss=0.265842 | lr=6.000000e-04 | norm=0.0411 | dt=8.50ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 271/600 | loss=0.260240 | lr=6.000000e-04 | norm=0.0331 | dt=8.99ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 281/600 | loss=0.256698 | lr=6.000000e-04 | norm=0.0324 | dt=9.99ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 291/600 | loss=0.253874 | lr=6.000000e-04 | norm=0.0361 | dt=8.53ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 301/600 | loss=0.257390 | lr=6.000000e-04 | norm=0.0324 | dt=7.12ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 311/600 | loss=0.253483 | lr=6.000000e-04 | norm=0.0337 | dt=10.29ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 321/600 | loss=0.254851 | lr=6.000000e-04 | norm=0.0348 | dt=7.63ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 331/600 | loss=0.254912 | lr=6.000000e-04 | norm=0.0361 | dt=8.15ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 341/600 | loss=0.252017 | lr=6.000000e-04 | norm=0.0329 | dt=8.94ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 351/600 | loss=0.248092 | lr=6.000000e-04 | norm=0.0337 | dt=9.27ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 361/600 | loss=0.246244 | lr=6.000000e-04 | norm=0.0519 | dt=8.86ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 371/600 | loss=0.241046 | lr=6.000000e-04 | norm=0.0529 | dt=10.12ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 381/600 | loss=0.246604 | lr=6.000000e-04 | norm=0.0343 | dt=11.33ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 391/600 | loss=0.241680 | lr=6.000000e-04 | norm=0.0382 | dt=9.92ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 401/600 | loss=0.243963 | lr=6.000000e-04 | norm=0.0367 | dt=10.10ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 411/600 | loss=0.240099 | lr=6.000000e-04 | norm=0.0345 | dt=12.67ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 421/600 | loss=0.238415 | lr=6.000000e-04 | norm=0.0345 | dt=10.53ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 431/600 | loss=0.235629 | lr=6.000000e-04 | norm=0.0398 | dt=8.34ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 441/600 | loss=0.237148 | lr=6.000000e-04 | norm=0.0358 | dt=9.07ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 451/600 | loss=0.233947 | lr=6.000000e-04 | norm=0.0481 | dt=9.59ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 461/600 | loss=0.231899 | lr=6.000000e-04 | norm=0.0400 | dt=8.51ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 471/600 | loss=0.236716 | lr=6.000000e-04 | norm=0.0363 | dt=8.77ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 481/600 | loss=0.231398 | lr=6.000000e-04 | norm=0.0336 | dt=10.29ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 491/600 | loss=0.237365 | lr=6.000000e-04 | norm=0.0326 | dt=9.40ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 501/600 | loss=0.233981 | lr=6.000000e-04 | norm=0.0361 | dt=7.82ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 511/600 | loss=0.231324 | lr=6.000000e-04 | norm=0.0396 | dt=8.25ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 521/600 | loss=0.231576 | lr=6.000000e-04 | norm=0.0388 | dt=10.83ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 531/600 | loss=0.230327 | lr=6.000000e-04 | norm=0.0398 | dt=9.81ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 541/600 | loss=0.227357 | lr=6.000000e-04 | norm=0.0351 | dt=10.10ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 551/600 | loss=0.229901 | lr=6.000000e-04 | norm=0.0371 | dt=10.32ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 561/600 | loss=0.229690 | lr=6.000000e-04 | norm=0.0350 | dt=9.85ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 571/600 | loss=0.230124 | lr=6.000000e-04 | norm=0.0330 | dt=9.48ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 581/600 | loss=0.227906 | lr=6.000000e-04 | norm=0.0353 | dt=8.92ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 591/600 | loss=0.230628 | lr=6.000000e-04 | norm=0.0336 | dt=7.76ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 600/600 | loss=0.232268 | lr=6.000000e-04 | norm=0.0341 | dt=10.12ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionWithAdapterModel] step 1801/4000 | val_loss=0.2093\n",
            "[MlpRegressionModel] step 1801/4000 | loss=0.180259 | lr=4.223454e-04 | norm=0.5321 | dt=13189.25ms\n",
            "[MlpRegressionModel] step 1851/4000 | loss=0.153725 | lr=4.111906e-04 | norm=0.4479 | dt=23.59ms\n",
            "[MlpRegressionModel] step 1901/4000 | loss=0.144371 | lr=3.998811e-04 | norm=0.4013 | dt=21.45ms\n",
            "[MlpRegressionModel] step 1951/4000 | loss=0.152503 | lr=3.884387e-04 | norm=0.4032 | dt=16.30ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionModel] step 2001/4000 | val_loss=0.2238\n",
            "Resetting data loader (valid split)...\n",
            "[MlpReconstructWithAdapterAndConsistency] step   1/600 | loss=0.617841 | lr=6.000000e-04 | norm=0.1284 | dt=16.84ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  11/600 | loss=0.535738 | lr=6.000000e-04 | norm=0.0470 | dt=9.73ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  21/600 | loss=0.494520 | lr=6.000000e-04 | norm=0.0410 | dt=8.86ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  31/600 | loss=0.459714 | lr=6.000000e-04 | norm=0.0393 | dt=9.35ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  41/600 | loss=0.424457 | lr=6.000000e-04 | norm=0.0372 | dt=8.15ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  51/600 | loss=0.400830 | lr=6.000000e-04 | norm=0.0361 | dt=9.28ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  61/600 | loss=0.379972 | lr=6.000000e-04 | norm=0.0343 | dt=9.56ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  71/600 | loss=0.368081 | lr=6.000000e-04 | norm=0.0347 | dt=7.98ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  81/600 | loss=0.351023 | lr=6.000000e-04 | norm=0.0337 | dt=8.06ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  91/600 | loss=0.339823 | lr=6.000000e-04 | norm=0.0339 | dt=9.34ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 101/600 | loss=0.328362 | lr=6.000000e-04 | norm=0.0347 | dt=7.66ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 111/600 | loss=0.327576 | lr=6.000000e-04 | norm=0.0322 | dt=10.53ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 121/600 | loss=0.314244 | lr=6.000000e-04 | norm=0.0333 | dt=7.80ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 131/600 | loss=0.312848 | lr=6.000000e-04 | norm=0.0347 | dt=8.00ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 141/600 | loss=0.305306 | lr=6.000000e-04 | norm=0.0329 | dt=11.02ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 151/600 | loss=0.301193 | lr=6.000000e-04 | norm=0.0339 | dt=9.94ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 161/600 | loss=0.293037 | lr=6.000000e-04 | norm=0.0333 | dt=9.00ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 171/600 | loss=0.292863 | lr=6.000000e-04 | norm=0.0341 | dt=9.52ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 181/600 | loss=0.286033 | lr=6.000000e-04 | norm=0.0371 | dt=8.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 191/600 | loss=0.283099 | lr=6.000000e-04 | norm=0.0335 | dt=9.15ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 201/600 | loss=0.278278 | lr=6.000000e-04 | norm=0.0350 | dt=10.69ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 211/600 | loss=0.279200 | lr=6.000000e-04 | norm=0.0370 | dt=8.59ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 221/600 | loss=0.275656 | lr=6.000000e-04 | norm=0.0328 | dt=8.57ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 231/600 | loss=0.270154 | lr=6.000000e-04 | norm=0.0376 | dt=8.32ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 241/600 | loss=0.270097 | lr=6.000000e-04 | norm=0.0440 | dt=9.26ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 251/600 | loss=0.272091 | lr=6.000000e-04 | norm=0.0344 | dt=9.23ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 261/600 | loss=0.269186 | lr=6.000000e-04 | norm=0.0355 | dt=8.97ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 271/600 | loss=0.263693 | lr=6.000000e-04 | norm=0.0339 | dt=9.23ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 281/600 | loss=0.259279 | lr=6.000000e-04 | norm=0.0339 | dt=9.05ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 291/600 | loss=0.262913 | lr=6.000000e-04 | norm=0.0356 | dt=10.03ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 301/600 | loss=0.262647 | lr=6.000000e-04 | norm=0.0380 | dt=9.41ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 311/600 | loss=0.257764 | lr=6.000000e-04 | norm=0.0340 | dt=8.09ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 321/600 | loss=0.257838 | lr=6.000000e-04 | norm=0.0328 | dt=8.71ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 331/600 | loss=0.259737 | lr=6.000000e-04 | norm=0.0387 | dt=9.51ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 341/600 | loss=0.256514 | lr=6.000000e-04 | norm=0.0491 | dt=10.05ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 351/600 | loss=0.252109 | lr=6.000000e-04 | norm=0.0395 | dt=8.23ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 361/600 | loss=0.249494 | lr=6.000000e-04 | norm=0.0348 | dt=8.41ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 371/600 | loss=0.245779 | lr=6.000000e-04 | norm=0.0527 | dt=9.07ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 381/600 | loss=0.251255 | lr=6.000000e-04 | norm=0.0355 | dt=10.57ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 391/600 | loss=0.247621 | lr=6.000000e-04 | norm=0.0470 | dt=8.46ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 401/600 | loss=0.248065 | lr=6.000000e-04 | norm=0.0428 | dt=7.85ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 411/600 | loss=0.243528 | lr=6.000000e-04 | norm=0.0431 | dt=8.95ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 421/600 | loss=0.242784 | lr=6.000000e-04 | norm=0.0426 | dt=9.11ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 431/600 | loss=0.243029 | lr=6.000000e-04 | norm=0.0381 | dt=8.24ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 441/600 | loss=0.239818 | lr=6.000000e-04 | norm=0.0388 | dt=9.25ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 451/600 | loss=0.240773 | lr=6.000000e-04 | norm=0.0417 | dt=9.64ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 461/600 | loss=0.237741 | lr=6.000000e-04 | norm=0.0429 | dt=8.41ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 471/600 | loss=0.241369 | lr=6.000000e-04 | norm=0.0347 | dt=9.23ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 481/600 | loss=0.235412 | lr=6.000000e-04 | norm=0.0348 | dt=7.88ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 491/600 | loss=0.240225 | lr=6.000000e-04 | norm=0.0360 | dt=9.50ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 501/600 | loss=0.240366 | lr=6.000000e-04 | norm=0.0380 | dt=9.22ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 511/600 | loss=0.234870 | lr=6.000000e-04 | norm=0.0353 | dt=10.32ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 521/600 | loss=0.235414 | lr=6.000000e-04 | norm=0.0430 | dt=9.39ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 531/600 | loss=0.236685 | lr=6.000000e-04 | norm=0.0359 | dt=8.37ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 541/600 | loss=0.230076 | lr=6.000000e-04 | norm=0.0426 | dt=8.17ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 551/600 | loss=0.234450 | lr=6.000000e-04 | norm=0.0341 | dt=12.93ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 561/600 | loss=0.232310 | lr=6.000000e-04 | norm=0.0389 | dt=8.31ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 571/600 | loss=0.236930 | lr=6.000000e-04 | norm=0.0341 | dt=9.31ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 581/600 | loss=0.232380 | lr=6.000000e-04 | norm=0.0394 | dt=9.21ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 591/600 | loss=0.235713 | lr=6.000000e-04 | norm=0.0461 | dt=11.11ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 600/600 | loss=0.234715 | lr=6.000000e-04 | norm=0.0381 | dt=8.15ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionWithAdapterModel] step 2001/4000 | val_loss=0.2151\n",
            "[MlpRegressionModel] step 2001/4000 | loss=0.163574 | lr=3.768850e-04 | norm=0.4519 | dt=13013.59ms\n",
            "[MlpRegressionModel] step 2051/4000 | loss=0.150262 | lr=3.652421e-04 | norm=0.4748 | dt=14.12ms\n",
            "Restarting data loader (train split)...\n",
            "[MlpRegressionModel] step 2101/4000 | loss=0.137998 | lr=3.535321e-04 | norm=0.4322 | dt=21.14ms\n",
            "[MlpRegressionModel] step 2151/4000 | loss=0.138701 | lr=3.417772e-04 | norm=0.4804 | dt=17.95ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionModel] step 2201/4000 | val_loss=0.2290\n",
            "Resetting data loader (valid split)...\n",
            "[MlpReconstructWithAdapterAndConsistency] step   1/600 | loss=0.605571 | lr=6.000000e-04 | norm=0.1081 | dt=15.51ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  11/600 | loss=0.534277 | lr=6.000000e-04 | norm=0.0447 | dt=10.02ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  21/600 | loss=0.494422 | lr=6.000000e-04 | norm=0.0396 | dt=13.57ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  31/600 | loss=0.459187 | lr=6.000000e-04 | norm=0.0384 | dt=8.74ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  41/600 | loss=0.428823 | lr=6.000000e-04 | norm=0.0360 | dt=8.11ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  51/600 | loss=0.402405 | lr=6.000000e-04 | norm=0.0357 | dt=8.18ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  61/600 | loss=0.384663 | lr=6.000000e-04 | norm=0.0371 | dt=7.72ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  71/600 | loss=0.369912 | lr=6.000000e-04 | norm=0.0381 | dt=8.80ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  81/600 | loss=0.353682 | lr=6.000000e-04 | norm=0.0362 | dt=9.86ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  91/600 | loss=0.343300 | lr=6.000000e-04 | norm=0.0396 | dt=8.38ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 101/600 | loss=0.334252 | lr=6.000000e-04 | norm=0.0389 | dt=9.43ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 111/600 | loss=0.328918 | lr=6.000000e-04 | norm=0.0368 | dt=8.05ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 121/600 | loss=0.317484 | lr=6.000000e-04 | norm=0.0449 | dt=9.00ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 131/600 | loss=0.313885 | lr=6.000000e-04 | norm=0.0344 | dt=9.11ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 141/600 | loss=0.310107 | lr=6.000000e-04 | norm=0.0380 | dt=8.30ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 151/600 | loss=0.303865 | lr=6.000000e-04 | norm=0.0345 | dt=7.80ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 161/600 | loss=0.298043 | lr=6.000000e-04 | norm=0.0337 | dt=8.58ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 171/600 | loss=0.293621 | lr=6.000000e-04 | norm=0.0402 | dt=10.25ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 181/600 | loss=0.293875 | lr=6.000000e-04 | norm=0.0382 | dt=9.10ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 191/600 | loss=0.286837 | lr=6.000000e-04 | norm=0.0343 | dt=9.38ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 201/600 | loss=0.284026 | lr=6.000000e-04 | norm=0.0448 | dt=8.88ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 211/600 | loss=0.282789 | lr=6.000000e-04 | norm=0.0388 | dt=9.81ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 221/600 | loss=0.280202 | lr=6.000000e-04 | norm=0.0363 | dt=9.98ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 231/600 | loss=0.276341 | lr=6.000000e-04 | norm=0.0427 | dt=8.45ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 241/600 | loss=0.275227 | lr=6.000000e-04 | norm=0.0409 | dt=9.96ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 251/600 | loss=0.273871 | lr=6.000000e-04 | norm=0.0372 | dt=12.38ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 261/600 | loss=0.273474 | lr=6.000000e-04 | norm=0.0414 | dt=8.91ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 271/600 | loss=0.268249 | lr=6.000000e-04 | norm=0.0331 | dt=10.57ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 281/600 | loss=0.264116 | lr=6.000000e-04 | norm=0.0339 | dt=8.57ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 291/600 | loss=0.264582 | lr=6.000000e-04 | norm=0.0338 | dt=9.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 301/600 | loss=0.266947 | lr=6.000000e-04 | norm=0.0414 | dt=11.86ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 311/600 | loss=0.262112 | lr=6.000000e-04 | norm=0.0377 | dt=11.12ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 321/600 | loss=0.264310 | lr=6.000000e-04 | norm=0.0400 | dt=10.23ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 331/600 | loss=0.264392 | lr=6.000000e-04 | norm=0.0431 | dt=11.39ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 341/600 | loss=0.263757 | lr=6.000000e-04 | norm=0.0364 | dt=9.47ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 351/600 | loss=0.258346 | lr=6.000000e-04 | norm=0.0420 | dt=9.33ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 361/600 | loss=0.255066 | lr=6.000000e-04 | norm=0.0496 | dt=9.22ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 371/600 | loss=0.250180 | lr=6.000000e-04 | norm=0.0354 | dt=12.49ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 381/600 | loss=0.253687 | lr=6.000000e-04 | norm=0.0359 | dt=10.64ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 391/600 | loss=0.251166 | lr=6.000000e-04 | norm=0.0356 | dt=10.76ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 401/600 | loss=0.253980 | lr=6.000000e-04 | norm=0.0418 | dt=9.31ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 411/600 | loss=0.247344 | lr=6.000000e-04 | norm=0.0459 | dt=9.74ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 421/600 | loss=0.246652 | lr=6.000000e-04 | norm=0.0334 | dt=8.86ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 431/600 | loss=0.246960 | lr=6.000000e-04 | norm=0.0532 | dt=10.38ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 441/600 | loss=0.246122 | lr=6.000000e-04 | norm=0.0470 | dt=8.87ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 451/600 | loss=0.246206 | lr=6.000000e-04 | norm=0.0345 | dt=8.31ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 461/600 | loss=0.240014 | lr=6.000000e-04 | norm=0.0429 | dt=11.02ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 471/600 | loss=0.245913 | lr=6.000000e-04 | norm=0.0373 | dt=10.37ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 481/600 | loss=0.240778 | lr=6.000000e-04 | norm=0.0413 | dt=9.86ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 491/600 | loss=0.246543 | lr=6.000000e-04 | norm=0.0406 | dt=8.26ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 501/600 | loss=0.245118 | lr=6.000000e-04 | norm=0.0360 | dt=9.09ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 511/600 | loss=0.239927 | lr=6.000000e-04 | norm=0.0332 | dt=10.58ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 521/600 | loss=0.240828 | lr=6.000000e-04 | norm=0.0364 | dt=9.05ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 531/600 | loss=0.242453 | lr=6.000000e-04 | norm=0.0402 | dt=8.33ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 541/600 | loss=0.238422 | lr=6.000000e-04 | norm=0.0413 | dt=9.67ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 551/600 | loss=0.237780 | lr=6.000000e-04 | norm=0.0349 | dt=9.78ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 561/600 | loss=0.239436 | lr=6.000000e-04 | norm=0.0363 | dt=8.59ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 571/600 | loss=0.240474 | lr=6.000000e-04 | norm=0.0373 | dt=8.15ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 581/600 | loss=0.238837 | lr=6.000000e-04 | norm=0.0407 | dt=9.67ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 591/600 | loss=0.238888 | lr=6.000000e-04 | norm=0.0402 | dt=10.86ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 600/600 | loss=0.239329 | lr=6.000000e-04 | norm=0.0350 | dt=8.51ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionWithAdapterModel] step 2201/4000 | val_loss=0.2202\n",
            "[MlpRegressionModel] step 2201/4000 | loss=0.132816 | lr=3.300000e-04 | norm=0.4331 | dt=12884.98ms\n",
            "[MlpRegressionModel] step 2251/4000 | loss=0.129231 | lr=3.182228e-04 | norm=0.4498 | dt=21.93ms\n",
            "[MlpRegressionModel] step 2301/4000 | loss=0.152060 | lr=3.064679e-04 | norm=0.5046 | dt=14.13ms\n",
            "[MlpRegressionModel] step 2351/4000 | loss=0.135794 | lr=2.947579e-04 | norm=0.6041 | dt=13.80ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionModel] step 2401/4000 | val_loss=0.2338\n",
            "Resetting data loader (valid split)...\n",
            "[MlpReconstructWithAdapterAndConsistency] step   1/600 | loss=0.608742 | lr=6.000000e-04 | norm=0.1063 | dt=18.16ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  11/600 | loss=0.533070 | lr=6.000000e-04 | norm=0.0448 | dt=11.18ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  21/600 | loss=0.495068 | lr=6.000000e-04 | norm=0.0402 | dt=14.17ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  31/600 | loss=0.460616 | lr=6.000000e-04 | norm=0.0387 | dt=8.72ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  41/600 | loss=0.427144 | lr=6.000000e-04 | norm=0.0377 | dt=7.47ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  51/600 | loss=0.402167 | lr=6.000000e-04 | norm=0.0355 | dt=10.16ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  61/600 | loss=0.386325 | lr=6.000000e-04 | norm=0.0338 | dt=9.42ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  71/600 | loss=0.372516 | lr=6.000000e-04 | norm=0.0337 | dt=10.16ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  81/600 | loss=0.356152 | lr=6.000000e-04 | norm=0.0335 | dt=8.32ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  91/600 | loss=0.345445 | lr=6.000000e-04 | norm=0.0319 | dt=9.54ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 101/600 | loss=0.336780 | lr=6.000000e-04 | norm=0.0327 | dt=8.32ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 111/600 | loss=0.331957 | lr=6.000000e-04 | norm=0.0340 | dt=10.11ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 121/600 | loss=0.321014 | lr=6.000000e-04 | norm=0.0368 | dt=9.41ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 131/600 | loss=0.318115 | lr=6.000000e-04 | norm=0.0329 | dt=8.09ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 141/600 | loss=0.310806 | lr=6.000000e-04 | norm=0.0322 | dt=8.31ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 151/600 | loss=0.306987 | lr=6.000000e-04 | norm=0.0381 | dt=8.97ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 161/600 | loss=0.299449 | lr=6.000000e-04 | norm=0.0345 | dt=10.33ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 171/600 | loss=0.298398 | lr=6.000000e-04 | norm=0.0334 | dt=8.05ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 181/600 | loss=0.294162 | lr=6.000000e-04 | norm=0.0382 | dt=10.08ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 191/600 | loss=0.290051 | lr=6.000000e-04 | norm=0.0364 | dt=8.67ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 201/600 | loss=0.286058 | lr=6.000000e-04 | norm=0.0365 | dt=10.30ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 211/600 | loss=0.284339 | lr=6.000000e-04 | norm=0.0394 | dt=9.10ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 221/600 | loss=0.282050 | lr=6.000000e-04 | norm=0.0354 | dt=10.36ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 231/600 | loss=0.277048 | lr=6.000000e-04 | norm=0.0330 | dt=7.80ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 241/600 | loss=0.277083 | lr=6.000000e-04 | norm=0.0402 | dt=9.59ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 251/600 | loss=0.279265 | lr=6.000000e-04 | norm=0.0411 | dt=8.27ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 261/600 | loss=0.274127 | lr=6.000000e-04 | norm=0.0368 | dt=10.22ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 271/600 | loss=0.270403 | lr=6.000000e-04 | norm=0.0384 | dt=7.90ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 281/600 | loss=0.268390 | lr=6.000000e-04 | norm=0.0367 | dt=9.97ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 291/600 | loss=0.267278 | lr=6.000000e-04 | norm=0.0404 | dt=9.30ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 301/600 | loss=0.270882 | lr=6.000000e-04 | norm=0.0345 | dt=10.20ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 311/600 | loss=0.266661 | lr=6.000000e-04 | norm=0.0368 | dt=9.59ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 321/600 | loss=0.266852 | lr=6.000000e-04 | norm=0.0389 | dt=9.27ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 331/600 | loss=0.266384 | lr=6.000000e-04 | norm=0.0387 | dt=8.30ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 341/600 | loss=0.264778 | lr=6.000000e-04 | norm=0.0356 | dt=10.31ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 351/600 | loss=0.257924 | lr=6.000000e-04 | norm=0.0349 | dt=9.54ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 361/600 | loss=0.259145 | lr=6.000000e-04 | norm=0.0363 | dt=8.26ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 371/600 | loss=0.252899 | lr=6.000000e-04 | norm=0.0350 | dt=9.47ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 381/600 | loss=0.256569 | lr=6.000000e-04 | norm=0.0438 | dt=10.87ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 391/600 | loss=0.251572 | lr=6.000000e-04 | norm=0.0377 | dt=9.28ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 401/600 | loss=0.253968 | lr=6.000000e-04 | norm=0.0375 | dt=10.22ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 411/600 | loss=0.250315 | lr=6.000000e-04 | norm=0.0390 | dt=9.26ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 421/600 | loss=0.248466 | lr=6.000000e-04 | norm=0.0350 | dt=8.29ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 431/600 | loss=0.246426 | lr=6.000000e-04 | norm=0.0456 | dt=8.54ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 441/600 | loss=0.246382 | lr=6.000000e-04 | norm=0.0367 | dt=9.80ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 451/600 | loss=0.245654 | lr=6.000000e-04 | norm=0.0412 | dt=7.66ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 461/600 | loss=0.243630 | lr=6.000000e-04 | norm=0.0381 | dt=9.06ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 471/600 | loss=0.247326 | lr=6.000000e-04 | norm=0.0367 | dt=9.37ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 481/600 | loss=0.245192 | lr=6.000000e-04 | norm=0.0475 | dt=9.09ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 491/600 | loss=0.248468 | lr=6.000000e-04 | norm=0.0383 | dt=8.11ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 501/600 | loss=0.249081 | lr=6.000000e-04 | norm=0.0355 | dt=9.50ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 511/600 | loss=0.240405 | lr=6.000000e-04 | norm=0.0372 | dt=10.12ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 521/600 | loss=0.241461 | lr=6.000000e-04 | norm=0.0371 | dt=10.26ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 531/600 | loss=0.243645 | lr=6.000000e-04 | norm=0.0366 | dt=7.96ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 541/600 | loss=0.238537 | lr=6.000000e-04 | norm=0.0401 | dt=9.26ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 551/600 | loss=0.240039 | lr=6.000000e-04 | norm=0.0408 | dt=8.18ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 561/600 | loss=0.240320 | lr=6.000000e-04 | norm=0.0367 | dt=9.64ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 571/600 | loss=0.245464 | lr=6.000000e-04 | norm=0.0411 | dt=7.83ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 581/600 | loss=0.240873 | lr=6.000000e-04 | norm=0.0410 | dt=8.85ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 591/600 | loss=0.240880 | lr=6.000000e-04 | norm=0.0396 | dt=8.20ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 600/600 | loss=0.244892 | lr=6.000000e-04 | norm=0.0467 | dt=7.74ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionWithAdapterModel] step 2401/4000 | val_loss=0.2241\n",
            "[MlpRegressionModel] step 2401/4000 | loss=0.129287 | lr=2.831150e-04 | norm=0.4254 | dt=13488.73ms\n",
            "[MlpRegressionModel] step 2451/4000 | loss=0.135199 | lr=2.715613e-04 | norm=0.4514 | dt=19.42ms\n",
            "[MlpRegressionModel] step 2501/4000 | loss=0.133246 | lr=2.601189e-04 | norm=0.5100 | dt=19.01ms\n",
            "[MlpRegressionModel] step 2551/4000 | loss=0.122855 | lr=2.488094e-04 | norm=0.4312 | dt=17.70ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionModel] step 2601/4000 | val_loss=0.2363\n",
            "Resetting data loader (valid split)...\n",
            "[MlpReconstructWithAdapterAndConsistency] step   1/600 | loss=0.605886 | lr=6.000000e-04 | norm=0.1063 | dt=16.14ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  11/600 | loss=0.532269 | lr=6.000000e-04 | norm=0.0439 | dt=9.15ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  21/600 | loss=0.497322 | lr=6.000000e-04 | norm=0.0397 | dt=13.63ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  31/600 | loss=0.460719 | lr=6.000000e-04 | norm=0.0383 | dt=9.72ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  41/600 | loss=0.429790 | lr=6.000000e-04 | norm=0.0370 | dt=10.09ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  51/600 | loss=0.406429 | lr=6.000000e-04 | norm=0.0367 | dt=8.01ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  61/600 | loss=0.387435 | lr=6.000000e-04 | norm=0.0342 | dt=7.96ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  71/600 | loss=0.373446 | lr=6.000000e-04 | norm=0.0338 | dt=8.97ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  81/600 | loss=0.357191 | lr=6.000000e-04 | norm=0.0327 | dt=9.67ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  91/600 | loss=0.348005 | lr=6.000000e-04 | norm=0.0345 | dt=7.83ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 101/600 | loss=0.338941 | lr=6.000000e-04 | norm=0.0322 | dt=10.12ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 111/600 | loss=0.333575 | lr=6.000000e-04 | norm=0.0345 | dt=9.25ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 121/600 | loss=0.320771 | lr=6.000000e-04 | norm=0.0366 | dt=9.05ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 131/600 | loss=0.319109 | lr=6.000000e-04 | norm=0.0430 | dt=7.61ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 141/600 | loss=0.313604 | lr=6.000000e-04 | norm=0.0323 | dt=11.10ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 151/600 | loss=0.306697 | lr=6.000000e-04 | norm=0.0361 | dt=8.77ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 161/600 | loss=0.299084 | lr=6.000000e-04 | norm=0.0335 | dt=10.24ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 171/600 | loss=0.299897 | lr=6.000000e-04 | norm=0.0326 | dt=7.69ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 181/600 | loss=0.294051 | lr=6.000000e-04 | norm=0.0342 | dt=9.35ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 191/600 | loss=0.291061 | lr=6.000000e-04 | norm=0.0345 | dt=9.55ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 201/600 | loss=0.288279 | lr=6.000000e-04 | norm=0.0347 | dt=9.24ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 211/600 | loss=0.288608 | lr=6.000000e-04 | norm=0.0374 | dt=7.34ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 221/600 | loss=0.284200 | lr=6.000000e-04 | norm=0.0385 | dt=10.18ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 231/600 | loss=0.280757 | lr=6.000000e-04 | norm=0.0363 | dt=7.49ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 241/600 | loss=0.277589 | lr=6.000000e-04 | norm=0.0364 | dt=9.44ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 251/600 | loss=0.281158 | lr=6.000000e-04 | norm=0.0343 | dt=9.65ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 261/600 | loss=0.274877 | lr=6.000000e-04 | norm=0.0428 | dt=10.65ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 271/600 | loss=0.271494 | lr=6.000000e-04 | norm=0.0402 | dt=10.65ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 281/600 | loss=0.269946 | lr=6.000000e-04 | norm=0.0397 | dt=10.20ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 291/600 | loss=0.271351 | lr=6.000000e-04 | norm=0.0424 | dt=8.82ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 301/600 | loss=0.274120 | lr=6.000000e-04 | norm=0.0359 | dt=9.55ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 311/600 | loss=0.266750 | lr=6.000000e-04 | norm=0.0394 | dt=9.36ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 321/600 | loss=0.268661 | lr=6.000000e-04 | norm=0.0374 | dt=10.34ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 331/600 | loss=0.268428 | lr=6.000000e-04 | norm=0.0389 | dt=10.16ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 341/600 | loss=0.264837 | lr=6.000000e-04 | norm=0.0425 | dt=8.85ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 351/600 | loss=0.262323 | lr=6.000000e-04 | norm=0.0414 | dt=7.62ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 361/600 | loss=0.259383 | lr=6.000000e-04 | norm=0.0463 | dt=10.11ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 371/600 | loss=0.255319 | lr=6.000000e-04 | norm=0.0352 | dt=9.38ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 381/600 | loss=0.258667 | lr=6.000000e-04 | norm=0.0340 | dt=7.44ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 391/600 | loss=0.253479 | lr=6.000000e-04 | norm=0.0344 | dt=8.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 401/600 | loss=0.257044 | lr=6.000000e-04 | norm=0.0365 | dt=9.82ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 411/600 | loss=0.250262 | lr=6.000000e-04 | norm=0.0339 | dt=7.88ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 421/600 | loss=0.251682 | lr=6.000000e-04 | norm=0.0342 | dt=8.28ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 431/600 | loss=0.250987 | lr=6.000000e-04 | norm=0.0380 | dt=10.17ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 441/600 | loss=0.250202 | lr=6.000000e-04 | norm=0.0355 | dt=10.91ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 451/600 | loss=0.248719 | lr=6.000000e-04 | norm=0.0462 | dt=11.14ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 461/600 | loss=0.249115 | lr=6.000000e-04 | norm=0.0398 | dt=11.96ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 471/600 | loss=0.250025 | lr=6.000000e-04 | norm=0.0382 | dt=8.42ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 481/600 | loss=0.246068 | lr=6.000000e-04 | norm=0.0403 | dt=10.92ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 491/600 | loss=0.249110 | lr=6.000000e-04 | norm=0.0374 | dt=11.07ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 501/600 | loss=0.251507 | lr=6.000000e-04 | norm=0.0420 | dt=10.23ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 511/600 | loss=0.244497 | lr=6.000000e-04 | norm=0.0391 | dt=8.58ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 521/600 | loss=0.244694 | lr=6.000000e-04 | norm=0.0407 | dt=8.65ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 531/600 | loss=0.243894 | lr=6.000000e-04 | norm=0.0419 | dt=7.82ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 541/600 | loss=0.241482 | lr=6.000000e-04 | norm=0.0337 | dt=7.79ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 551/600 | loss=0.242175 | lr=6.000000e-04 | norm=0.0361 | dt=7.41ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 561/600 | loss=0.244838 | lr=6.000000e-04 | norm=0.0398 | dt=8.62ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 571/600 | loss=0.245136 | lr=6.000000e-04 | norm=0.0341 | dt=9.41ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 581/600 | loss=0.242585 | lr=6.000000e-04 | norm=0.0404 | dt=11.71ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 591/600 | loss=0.244648 | lr=6.000000e-04 | norm=0.0361 | dt=9.54ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 600/600 | loss=0.243699 | lr=6.000000e-04 | norm=0.0404 | dt=9.87ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionWithAdapterModel] step 2601/4000 | val_loss=0.2244\n",
            "[MlpRegressionModel] step 2601/4000 | loss=0.141115 | lr=2.376546e-04 | norm=0.5291 | dt=13340.19ms\n",
            "Restarting data loader (train split)...\n",
            "[MlpRegressionModel] step 2651/4000 | loss=0.122199 | lr=2.266755e-04 | norm=0.4110 | dt=16.51ms\n",
            "[MlpRegressionModel] step 2701/4000 | loss=0.129251 | lr=2.158931e-04 | norm=0.5408 | dt=19.04ms\n",
            "[MlpRegressionModel] step 2751/4000 | loss=0.125280 | lr=2.053279e-04 | norm=0.4640 | dt=17.51ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionModel] step 2801/4000 | val_loss=0.2480\n",
            "Resetting data loader (valid split)...\n",
            "[MlpReconstructWithAdapterAndConsistency] step   1/600 | loss=0.602709 | lr=6.000000e-04 | norm=0.0959 | dt=15.96ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  11/600 | loss=0.533821 | lr=6.000000e-04 | norm=0.0430 | dt=9.04ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  21/600 | loss=0.494899 | lr=6.000000e-04 | norm=0.0391 | dt=11.72ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  31/600 | loss=0.461995 | lr=6.000000e-04 | norm=0.0378 | dt=8.35ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  41/600 | loss=0.432714 | lr=6.000000e-04 | norm=0.0359 | dt=10.39ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  51/600 | loss=0.410523 | lr=6.000000e-04 | norm=0.0355 | dt=11.30ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  61/600 | loss=0.391815 | lr=6.000000e-04 | norm=0.0343 | dt=8.21ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  71/600 | loss=0.380136 | lr=6.000000e-04 | norm=0.0335 | dt=8.48ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  81/600 | loss=0.362833 | lr=6.000000e-04 | norm=0.0347 | dt=9.71ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  91/600 | loss=0.351531 | lr=6.000000e-04 | norm=0.0327 | dt=8.85ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 101/600 | loss=0.340840 | lr=6.000000e-04 | norm=0.0337 | dt=10.22ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 111/600 | loss=0.336618 | lr=6.000000e-04 | norm=0.0394 | dt=8.61ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 121/600 | loss=0.324529 | lr=6.000000e-04 | norm=0.0402 | dt=8.89ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 131/600 | loss=0.323183 | lr=6.000000e-04 | norm=0.0363 | dt=9.75ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 141/600 | loss=0.316575 | lr=6.000000e-04 | norm=0.0334 | dt=11.66ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 151/600 | loss=0.312027 | lr=6.000000e-04 | norm=0.0343 | dt=11.19ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 161/600 | loss=0.303608 | lr=6.000000e-04 | norm=0.0367 | dt=9.52ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 171/600 | loss=0.304575 | lr=6.000000e-04 | norm=0.0371 | dt=10.92ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 181/600 | loss=0.297303 | lr=6.000000e-04 | norm=0.0396 | dt=12.38ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 191/600 | loss=0.293878 | lr=6.000000e-04 | norm=0.0343 | dt=10.74ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 201/600 | loss=0.292447 | lr=6.000000e-04 | norm=0.0390 | dt=11.97ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 211/600 | loss=0.290135 | lr=6.000000e-04 | norm=0.0344 | dt=7.88ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 221/600 | loss=0.288552 | lr=6.000000e-04 | norm=0.0365 | dt=9.04ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 231/600 | loss=0.282394 | lr=6.000000e-04 | norm=0.0394 | dt=13.18ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 241/600 | loss=0.280410 | lr=6.000000e-04 | norm=0.0347 | dt=8.46ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 251/600 | loss=0.284336 | lr=6.000000e-04 | norm=0.0389 | dt=10.65ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 261/600 | loss=0.279279 | lr=6.000000e-04 | norm=0.0367 | dt=7.54ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 271/600 | loss=0.275172 | lr=6.000000e-04 | norm=0.0356 | dt=7.77ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 281/600 | loss=0.273471 | lr=6.000000e-04 | norm=0.0335 | dt=8.44ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 291/600 | loss=0.271807 | lr=6.000000e-04 | norm=0.0336 | dt=9.73ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 301/600 | loss=0.275198 | lr=6.000000e-04 | norm=0.0394 | dt=11.70ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 311/600 | loss=0.270023 | lr=6.000000e-04 | norm=0.0383 | dt=12.66ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 321/600 | loss=0.271150 | lr=6.000000e-04 | norm=0.0391 | dt=8.78ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 331/600 | loss=0.272483 | lr=6.000000e-04 | norm=0.0456 | dt=11.61ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 341/600 | loss=0.270511 | lr=6.000000e-04 | norm=0.0375 | dt=8.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 351/600 | loss=0.265920 | lr=6.000000e-04 | norm=0.0505 | dt=8.37ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 361/600 | loss=0.263010 | lr=6.000000e-04 | norm=0.0506 | dt=9.20ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 371/600 | loss=0.259236 | lr=6.000000e-04 | norm=0.0379 | dt=8.73ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 381/600 | loss=0.261815 | lr=6.000000e-04 | norm=0.0353 | dt=9.84ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 391/600 | loss=0.258226 | lr=6.000000e-04 | norm=0.0400 | dt=8.46ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 401/600 | loss=0.258738 | lr=6.000000e-04 | norm=0.0353 | dt=10.82ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 411/600 | loss=0.258200 | lr=6.000000e-04 | norm=0.0342 | dt=8.22ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 421/600 | loss=0.255602 | lr=6.000000e-04 | norm=0.0356 | dt=8.54ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 431/600 | loss=0.254558 | lr=6.000000e-04 | norm=0.0446 | dt=9.31ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 441/600 | loss=0.253810 | lr=6.000000e-04 | norm=0.0400 | dt=9.42ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 451/600 | loss=0.252500 | lr=6.000000e-04 | norm=0.0356 | dt=8.29ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 461/600 | loss=0.251838 | lr=6.000000e-04 | norm=0.0536 | dt=9.52ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 471/600 | loss=0.253251 | lr=6.000000e-04 | norm=0.0351 | dt=9.69ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 481/600 | loss=0.249283 | lr=6.000000e-04 | norm=0.0389 | dt=9.92ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 491/600 | loss=0.254225 | lr=6.000000e-04 | norm=0.0376 | dt=8.53ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 501/600 | loss=0.253163 | lr=6.000000e-04 | norm=0.0453 | dt=9.22ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 511/600 | loss=0.247502 | lr=6.000000e-04 | norm=0.0455 | dt=8.93ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 521/600 | loss=0.249150 | lr=6.000000e-04 | norm=0.0376 | dt=9.73ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 531/600 | loss=0.249010 | lr=6.000000e-04 | norm=0.0382 | dt=8.89ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 541/600 | loss=0.246295 | lr=6.000000e-04 | norm=0.0448 | dt=9.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 551/600 | loss=0.249824 | lr=6.000000e-04 | norm=0.0462 | dt=9.02ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 561/600 | loss=0.249268 | lr=6.000000e-04 | norm=0.0373 | dt=9.78ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 571/600 | loss=0.249260 | lr=6.000000e-04 | norm=0.0367 | dt=9.21ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 581/600 | loss=0.245577 | lr=6.000000e-04 | norm=0.0359 | dt=11.32ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 591/600 | loss=0.246318 | lr=6.000000e-04 | norm=0.0449 | dt=8.49ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 600/600 | loss=0.246782 | lr=6.000000e-04 | norm=0.0370 | dt=11.55ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionWithAdapterModel] step 2801/4000 | val_loss=0.2353\n",
            "[MlpRegressionModel] step 2801/4000 | loss=0.121164 | lr=1.950000e-04 | norm=0.4507 | dt=13304.12ms\n",
            "[MlpRegressionModel] step 2851/4000 | loss=0.111167 | lr=1.849291e-04 | norm=0.4352 | dt=12.50ms\n",
            "[MlpRegressionModel] step 2901/4000 | loss=0.128868 | lr=1.751344e-04 | norm=0.5401 | dt=12.59ms\n",
            "[MlpRegressionModel] step 2951/4000 | loss=0.125287 | lr=1.656344e-04 | norm=0.4358 | dt=20.56ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionModel] step 3001/4000 | val_loss=0.2505\n",
            "Resetting data loader (valid split)...\n",
            "[MlpReconstructWithAdapterAndConsistency] step   1/600 | loss=0.600190 | lr=6.000000e-04 | norm=0.0979 | dt=20.26ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  11/600 | loss=0.534498 | lr=6.000000e-04 | norm=0.0429 | dt=10.96ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  21/600 | loss=0.496595 | lr=6.000000e-04 | norm=0.0388 | dt=12.67ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  31/600 | loss=0.463804 | lr=6.000000e-04 | norm=0.0370 | dt=10.56ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  41/600 | loss=0.435251 | lr=6.000000e-04 | norm=0.0361 | dt=8.01ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  51/600 | loss=0.410609 | lr=6.000000e-04 | norm=0.0354 | dt=8.17ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  61/600 | loss=0.394003 | lr=6.000000e-04 | norm=0.0329 | dt=10.53ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  71/600 | loss=0.381262 | lr=6.000000e-04 | norm=0.0326 | dt=8.40ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  81/600 | loss=0.365088 | lr=6.000000e-04 | norm=0.0334 | dt=9.86ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  91/600 | loss=0.352263 | lr=6.000000e-04 | norm=0.0328 | dt=8.22ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 101/600 | loss=0.344317 | lr=6.000000e-04 | norm=0.0334 | dt=10.11ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 111/600 | loss=0.338328 | lr=6.000000e-04 | norm=0.0325 | dt=7.81ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 121/600 | loss=0.326605 | lr=6.000000e-04 | norm=0.0385 | dt=9.33ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 131/600 | loss=0.325199 | lr=6.000000e-04 | norm=0.0354 | dt=8.20ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 141/600 | loss=0.319084 | lr=6.000000e-04 | norm=0.0347 | dt=8.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 151/600 | loss=0.314427 | lr=6.000000e-04 | norm=0.0361 | dt=10.65ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 161/600 | loss=0.307638 | lr=6.000000e-04 | norm=0.0394 | dt=10.07ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 171/600 | loss=0.305577 | lr=6.000000e-04 | norm=0.0362 | dt=8.73ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 181/600 | loss=0.300555 | lr=6.000000e-04 | norm=0.0360 | dt=10.08ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 191/600 | loss=0.297709 | lr=6.000000e-04 | norm=0.0335 | dt=9.06ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 201/600 | loss=0.295040 | lr=6.000000e-04 | norm=0.0430 | dt=10.54ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 211/600 | loss=0.293632 | lr=6.000000e-04 | norm=0.0368 | dt=8.33ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 221/600 | loss=0.291454 | lr=6.000000e-04 | norm=0.0365 | dt=8.50ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 231/600 | loss=0.285986 | lr=6.000000e-04 | norm=0.0442 | dt=9.99ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 241/600 | loss=0.284587 | lr=6.000000e-04 | norm=0.0465 | dt=9.62ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 251/600 | loss=0.286041 | lr=6.000000e-04 | norm=0.0357 | dt=9.27ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 261/600 | loss=0.285002 | lr=6.000000e-04 | norm=0.0339 | dt=10.11ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 271/600 | loss=0.278829 | lr=6.000000e-04 | norm=0.0374 | dt=8.94ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 281/600 | loss=0.275935 | lr=6.000000e-04 | norm=0.0345 | dt=11.45ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 291/600 | loss=0.275134 | lr=6.000000e-04 | norm=0.0369 | dt=10.22ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 301/600 | loss=0.280795 | lr=6.000000e-04 | norm=0.0394 | dt=8.96ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 311/600 | loss=0.273375 | lr=6.000000e-04 | norm=0.0399 | dt=9.86ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 321/600 | loss=0.273070 | lr=6.000000e-04 | norm=0.0396 | dt=7.95ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 331/600 | loss=0.271871 | lr=6.000000e-04 | norm=0.0360 | dt=8.70ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 341/600 | loss=0.274598 | lr=6.000000e-04 | norm=0.0364 | dt=9.72ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 351/600 | loss=0.269710 | lr=6.000000e-04 | norm=0.0391 | dt=10.30ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 361/600 | loss=0.264914 | lr=6.000000e-04 | norm=0.0406 | dt=10.33ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 371/600 | loss=0.261460 | lr=6.000000e-04 | norm=0.0508 | dt=8.73ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 381/600 | loss=0.264945 | lr=6.000000e-04 | norm=0.0394 | dt=7.82ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 391/600 | loss=0.262204 | lr=6.000000e-04 | norm=0.0391 | dt=10.41ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 401/600 | loss=0.263545 | lr=6.000000e-04 | norm=0.0399 | dt=10.01ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 411/600 | loss=0.256789 | lr=6.000000e-04 | norm=0.0384 | dt=8.08ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 421/600 | loss=0.257888 | lr=6.000000e-04 | norm=0.0422 | dt=8.46ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 431/600 | loss=0.254853 | lr=6.000000e-04 | norm=0.0394 | dt=8.88ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 441/600 | loss=0.257043 | lr=6.000000e-04 | norm=0.0352 | dt=10.46ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 451/600 | loss=0.256775 | lr=6.000000e-04 | norm=0.0459 | dt=9.89ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 461/600 | loss=0.252005 | lr=6.000000e-04 | norm=0.0372 | dt=11.71ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 471/600 | loss=0.255300 | lr=6.000000e-04 | norm=0.0347 | dt=10.05ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 481/600 | loss=0.254187 | lr=6.000000e-04 | norm=0.0393 | dt=8.23ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 491/600 | loss=0.257046 | lr=6.000000e-04 | norm=0.0384 | dt=8.25ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 501/600 | loss=0.255197 | lr=6.000000e-04 | norm=0.0409 | dt=8.24ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 511/600 | loss=0.251032 | lr=6.000000e-04 | norm=0.0443 | dt=8.43ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 521/600 | loss=0.250422 | lr=6.000000e-04 | norm=0.0450 | dt=9.90ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 531/600 | loss=0.250268 | lr=6.000000e-04 | norm=0.0369 | dt=7.86ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 541/600 | loss=0.246164 | lr=6.000000e-04 | norm=0.0405 | dt=8.38ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 551/600 | loss=0.250957 | lr=6.000000e-04 | norm=0.0374 | dt=10.25ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 561/600 | loss=0.251524 | lr=6.000000e-04 | norm=0.0405 | dt=9.12ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 571/600 | loss=0.252824 | lr=6.000000e-04 | norm=0.0351 | dt=10.87ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 581/600 | loss=0.248457 | lr=6.000000e-04 | norm=0.0438 | dt=10.98ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 591/600 | loss=0.248730 | lr=6.000000e-04 | norm=0.0384 | dt=10.80ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 600/600 | loss=0.250171 | lr=6.000000e-04 | norm=0.0393 | dt=9.73ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionWithAdapterModel] step 3001/4000 | val_loss=0.2376\n",
            "[MlpRegressionModel] step 3001/4000 | loss=0.122453 | lr=1.564473e-04 | norm=0.5245 | dt=13750.62ms\n",
            "[MlpRegressionModel] step 3051/4000 | loss=0.111349 | lr=1.475906e-04 | norm=0.4828 | dt=14.45ms\n",
            "[MlpRegressionModel] step 3101/4000 | loss=0.104084 | lr=1.390812e-04 | norm=0.4486 | dt=25.18ms\n",
            "Restarting data loader (train split)...\n",
            "[MlpRegressionModel] step 3151/4000 | loss=0.103065 | lr=1.309351e-04 | norm=0.4761 | dt=19.30ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionModel] step 3201/4000 | val_loss=0.2601\n",
            "Resetting data loader (valid split)...\n",
            "[MlpReconstructWithAdapterAndConsistency] step   1/600 | loss=0.607907 | lr=6.000000e-04 | norm=0.1053 | dt=17.35ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  11/600 | loss=0.535006 | lr=6.000000e-04 | norm=0.0430 | dt=10.58ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  21/600 | loss=0.500932 | lr=6.000000e-04 | norm=0.0381 | dt=8.80ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  31/600 | loss=0.467433 | lr=6.000000e-04 | norm=0.0373 | dt=8.29ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  41/600 | loss=0.439360 | lr=6.000000e-04 | norm=0.0355 | dt=8.16ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  51/600 | loss=0.413630 | lr=6.000000e-04 | norm=0.0341 | dt=10.58ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  61/600 | loss=0.399548 | lr=6.000000e-04 | norm=0.0336 | dt=8.91ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  71/600 | loss=0.385927 | lr=6.000000e-04 | norm=0.0345 | dt=9.33ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  81/600 | loss=0.368576 | lr=6.000000e-04 | norm=0.0339 | dt=8.24ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  91/600 | loss=0.355331 | lr=6.000000e-04 | norm=0.0359 | dt=7.94ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 101/600 | loss=0.346840 | lr=6.000000e-04 | norm=0.0380 | dt=11.90ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 111/600 | loss=0.340644 | lr=6.000000e-04 | norm=0.0363 | dt=9.57ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 121/600 | loss=0.329589 | lr=6.000000e-04 | norm=0.0356 | dt=10.66ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 131/600 | loss=0.328026 | lr=6.000000e-04 | norm=0.0354 | dt=10.07ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 141/600 | loss=0.321944 | lr=6.000000e-04 | norm=0.0357 | dt=10.36ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 151/600 | loss=0.315990 | lr=6.000000e-04 | norm=0.0367 | dt=7.43ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 161/600 | loss=0.309755 | lr=6.000000e-04 | norm=0.0352 | dt=8.40ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 171/600 | loss=0.306535 | lr=6.000000e-04 | norm=0.0344 | dt=8.29ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 181/600 | loss=0.302634 | lr=6.000000e-04 | norm=0.0411 | dt=10.09ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 191/600 | loss=0.298224 | lr=6.000000e-04 | norm=0.0345 | dt=7.84ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 201/600 | loss=0.295911 | lr=6.000000e-04 | norm=0.0353 | dt=9.50ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 211/600 | loss=0.294851 | lr=6.000000e-04 | norm=0.0382 | dt=9.37ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 221/600 | loss=0.294435 | lr=6.000000e-04 | norm=0.0337 | dt=9.48ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 231/600 | loss=0.285934 | lr=6.000000e-04 | norm=0.0424 | dt=8.23ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 241/600 | loss=0.287009 | lr=6.000000e-04 | norm=0.0389 | dt=8.76ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 251/600 | loss=0.287453 | lr=6.000000e-04 | norm=0.0520 | dt=7.07ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 261/600 | loss=0.285503 | lr=6.000000e-04 | norm=0.0380 | dt=11.52ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 271/600 | loss=0.281417 | lr=6.000000e-04 | norm=0.0355 | dt=7.80ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 281/600 | loss=0.277476 | lr=6.000000e-04 | norm=0.0422 | dt=9.96ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 291/600 | loss=0.276415 | lr=6.000000e-04 | norm=0.0357 | dt=7.79ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 301/600 | loss=0.278379 | lr=6.000000e-04 | norm=0.0407 | dt=9.67ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 311/600 | loss=0.274092 | lr=6.000000e-04 | norm=0.0412 | dt=10.18ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 321/600 | loss=0.274757 | lr=6.000000e-04 | norm=0.0366 | dt=9.58ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 331/600 | loss=0.274284 | lr=6.000000e-04 | norm=0.0433 | dt=9.74ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 341/600 | loss=0.274928 | lr=6.000000e-04 | norm=0.0376 | dt=10.69ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 351/600 | loss=0.270421 | lr=6.000000e-04 | norm=0.0399 | dt=8.55ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 361/600 | loss=0.266285 | lr=6.000000e-04 | norm=0.0369 | dt=7.95ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 371/600 | loss=0.264111 | lr=6.000000e-04 | norm=0.0393 | dt=10.59ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 381/600 | loss=0.263861 | lr=6.000000e-04 | norm=0.0377 | dt=9.40ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 391/600 | loss=0.262315 | lr=6.000000e-04 | norm=0.0358 | dt=13.26ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 401/600 | loss=0.264603 | lr=6.000000e-04 | norm=0.0404 | dt=10.79ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 411/600 | loss=0.259432 | lr=6.000000e-04 | norm=0.0349 | dt=8.81ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 421/600 | loss=0.259743 | lr=6.000000e-04 | norm=0.0410 | dt=8.95ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 431/600 | loss=0.257976 | lr=6.000000e-04 | norm=0.0401 | dt=10.91ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 441/600 | loss=0.256028 | lr=6.000000e-04 | norm=0.0347 | dt=9.88ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 451/600 | loss=0.256305 | lr=6.000000e-04 | norm=0.0414 | dt=9.80ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 461/600 | loss=0.255435 | lr=6.000000e-04 | norm=0.0345 | dt=8.62ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 471/600 | loss=0.256728 | lr=6.000000e-04 | norm=0.0350 | dt=9.55ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 481/600 | loss=0.253431 | lr=6.000000e-04 | norm=0.0429 | dt=8.14ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 491/600 | loss=0.258070 | lr=6.000000e-04 | norm=0.0425 | dt=9.21ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 501/600 | loss=0.256532 | lr=6.000000e-04 | norm=0.0362 | dt=7.49ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 511/600 | loss=0.252601 | lr=6.000000e-04 | norm=0.0415 | dt=9.35ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 521/600 | loss=0.254096 | lr=6.000000e-04 | norm=0.0387 | dt=8.54ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 531/600 | loss=0.251678 | lr=6.000000e-04 | norm=0.0395 | dt=10.20ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 541/600 | loss=0.248861 | lr=6.000000e-04 | norm=0.0422 | dt=9.99ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 551/600 | loss=0.252098 | lr=6.000000e-04 | norm=0.0398 | dt=8.42ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 561/600 | loss=0.252739 | lr=6.000000e-04 | norm=0.0400 | dt=10.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 571/600 | loss=0.252849 | lr=6.000000e-04 | norm=0.0358 | dt=9.61ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 581/600 | loss=0.249022 | lr=6.000000e-04 | norm=0.0399 | dt=10.59ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 591/600 | loss=0.250178 | lr=6.000000e-04 | norm=0.0386 | dt=10.92ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 600/600 | loss=0.250706 | lr=6.000000e-04 | norm=0.0360 | dt=11.69ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionWithAdapterModel] step 3201/4000 | val_loss=0.2470\n",
            "[MlpRegressionModel] step 3201/4000 | loss=0.100067 | lr=1.231680e-04 | norm=0.4155 | dt=13096.00ms\n",
            "[MlpRegressionModel] step 3251/4000 | loss=0.105884 | lr=1.157946e-04 | norm=0.4813 | dt=15.34ms\n",
            "[MlpRegressionModel] step 3301/4000 | loss=0.107485 | lr=1.088289e-04 | norm=0.4668 | dt=19.02ms\n",
            "[MlpRegressionModel] step 3351/4000 | loss=0.099785 | lr=1.022843e-04 | norm=0.4509 | dt=24.00ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionModel] step 3401/4000 | val_loss=0.2647\n",
            "Resetting data loader (valid split)...\n",
            "[MlpReconstructWithAdapterAndConsistency] step   1/600 | loss=0.607199 | lr=6.000000e-04 | norm=0.1079 | dt=17.49ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  11/600 | loss=0.534476 | lr=6.000000e-04 | norm=0.0434 | dt=10.72ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  21/600 | loss=0.499102 | lr=6.000000e-04 | norm=0.0387 | dt=11.73ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  31/600 | loss=0.467523 | lr=6.000000e-04 | norm=0.0369 | dt=9.59ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  41/600 | loss=0.440266 | lr=6.000000e-04 | norm=0.0363 | dt=10.57ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  51/600 | loss=0.416628 | lr=6.000000e-04 | norm=0.0340 | dt=10.81ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  61/600 | loss=0.398039 | lr=6.000000e-04 | norm=0.0338 | dt=10.94ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  71/600 | loss=0.384660 | lr=6.000000e-04 | norm=0.0325 | dt=9.08ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  81/600 | loss=0.368541 | lr=6.000000e-04 | norm=0.0344 | dt=9.33ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  91/600 | loss=0.357132 | lr=6.000000e-04 | norm=0.0327 | dt=7.83ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 101/600 | loss=0.348344 | lr=6.000000e-04 | norm=0.0366 | dt=8.32ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 111/600 | loss=0.341525 | lr=6.000000e-04 | norm=0.0332 | dt=11.04ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 121/600 | loss=0.331994 | lr=6.000000e-04 | norm=0.0334 | dt=9.67ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 131/600 | loss=0.331071 | lr=6.000000e-04 | norm=0.0347 | dt=12.91ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 141/600 | loss=0.325081 | lr=6.000000e-04 | norm=0.0352 | dt=8.59ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 151/600 | loss=0.316734 | lr=6.000000e-04 | norm=0.0363 | dt=10.95ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 161/600 | loss=0.313053 | lr=6.000000e-04 | norm=0.0346 | dt=10.57ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 171/600 | loss=0.308241 | lr=6.000000e-04 | norm=0.0383 | dt=9.36ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 181/600 | loss=0.304428 | lr=6.000000e-04 | norm=0.0379 | dt=10.34ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 191/600 | loss=0.301740 | lr=6.000000e-04 | norm=0.0359 | dt=8.48ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 201/600 | loss=0.297711 | lr=6.000000e-04 | norm=0.0381 | dt=10.11ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 211/600 | loss=0.296112 | lr=6.000000e-04 | norm=0.0350 | dt=8.61ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 221/600 | loss=0.292732 | lr=6.000000e-04 | norm=0.0339 | dt=8.07ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 231/600 | loss=0.289479 | lr=6.000000e-04 | norm=0.0394 | dt=10.55ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 241/600 | loss=0.289217 | lr=6.000000e-04 | norm=0.0389 | dt=7.98ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 251/600 | loss=0.289089 | lr=6.000000e-04 | norm=0.0382 | dt=8.09ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 261/600 | loss=0.285008 | lr=6.000000e-04 | norm=0.0355 | dt=9.46ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 271/600 | loss=0.281914 | lr=6.000000e-04 | norm=0.0333 | dt=8.22ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 281/600 | loss=0.278535 | lr=6.000000e-04 | norm=0.0362 | dt=8.19ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 291/600 | loss=0.277843 | lr=6.000000e-04 | norm=0.0373 | dt=10.53ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 301/600 | loss=0.281501 | lr=6.000000e-04 | norm=0.0363 | dt=11.01ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 311/600 | loss=0.275568 | lr=6.000000e-04 | norm=0.0370 | dt=10.10ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 321/600 | loss=0.277812 | lr=6.000000e-04 | norm=0.0387 | dt=8.16ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 331/600 | loss=0.278517 | lr=6.000000e-04 | norm=0.0406 | dt=9.86ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 341/600 | loss=0.277759 | lr=6.000000e-04 | norm=0.0361 | dt=9.59ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 351/600 | loss=0.272605 | lr=6.000000e-04 | norm=0.0366 | dt=9.31ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 361/600 | loss=0.269499 | lr=6.000000e-04 | norm=0.0423 | dt=10.02ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 371/600 | loss=0.263819 | lr=6.000000e-04 | norm=0.0341 | dt=8.39ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 381/600 | loss=0.268056 | lr=6.000000e-04 | norm=0.0348 | dt=7.75ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 391/600 | loss=0.264667 | lr=6.000000e-04 | norm=0.0337 | dt=8.98ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 401/600 | loss=0.267764 | lr=6.000000e-04 | norm=0.0356 | dt=8.36ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 411/600 | loss=0.262038 | lr=6.000000e-04 | norm=0.0420 | dt=10.45ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 421/600 | loss=0.261146 | lr=6.000000e-04 | norm=0.0379 | dt=10.12ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 431/600 | loss=0.258667 | lr=6.000000e-04 | norm=0.0387 | dt=8.41ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 441/600 | loss=0.259416 | lr=6.000000e-04 | norm=0.0374 | dt=9.16ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 451/600 | loss=0.260470 | lr=6.000000e-04 | norm=0.0408 | dt=10.58ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 461/600 | loss=0.257145 | lr=6.000000e-04 | norm=0.0403 | dt=8.53ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 471/600 | loss=0.256527 | lr=6.000000e-04 | norm=0.0405 | dt=8.97ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 481/600 | loss=0.255925 | lr=6.000000e-04 | norm=0.0337 | dt=7.94ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 491/600 | loss=0.259971 | lr=6.000000e-04 | norm=0.0443 | dt=9.09ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 501/600 | loss=0.258013 | lr=6.000000e-04 | norm=0.0358 | dt=7.72ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 511/600 | loss=0.253391 | lr=6.000000e-04 | norm=0.0379 | dt=9.11ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 521/600 | loss=0.255610 | lr=6.000000e-04 | norm=0.0382 | dt=8.35ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 531/600 | loss=0.255059 | lr=6.000000e-04 | norm=0.0361 | dt=9.29ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 541/600 | loss=0.250445 | lr=6.000000e-04 | norm=0.0378 | dt=8.53ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 551/600 | loss=0.255453 | lr=6.000000e-04 | norm=0.0425 | dt=10.03ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 561/600 | loss=0.253160 | lr=6.000000e-04 | norm=0.0354 | dt=9.03ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 571/600 | loss=0.253558 | lr=6.000000e-04 | norm=0.0376 | dt=7.66ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 581/600 | loss=0.251852 | lr=6.000000e-04 | norm=0.0464 | dt=10.86ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 591/600 | loss=0.254052 | lr=6.000000e-04 | norm=0.0385 | dt=11.83ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 600/600 | loss=0.252332 | lr=6.000000e-04 | norm=0.0364 | dt=8.72ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionWithAdapterModel] step 3401/4000 | val_loss=0.2486\n",
            "[MlpRegressionModel] step 3401/4000 | loss=0.096970 | lr=9.617314e-05 | norm=0.4085 | dt=13264.67ms\n",
            "[MlpRegressionModel] step 3451/4000 | loss=0.100939 | lr=9.050708e-05 | norm=0.4540 | dt=20.10ms\n",
            "[MlpRegressionModel] step 3501/4000 | loss=0.093476 | lr=8.529690e-05 | norm=0.4086 | dt=18.06ms\n",
            "[MlpRegressionModel] step 3551/4000 | loss=0.104297 | lr=8.055253e-05 | norm=0.4597 | dt=12.66ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionModel] step 3601/4000 | val_loss=0.2655\n",
            "Resetting data loader (valid split)...\n",
            "[MlpReconstructWithAdapterAndConsistency] step   1/600 | loss=0.609293 | lr=6.000000e-04 | norm=0.1179 | dt=15.97ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  11/600 | loss=0.535933 | lr=6.000000e-04 | norm=0.0425 | dt=10.70ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  21/600 | loss=0.499168 | lr=6.000000e-04 | norm=0.0377 | dt=8.86ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  31/600 | loss=0.470750 | lr=6.000000e-04 | norm=0.0359 | dt=10.10ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  41/600 | loss=0.444707 | lr=6.000000e-04 | norm=0.0345 | dt=8.79ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  51/600 | loss=0.418021 | lr=6.000000e-04 | norm=0.0337 | dt=8.69ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  61/600 | loss=0.397427 | lr=6.000000e-04 | norm=0.0335 | dt=9.23ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  71/600 | loss=0.388043 | lr=6.000000e-04 | norm=0.0376 | dt=9.97ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  81/600 | loss=0.369563 | lr=6.000000e-04 | norm=0.0396 | dt=7.81ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  91/600 | loss=0.361312 | lr=6.000000e-04 | norm=0.0348 | dt=10.70ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 101/600 | loss=0.351989 | lr=6.000000e-04 | norm=0.0330 | dt=9.61ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 111/600 | loss=0.345569 | lr=6.000000e-04 | norm=0.0372 | dt=7.93ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 121/600 | loss=0.334821 | lr=6.000000e-04 | norm=0.0353 | dt=7.34ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 131/600 | loss=0.333123 | lr=6.000000e-04 | norm=0.0341 | dt=9.88ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 141/600 | loss=0.324531 | lr=6.000000e-04 | norm=0.0360 | dt=9.57ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 151/600 | loss=0.318735 | lr=6.000000e-04 | norm=0.0403 | dt=9.02ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 161/600 | loss=0.313223 | lr=6.000000e-04 | norm=0.0337 | dt=9.53ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 171/600 | loss=0.312500 | lr=6.000000e-04 | norm=0.0362 | dt=8.07ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 181/600 | loss=0.307020 | lr=6.000000e-04 | norm=0.0363 | dt=10.20ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 191/600 | loss=0.303228 | lr=6.000000e-04 | norm=0.0354 | dt=9.04ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 201/600 | loss=0.301238 | lr=6.000000e-04 | norm=0.0364 | dt=9.98ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 211/600 | loss=0.296974 | lr=6.000000e-04 | norm=0.0393 | dt=8.24ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 221/600 | loss=0.295268 | lr=6.000000e-04 | norm=0.0346 | dt=8.93ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 231/600 | loss=0.290967 | lr=6.000000e-04 | norm=0.0377 | dt=10.41ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 241/600 | loss=0.288587 | lr=6.000000e-04 | norm=0.0514 | dt=9.36ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 251/600 | loss=0.291174 | lr=6.000000e-04 | norm=0.0388 | dt=7.91ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 261/600 | loss=0.288073 | lr=6.000000e-04 | norm=0.0369 | dt=10.32ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 271/600 | loss=0.283274 | lr=6.000000e-04 | norm=0.0361 | dt=8.29ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 281/600 | loss=0.281345 | lr=6.000000e-04 | norm=0.0378 | dt=11.33ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 291/600 | loss=0.281027 | lr=6.000000e-04 | norm=0.0434 | dt=12.28ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 301/600 | loss=0.283466 | lr=6.000000e-04 | norm=0.0390 | dt=10.40ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 311/600 | loss=0.277214 | lr=6.000000e-04 | norm=0.0378 | dt=8.82ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 321/600 | loss=0.279272 | lr=6.000000e-04 | norm=0.0412 | dt=8.45ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 331/600 | loss=0.278442 | lr=6.000000e-04 | norm=0.0493 | dt=10.69ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 341/600 | loss=0.276812 | lr=6.000000e-04 | norm=0.0371 | dt=8.38ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 351/600 | loss=0.273655 | lr=6.000000e-04 | norm=0.0516 | dt=9.27ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 361/600 | loss=0.271640 | lr=6.000000e-04 | norm=0.0376 | dt=8.76ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 371/600 | loss=0.264442 | lr=6.000000e-04 | norm=0.0345 | dt=10.78ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 381/600 | loss=0.268994 | lr=6.000000e-04 | norm=0.0411 | dt=7.80ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 391/600 | loss=0.266317 | lr=6.000000e-04 | norm=0.0377 | dt=9.57ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 401/600 | loss=0.269864 | lr=6.000000e-04 | norm=0.0403 | dt=8.24ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 411/600 | loss=0.264465 | lr=6.000000e-04 | norm=0.0428 | dt=10.22ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 421/600 | loss=0.263559 | lr=6.000000e-04 | norm=0.0405 | dt=8.07ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 431/600 | loss=0.261825 | lr=6.000000e-04 | norm=0.0491 | dt=9.77ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 441/600 | loss=0.257272 | lr=6.000000e-04 | norm=0.0421 | dt=8.65ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 451/600 | loss=0.259737 | lr=6.000000e-04 | norm=0.0437 | dt=11.56ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 461/600 | loss=0.259109 | lr=6.000000e-04 | norm=0.0435 | dt=9.21ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 471/600 | loss=0.260671 | lr=6.000000e-04 | norm=0.0365 | dt=8.18ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 481/600 | loss=0.257728 | lr=6.000000e-04 | norm=0.0435 | dt=9.16ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 491/600 | loss=0.261185 | lr=6.000000e-04 | norm=0.0455 | dt=10.56ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 501/600 | loss=0.259999 | lr=6.000000e-04 | norm=0.0418 | dt=8.36ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 511/600 | loss=0.255634 | lr=6.000000e-04 | norm=0.0409 | dt=9.14ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 521/600 | loss=0.254569 | lr=6.000000e-04 | norm=0.0395 | dt=8.61ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 531/600 | loss=0.257647 | lr=6.000000e-04 | norm=0.0380 | dt=10.18ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 541/600 | loss=0.253543 | lr=6.000000e-04 | norm=0.0435 | dt=8.13ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 551/600 | loss=0.253748 | lr=6.000000e-04 | norm=0.0355 | dt=11.53ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 561/600 | loss=0.253252 | lr=6.000000e-04 | norm=0.0380 | dt=156.93ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 571/600 | loss=0.256715 | lr=6.000000e-04 | norm=0.0363 | dt=8.05ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 581/600 | loss=0.252526 | lr=6.000000e-04 | norm=0.0365 | dt=8.39ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 591/600 | loss=0.254656 | lr=6.000000e-04 | norm=0.0374 | dt=8.47ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 600/600 | loss=0.254426 | lr=6.000000e-04 | norm=0.0437 | dt=7.87ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionWithAdapterModel] step 3601/4000 | val_loss=0.2469\n",
            "[MlpRegressionModel] step 3601/4000 | loss=0.096152 | lr=7.628299e-05 | norm=0.4262 | dt=13188.03ms\n",
            "[MlpRegressionModel] step 3651/4000 | loss=0.101412 | lr=7.249642e-05 | norm=0.4473 | dt=19.38ms\n",
            "Restarting data loader (train split)...\n",
            "[MlpRegressionModel] step 3701/4000 | loss=0.084325 | lr=6.920003e-05 | norm=0.4165 | dt=17.56ms\n",
            "[MlpRegressionModel] step 3751/4000 | loss=0.089580 | lr=6.640008e-05 | norm=0.4028 | dt=13.05ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionModel] step 3801/4000 | val_loss=0.2692\n",
            "Resetting data loader (valid split)...\n",
            "[MlpReconstructWithAdapterAndConsistency] step   1/600 | loss=0.602037 | lr=6.000000e-04 | norm=0.1000 | dt=18.11ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  11/600 | loss=0.533563 | lr=6.000000e-04 | norm=0.0422 | dt=8.07ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  21/600 | loss=0.499656 | lr=6.000000e-04 | norm=0.0371 | dt=10.20ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  31/600 | loss=0.471465 | lr=6.000000e-04 | norm=0.0358 | dt=10.39ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  41/600 | loss=0.441238 | lr=6.000000e-04 | norm=0.0352 | dt=8.66ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  51/600 | loss=0.417135 | lr=6.000000e-04 | norm=0.0333 | dt=8.45ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  61/600 | loss=0.399924 | lr=6.000000e-04 | norm=0.0337 | dt=10.67ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  71/600 | loss=0.386809 | lr=6.000000e-04 | norm=0.0331 | dt=9.46ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  81/600 | loss=0.371059 | lr=6.000000e-04 | norm=0.0358 | dt=10.05ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  91/600 | loss=0.361188 | lr=6.000000e-04 | norm=0.0350 | dt=10.73ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 101/600 | loss=0.349786 | lr=6.000000e-04 | norm=0.0329 | dt=9.71ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 111/600 | loss=0.347431 | lr=6.000000e-04 | norm=0.0344 | dt=9.68ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 121/600 | loss=0.333965 | lr=6.000000e-04 | norm=0.0337 | dt=8.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 131/600 | loss=0.331028 | lr=6.000000e-04 | norm=0.0338 | dt=8.27ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 141/600 | loss=0.324807 | lr=6.000000e-04 | norm=0.0356 | dt=8.41ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 151/600 | loss=0.320993 | lr=6.000000e-04 | norm=0.0405 | dt=8.11ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 161/600 | loss=0.314701 | lr=6.000000e-04 | norm=0.0400 | dt=10.20ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 171/600 | loss=0.313483 | lr=6.000000e-04 | norm=0.0414 | dt=10.85ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 181/600 | loss=0.307982 | lr=6.000000e-04 | norm=0.0412 | dt=11.07ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 191/600 | loss=0.303459 | lr=6.000000e-04 | norm=0.0375 | dt=8.48ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 201/600 | loss=0.300994 | lr=6.000000e-04 | norm=0.0357 | dt=9.19ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 211/600 | loss=0.297621 | lr=6.000000e-04 | norm=0.0475 | dt=10.32ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 221/600 | loss=0.297615 | lr=6.000000e-04 | norm=0.0427 | dt=8.70ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 231/600 | loss=0.292729 | lr=6.000000e-04 | norm=0.0440 | dt=10.52ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 241/600 | loss=0.290664 | lr=6.000000e-04 | norm=0.0384 | dt=10.21ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 251/600 | loss=0.292426 | lr=6.000000e-04 | norm=0.0392 | dt=8.04ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 261/600 | loss=0.288121 | lr=6.000000e-04 | norm=0.0385 | dt=8.68ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 271/600 | loss=0.284577 | lr=6.000000e-04 | norm=0.0389 | dt=8.87ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 281/600 | loss=0.281059 | lr=6.000000e-04 | norm=0.0433 | dt=9.88ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 291/600 | loss=0.282625 | lr=6.000000e-04 | norm=0.0384 | dt=11.78ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 301/600 | loss=0.284444 | lr=6.000000e-04 | norm=0.0365 | dt=10.40ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 311/600 | loss=0.279915 | lr=6.000000e-04 | norm=0.0455 | dt=9.45ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 321/600 | loss=0.278966 | lr=6.000000e-04 | norm=0.0378 | dt=10.29ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 331/600 | loss=0.280868 | lr=6.000000e-04 | norm=0.0411 | dt=9.14ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 341/600 | loss=0.277722 | lr=6.000000e-04 | norm=0.0400 | dt=10.86ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 351/600 | loss=0.272939 | lr=6.000000e-04 | norm=0.0359 | dt=10.10ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 361/600 | loss=0.270826 | lr=6.000000e-04 | norm=0.0395 | dt=8.87ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 371/600 | loss=0.268888 | lr=6.000000e-04 | norm=0.0401 | dt=10.50ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 381/600 | loss=0.271494 | lr=6.000000e-04 | norm=0.0405 | dt=8.20ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 391/600 | loss=0.267176 | lr=6.000000e-04 | norm=0.0381 | dt=8.68ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 401/600 | loss=0.270069 | lr=6.000000e-04 | norm=0.0439 | dt=9.49ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 411/600 | loss=0.262118 | lr=6.000000e-04 | norm=0.0373 | dt=8.74ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 421/600 | loss=0.263212 | lr=6.000000e-04 | norm=0.0351 | dt=11.50ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 431/600 | loss=0.260855 | lr=6.000000e-04 | norm=0.0431 | dt=9.37ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 441/600 | loss=0.262333 | lr=6.000000e-04 | norm=0.0364 | dt=9.06ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 451/600 | loss=0.261449 | lr=6.000000e-04 | norm=0.0396 | dt=10.11ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 461/600 | loss=0.259156 | lr=6.000000e-04 | norm=0.0478 | dt=9.10ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 471/600 | loss=0.260854 | lr=6.000000e-04 | norm=0.0408 | dt=13.16ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 481/600 | loss=0.257683 | lr=6.000000e-04 | norm=0.0376 | dt=11.28ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 491/600 | loss=0.262032 | lr=6.000000e-04 | norm=0.0381 | dt=11.54ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 501/600 | loss=0.260759 | lr=6.000000e-04 | norm=0.0391 | dt=9.85ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 511/600 | loss=0.255293 | lr=6.000000e-04 | norm=0.0418 | dt=10.85ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 521/600 | loss=0.256048 | lr=6.000000e-04 | norm=0.0424 | dt=7.41ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 531/600 | loss=0.258346 | lr=6.000000e-04 | norm=0.0440 | dt=13.87ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 541/600 | loss=0.252956 | lr=6.000000e-04 | norm=0.0446 | dt=8.55ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 551/600 | loss=0.254653 | lr=6.000000e-04 | norm=0.0378 | dt=10.79ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 561/600 | loss=0.255319 | lr=6.000000e-04 | norm=0.0415 | dt=7.74ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 571/600 | loss=0.257931 | lr=6.000000e-04 | norm=0.0391 | dt=10.44ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 581/600 | loss=0.253102 | lr=6.000000e-04 | norm=0.0412 | dt=7.72ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 591/600 | loss=0.256796 | lr=6.000000e-04 | norm=0.0456 | dt=11.11ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 600/600 | loss=0.258721 | lr=6.000000e-04 | norm=0.0496 | dt=9.80ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionWithAdapterModel] step 3801/4000 | val_loss=0.2521\n",
            "[MlpRegressionModel] step 3801/4000 | loss=0.084020 | lr=6.410191e-05 | norm=0.4898 | dt=13075.15ms\n",
            "[MlpRegressionModel] step 3851/4000 | loss=0.102779 | lr=6.230989e-05 | norm=0.4491 | dt=15.76ms\n",
            "[MlpRegressionModel] step 3901/4000 | loss=0.089531 | lr=6.102743e-05 | norm=0.3843 | dt=16.43ms\n",
            "[MlpRegressionModel] step 3951/4000 | loss=0.096183 | lr=6.025698e-05 | norm=0.4343 | dt=21.49ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionModel] step 4000/4000 | val_loss=0.2729\n",
            "Resetting data loader (valid split)...\n",
            "[MlpReconstructWithAdapterAndConsistency] step   1/600 | loss=0.596325 | lr=6.000000e-04 | norm=0.0970 | dt=17.75ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  11/600 | loss=0.533090 | lr=6.000000e-04 | norm=0.0413 | dt=11.29ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  21/600 | loss=0.499502 | lr=6.000000e-04 | norm=0.0378 | dt=9.95ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  31/600 | loss=0.469549 | lr=6.000000e-04 | norm=0.0360 | dt=10.34ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  41/600 | loss=0.439163 | lr=6.000000e-04 | norm=0.0345 | dt=11.54ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  51/600 | loss=0.414513 | lr=6.000000e-04 | norm=0.0377 | dt=11.63ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  61/600 | loss=0.400221 | lr=6.000000e-04 | norm=0.0334 | dt=10.99ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  71/600 | loss=0.385180 | lr=6.000000e-04 | norm=0.0333 | dt=9.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  81/600 | loss=0.369090 | lr=6.000000e-04 | norm=0.0323 | dt=11.44ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step  91/600 | loss=0.358279 | lr=6.000000e-04 | norm=0.0324 | dt=9.12ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 101/600 | loss=0.350787 | lr=6.000000e-04 | norm=0.0353 | dt=8.72ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 111/600 | loss=0.347347 | lr=6.000000e-04 | norm=0.0353 | dt=9.82ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 121/600 | loss=0.332117 | lr=6.000000e-04 | norm=0.0346 | dt=8.49ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 131/600 | loss=0.330547 | lr=6.000000e-04 | norm=0.0327 | dt=9.99ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 141/600 | loss=0.325391 | lr=6.000000e-04 | norm=0.0351 | dt=8.94ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 151/600 | loss=0.319015 | lr=6.000000e-04 | norm=0.0342 | dt=10.92ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 161/600 | loss=0.312758 | lr=6.000000e-04 | norm=0.0322 | dt=8.09ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 171/600 | loss=0.311467 | lr=6.000000e-04 | norm=0.0386 | dt=11.13ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 181/600 | loss=0.307125 | lr=6.000000e-04 | norm=0.0464 | dt=10.86ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 191/600 | loss=0.304021 | lr=6.000000e-04 | norm=0.0372 | dt=9.78ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 201/600 | loss=0.300663 | lr=6.000000e-04 | norm=0.0355 | dt=8.72ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 211/600 | loss=0.296046 | lr=6.000000e-04 | norm=0.0403 | dt=10.39ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 221/600 | loss=0.295213 | lr=6.000000e-04 | norm=0.0355 | dt=8.06ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 231/600 | loss=0.291085 | lr=6.000000e-04 | norm=0.0465 | dt=9.22ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 241/600 | loss=0.290394 | lr=6.000000e-04 | norm=0.0335 | dt=10.08ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 251/600 | loss=0.293898 | lr=6.000000e-04 | norm=0.0467 | dt=8.40ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 261/600 | loss=0.288222 | lr=6.000000e-04 | norm=0.0409 | dt=8.68ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 271/600 | loss=0.282598 | lr=6.000000e-04 | norm=0.0383 | dt=8.49ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 281/600 | loss=0.282379 | lr=6.000000e-04 | norm=0.0396 | dt=10.05ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 291/600 | loss=0.280432 | lr=6.000000e-04 | norm=0.0343 | dt=9.12ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 301/600 | loss=0.283501 | lr=6.000000e-04 | norm=0.0346 | dt=9.09ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 311/600 | loss=0.279798 | lr=6.000000e-04 | norm=0.0364 | dt=10.40ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 321/600 | loss=0.279755 | lr=6.000000e-04 | norm=0.0370 | dt=10.29ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 331/600 | loss=0.279501 | lr=6.000000e-04 | norm=0.0381 | dt=7.69ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 341/600 | loss=0.278829 | lr=6.000000e-04 | norm=0.0407 | dt=8.76ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 351/600 | loss=0.274013 | lr=6.000000e-04 | norm=0.0404 | dt=10.47ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 361/600 | loss=0.274316 | lr=6.000000e-04 | norm=0.0465 | dt=8.34ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 371/600 | loss=0.268163 | lr=6.000000e-04 | norm=0.0375 | dt=7.81ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 381/600 | loss=0.272865 | lr=6.000000e-04 | norm=0.0355 | dt=8.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 391/600 | loss=0.266045 | lr=6.000000e-04 | norm=0.0353 | dt=11.05ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 401/600 | loss=0.267324 | lr=6.000000e-04 | norm=0.0336 | dt=8.48ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 411/600 | loss=0.262774 | lr=6.000000e-04 | norm=0.0376 | dt=9.68ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 421/600 | loss=0.263328 | lr=6.000000e-04 | norm=0.0424 | dt=10.96ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 431/600 | loss=0.262988 | lr=6.000000e-04 | norm=0.0443 | dt=10.38ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 441/600 | loss=0.261973 | lr=6.000000e-04 | norm=0.0426 | dt=8.27ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 451/600 | loss=0.262091 | lr=6.000000e-04 | norm=0.0428 | dt=8.23ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 461/600 | loss=0.257360 | lr=6.000000e-04 | norm=0.0439 | dt=7.88ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 471/600 | loss=0.260257 | lr=6.000000e-04 | norm=0.0358 | dt=8.16ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 481/600 | loss=0.257792 | lr=6.000000e-04 | norm=0.0393 | dt=7.60ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 491/600 | loss=0.260841 | lr=6.000000e-04 | norm=0.0403 | dt=9.52ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 501/600 | loss=0.262795 | lr=6.000000e-04 | norm=0.0390 | dt=9.02ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 511/600 | loss=0.253965 | lr=6.000000e-04 | norm=0.0367 | dt=9.19ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 521/600 | loss=0.254880 | lr=6.000000e-04 | norm=0.0358 | dt=10.10ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 531/600 | loss=0.256178 | lr=6.000000e-04 | norm=0.0376 | dt=9.05ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 541/600 | loss=0.253846 | lr=6.000000e-04 | norm=0.0427 | dt=8.33ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 551/600 | loss=0.255415 | lr=6.000000e-04 | norm=0.0366 | dt=9.75ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 561/600 | loss=0.254863 | lr=6.000000e-04 | norm=0.0381 | dt=8.62ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 571/600 | loss=0.258695 | lr=6.000000e-04 | norm=0.0365 | dt=8.95ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 581/600 | loss=0.252096 | lr=6.000000e-04 | norm=0.0426 | dt=8.62ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 591/600 | loss=0.254927 | lr=6.000000e-04 | norm=0.0404 | dt=10.41ms\n",
            "[MlpReconstructWithAdapterAndConsistency] step 600/600 | loss=0.258029 | lr=6.000000e-04 | norm=0.0409 | dt=11.39ms\n",
            "Resetting data loader (valid split)...\n",
            "[MlpRegressionWithAdapterModel] step 4000/4000 | val_loss=0.2587\n",
            "[MlpRegressionModel] step 4000/4000 | loss=0.093675 | lr=6.000010e-05 | norm=0.4114 | dt=13292.38ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.arange(len(train_losses)), train_losses)\n",
        "plt.scatter(np.arange(len(valid_losses))*val_loss_n_steps, valid_losses, color='red', zorder=3, s=10)\n",
        "plt.scatter(np.arange(len(valid_losses_with_adapter))*val_loss_n_steps, valid_losses_with_adapter, color='green', zorder=3, s=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xwIpu32bdJFY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "outputId": "1a40b060-88c6-484f-d919-09862132c943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAp8AAAGvCAYAAAAder44AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVo5JREFUeJzt3Xd4VFX+x/HPpCeQAoQkBEIndEKTEJogkaKLdRXLAqJiWVwLKogFrDTL2ntb/a0iunaRIoiIQigSeq8BklBTKKlzf3+E3GTITAqZzCTh/XqePE7u3Dv3zHVCPjn3nO+xGIZhCAAAAHABD3c3AAAAABcOwicAAABchvAJAAAAlyF8AgAAwGUInwAAAHAZwicAAABchvAJAAAAlyF8AgAAwGUInwAAAHAZwicAAABcxquiByxdulTPP/+81qxZo+TkZH3zzTe66qqrHO7/9ddf66233lJiYqKys7PVsWNHPfnkkxo6dGi5z2m1WnXo0CEFBgbKYrFUtMkAAACoYoZhKDMzU5GRkfLwcNy/WeHweerUKcXExOjWW2/VNddcU+b+S5cu1aWXXqpp06YpJCREH330kUaMGKGEhAR169atXOc8dOiQoqKiKtpUAAAAuFhSUpKaNGni8HmLYRjG+b64xWIps+fTno4dO2rkyJGaMmVKufZPT09XSEiIkpKSFBQUdB4tBQAAQFXKyMhQVFSU0tLSFBwc7HC/Cvd8VpbValVmZqbq16/vcJ/s7GxlZ2eb32dmZkqSgoKCCJ8AAADVWFlDJF0+4eiFF17QyZMndf311zvcZ/r06QoODja/uOUOAABQO7g0fH722Wd66qmnNGfOHIWFhTncb/LkyUpPTze/kpKSXNhKAAAAVBWX3XafPXu2br/9dn355ZeKj48vdV9fX1/5+vq6qGUAAABwFZf0fH7++ecaO3asPv/8c11++eWuOCUAAACqoQr3fJ48eVI7d+40v9+zZ48SExNVv359NW3aVJMnT9bBgwf1ySefSCq41T5mzBi98sorio2NVUpKiiTJ39+/1JlQAAAAqH0q3PO5evVqdevWzazROWHCBHXr1s0sm5ScnKz9+/eb+7/77rvKy8vT+PHj1ahRI/Prvvvuc9JbAAAAQE1RqTqfrpKRkaHg4GClp6dTagkAAKAaKm9eY213AAAAuAzhEwAAAC5D+AQAAIDLED4BAADgMoRPAAAAuIzLVjiqSRIOJGj7se2KbhCt2Cax7m4OAABArUH4PMekhZM0689Z5vcT+0zUzEtnurFFAAAAtQe33YtJOJBgEzwladafs5RwIMFNLQIAAKhdCJ/FbD+2vULbAQAAUDGEz2KiU3IqtB0AAAAVQ/gsJvawjyYus9026feC7QAAAKg8JhwVFx2tmb9I12yRtjeQoo9JsQclPRft7pYBAADUCvR8FhcbK02cqNiD0qj1Z4PnpEkF2wEAAFBp9Hyea+ZMXZUcrhbHDyqsR2dNfuoWd7cIAACg1iB82pEY2VaJkW01rHWEu5sCAABQq3DbvRSeHhZ3NwEAAKBWIXyWwkL2BAAAcCrCZyk8SJ8AAABORfgsBbfdAQAAnIvwWQo6PgEAAJyL8FkKT9InAACAUxE+S8FtdwAAAOcifJbCg/AJAADgVITPUpA9AQAAnIvwWQpKLQEAADgX4bMUhE8AAADnInyewzAM8zHhEwAAwLkIn+fItxaFT0+uDgAAgFMRr86RZ6XnEwAAoKoQPs9RvOeTUksAAADORfg8R/Gezx/XH3JjSwAAAGofwuc5ivd8Jh0/48aWAAAA1D6Ez3PkWa3m457N6rmxJQAAALUP4fMceflFPZ+Bfl5ubAkAAEDtQ/g8R/Hb7jn51lL2BAAAQEURPs9RfMJRbp5Ryp4AAACoKMLnORoF++neS1pLkrLp+QQAAHAqwuc5/Lw91aN5fUlSbh7hEwAAwJkIn3Z4exYUl9+cnGEzBhQAAACVQ/i0w9er6LL8N2GfG1sCAABQuxA+7fD2LLosS7cfcWNLAAAAahfCpx3Fw6eHhfXdAQAAnIXwaYdPsdvunh6ETwAAAGchfNrhU7znk/AJAADgNIRPO4rfdvfktjsAAIDTED7tKH7bnewJAADgPIRPOwrrfEqizicAAIATET7tKH7bnfAJAADgPIRPO4oXme/etJ4bWwIAAFC7ED7tsFgsim8fLkkK8PV0c2sAAABqD8KnA4V33g3uugMAADgN4dMBiwomHRmkTwAAAKchfDpQWGKJ6AkAAOA8hE8HCtd0p+MTAADAeSocPpcuXaoRI0YoMjJSFotF3377bZnHLFmyRN27d5evr69at26tjz/++Dya6mKFPZ+kTwAAAKepcPg8deqUYmJi9MYbb5Rr/z179ujyyy/XoEGDlJiYqPvvv1+333675s+fX+HGulJhmXnKfAIAADiPV0UPGD58uIYPH17u/d9++221aNFCL774oiSpffv2WrZsmf79739r6NChFT29y1gKb7u7uR0AAAC1SZWP+Vy+fLni4+Nttg0dOlTLly93eEx2drYyMjJsvlzNg9vuAAAATlfl4TMlJUXh4eE228LDw5WRkaEzZ87YPWb69OkKDg42v6Kioqq6mSVYyt4FAAAAFVQtZ7tPnjxZ6enp5ldSUpLL21B4291KzycAAIDTVHjMZ0VFREQoNTXVZltqaqqCgoLk7+9v9xhfX1/5+vpWddNKVdjzSfYEAABwnirv+YyLi9OiRYtsti1cuFBxcXFVfepKYcIRAACA81U4fJ48eVKJiYlKTEyUVFBKKTExUfv375dUcMt89OjR5v533XWXdu/erYkTJ2rr1q168803NWfOHD3wwAPOeQdVxFzhiPQJAADgNBUOn6tXr1a3bt3UrVs3SdKECRPUrVs3TZkyRZKUnJxsBlFJatGihX766SctXLhQMTExevHFF/X+++9X6zJLUvE6n6RPAAAAZ6nwmM+BAweWWn7I3upFAwcO1Nq1ayt6KreyMN0dAADA6arlbPfqoGhtd3o+AQAAnIXw6QBjPgEAAJyP8OlQYZ1PNzcDAACgFiF8OmAur0mxJQAAAKchfDrAbXcAAADnI3w6YBFF5gEAAJyN8OlAUc8n8RMAAMBZCJ8OFJVacnNDAAAAahHCZxmYcAQAAOA8hE8HmHAEAADgfIRPB5hwBAAA4HyETwcK63xa6foEAABwGsKnA4W33en6BAAAcB7CpwMWC7fdAQAAnI3w6YDZ8cltdwAAAKchfDpQ2PNpJXsCAAA4DeHTAUotAQAAOB/h04Gi+UakTwAAAGchfDpAzycAAIDzET4dKFrbnfQJAADgLIRPByjzCQAA4HyET0fMnk83twMAAKAWIXw6wIQjAAAA5yN8OuBBnU8AAACnI3w6wGx3AAAA5yN8OmAxH5E+AQAAnIXw6UBh5Nx5+KRb2wEAAFCbED4dmLM6SZK0au8JN7cEAACg9iB8OnDgxBl3NwEAAKDWIXwCAADAZQifAAAAcBnCJwAAAFyG8OlAoK+Xu5sAAABQ6xA+HXjmqk6SpLqEUAAAAKchfDoQEuAtSWrWIMDNLQEAAKg9CJ8OFK7tzvKaAAAAzkP4dKBwbXcr6RMAAMBpCJ8OWIqt7g4AAADnIHw6UNjzSccnAACA8xA+HSjs9zRE+gQAAHAWwqcj9HwCAAA4HeHTgcIxn2RPAAAA5yF8OlA05pP4CQAA4CyETweKxnwCAADAWQifDljMrk/3tgMAAKA2IXw6QPYEAABwPsKnA+Ztd8Z8AgAAOA3h0wF6PgEAAJyP8OnQ2VJLpE8AAACnIXw6UNTzSfoEAABwFsKnA0VjPt3aDAAAgFqF8OmAh4Xb7gAAAM5G+HSAFY4AAACcj/DpAGu7AwAAON95hc833nhDzZs3l5+fn2JjY7Vy5cpS93/55ZfVtm1b+fv7KyoqSg888ICysrLOq8GuUtTz6d52AAAA1CYVDp9ffPGFJkyYoKlTp+qvv/5STEyMhg4dqsOHD9vd/7PPPtMjjzyiqVOnasuWLfrggw/0xRdf6NFHH610412B2e4AAADOU+Hw+dJLL2ncuHEaO3asOnTooLffflsBAQH68MMP7e7/559/qm/fvrrpppvUvHlzDRkyRDfeeGOZvaXuRs8nAACA81UofObk5GjNmjWKj48vegEPD8XHx2v58uV2j+nTp4/WrFljhs3du3dr7ty5uuyyyxyeJzs7WxkZGTZfrsaYTwAAAOfzqsjOR48eVX5+vsLDw222h4eHa+vWrXaPuemmm3T06FH169dPhmEoLy9Pd911V6m33adPn66nnnqqIk1zOno+AQAAnK/KZ7svWbJE06ZN05tvvqm//vpLX3/9tX766Sc988wzDo+ZPHmy0tPTza+kpKSqbmYJheGTvk8AAADnqVDPZ2hoqDw9PZWammqzPTU1VREREXaPeeKJJzRq1CjdfvvtkqTOnTvr1KlTuuOOO/TYY4/Jw6Nk/vX19ZWvr29FmuZ0FtZ2BwAAcLoK9Xz6+PioR48eWrRokbnNarVq0aJFiouLs3vM6dOnSwRMT09PSdW7gHuAT0Ebj53KUVZuvptbAwAAUDtUqOdTkiZMmKAxY8aoZ8+e6tWrl15++WWdOnVKY8eOlSSNHj1ajRs31vTp0yVJI0aM0EsvvaRu3bopNjZWO3fu1BNPPKERI0aYIbQ6igj2Mx9n51rl51192woAAFBTVDh8jhw5UkeOHNGUKVOUkpKirl27at68eeYkpP3799v0dD7++OOyWCx6/PHHdfDgQTVs2FAjRozQc88957x3UQU8igZ9ylqNe2gBAABqEotRne99n5WRkaHg4GClp6crKCjIJec0DEMtJs+VJK15PF4N6rp3DCoAAEB1Vt68xtruDlgsFnPGu7Xax3MAAICagfBZisJb7zWgcxgAAKBGIHyWwuNsz2c+4RMAAMApCJ+lKOz55LY7AACAcxA+S2GGT9InAACAUxA+S+HB+u4AAABORfgsRWHPJ2M+AQAAnIPwWQoPj8Ixn4RPAAAAZyB8lqLotjvhEwAAwBkIn6VgtjsAAIBzET5LYSkc80n6BAAAcArCZyk8z14dxnwCAAA4B+GzFEXLa7q5IQAAALUE4bMURWM+SZ8AAADOQPgshaVwbXfGfAIAADgF4bMUnh7MdgcAAHAmwmcpisZ8kj4BAACcgfBZij1HT0mSvl93yM0tAQAAqB0In+XwyfJ97m4CAABArUD4BAAAgMsQPgEAAOAyhE8AAAC4DOETAAAALkP4BAAAgMsQPgEAAOAyhE8AAAC4DOETAAAALkP4BAAAgMsQPgEAAOAyhM9SNA7xlyT9rUsjN7cEAACgdiB8luLa7o0lSQ3q+Li5JQAAALUD4bMUFotFkmQ13NwQAACAWoLwWQoPM3ySPgEAAJyB8FkKj4LsSc8nAACAkxA+S+FxNn0a9HwCAAA4BeGzFBaz55PwCQAA4AyEz1JYxIQjAAAAZyJ8lsKDnk8AAACnInyWonC2O9kTAADAOQifpWDMJwAAgHMRPkvhQZF5AAAApyJ8loIxnwAAAM5F+CwFdT4BAACci/BZCnNtd6ubGwIAAFBLED5LwW13AAAA5yJ8loIJRwAAAM5F+CxFYc8nYz4BAACcg/BZCnPMJ+ETAADAKQifpeC2OwAAgHMRPkvBhCMAAADnInyWgrXdAQAAnIvwWYrCtd2zcvPd2xAAAIBagvBZisIez9X7TiifgZ8AAACVRvgsRdrpHPPxiWKPAQAAcH7OK3y+8cYbat68ufz8/BQbG6uVK1eWun9aWprGjx+vRo0aydfXV9HR0Zo7d+55NdiVivd1ehbegwcAAMB586roAV988YUmTJigt99+W7GxsXr55Zc1dOhQbdu2TWFhYSX2z8nJ0aWXXqqwsDB99dVXaty4sfbt26eQkBBntL9KFY+bHh6ETwAAgMqqcPh86aWXNG7cOI0dO1aS9Pbbb+unn37Shx9+qEceeaTE/h9++KGOHz+uP//8U97e3pKk5s2bV67VLmIp1ttJxycAAEDlVei2e05OjtasWaP4+PiiF/DwUHx8vJYvX273mO+//15xcXEaP368wsPD1alTJ02bNk35+Y5nkGdnZysjI8Pmyx2Kd3ZSbgkAAKDyKhQ+jx49qvz8fIWHh9tsDw8PV0pKit1jdu/era+++kr5+fmaO3eunnjiCb344ot69tlnHZ5n+vTpCg4ONr+ioqIq0kynsbnVTvgEAACotCqf7W61WhUWFqZ3331XPXr00MiRI/XYY4/p7bffdnjM5MmTlZ6ebn4lJSVVdTPt8ih2r90gfQIAAFRahcZ8hoaGytPTU6mpqTbbU1NTFRERYfeYRo0aydvbW56enua29u3bKyUlRTk5OfLx8SlxjK+vr3x9fSvStCrBbXcAAADnqlDPp4+Pj3r06KFFixaZ26xWqxYtWqS4uDi7x/Tt21c7d+6U1Wo1t23fvl2NGjWyGzyrE4tNzycAAAAqq8K33SdMmKD33ntP//nPf7RlyxbdfffdOnXqlDn7ffTo0Zo8ebK5/913363jx4/rvvvu0/bt2/XTTz9p2rRpGj9+vPPeRRUZ3qmoN9eg6xMAAKDSKlxqaeTIkTpy5IimTJmilJQUde3aVfPmzTMnIe3fv18eHkWZNioqSvPnz9cDDzygLl26qHHjxrrvvvs0adIk572LKlLXt+jyED0BAAAqz2LUgC69jIwMBQcHKz09XUFBQS49d/NHfpIkrXosXg0D3T8OFQAAoDoqb15jbfdyYrY7AABA5RE+y2DOOSJ7AgAAVBrhswxkTwAAAOchfJahsNxS9R8ZCwAAUP0RPstQ1PNJ+gQAAKgswmcZCsd80vMJAABQeYTPMljO9n2SPQEAACqP8FkWs+eT+AkAAFBZhM8ymGM+yZ4AAACVRvgsg1nnEwAAAJVG+CyDOeaTnk8AAIBKI3yWwZztzpQjAACASiN8loExnwAAAM5D+CyDucKRm9sBAABQGxA+y1DU80n8BAAAqCzCZ1nMMZ8AAACoLMJnGTKz8iRJaadz3NwSAACAmo/wWU4vLdzu7iYAAADUeITPcjp2kp5PAACAyiJ8AgAAwGUInwAAAHAZwmc5UWkJAACg8gif5cTymgAAAJVH+Cwnej4BAAAqj/BZThZL2fsAAACgdITPcuoWVc/dTQAAAKjxCJ9l8PQo6PL8YnWSm1sCAABQ8xE+y5BvZbAnAACAsxA+AQAA4DKETwAAALgM4RMAAAAuQ/gEAACAyxA+AQAA4DKEzzJE1fd3dxMAAABqDcJnGR67rIMkqVmDADe3BAAAoOYjfJbB38dTklTX18vNLQEAAKj5CJ9lKFzSnVrzAAAAlUf4LIOHpSB+GgbpEwAAoLIIn2U4u7S7yJ4AAACVR/gsy9nwaSV9AgAAVBrhswzmbXc3twMAAKA2IHyWoWjCEfETAACgsgifZfA4O+hz95FTymfKOwAAQKUQPstQOOFIkr5fd9B9DQEAAKgFCJ9lKkqf6w+ku7EdAAAANR/hswzFez6D/Lzd1xAAAIBagPBZhuITjYL8CZ8AAACVQfgsQ/qZXPNxXV9PN7YEAACg5iN8liEvv6jn87ftR9zYEgAAgJqP8FmGQe3CzMdzN6S4sSUAAAA1H+GzDN6eXCIAAABnIVkBAADAZQifAAAAcBnCZzlM+VsHSVKvFvXd3BIAAICa7bzC5xtvvKHmzZvLz89PsbGxWrlyZbmOmz17tiwWi6666qrzOa3bhAX5ursJAAAAtUKFw+cXX3yhCRMmaOrUqfrrr78UExOjoUOH6vDhw6Uet3fvXj300EPq37//eTfWXXy9Cup75uRZ3dwSAACAmq3C4fOll17SuHHjNHbsWHXo0EFvv/22AgIC9OGHHzo8Jj8/XzfffLOeeuoptWzZslINdgcfr4LLlE34BAAAqJQKhc+cnBytWbNG8fHxRS/g4aH4+HgtX77c4XFPP/20wsLCdNttt5XrPNnZ2crIyLD5ciffs+EzJy/fre0AAACo6SoUPo8ePar8/HyFh4fbbA8PD1dKiv0C7MuWLdMHH3yg9957r9znmT59uoKDg82vqKioijTT6ej5BAAAcI4qne2emZmpUaNG6b333lNoaGi5j5s8ebLS09PNr6SkpCpsZdmKej4JnwAAAJXhVZGdQ0ND5enpqdTUVJvtqampioiIKLH/rl27tHfvXo0YMcLcZrUWBDgvLy9t27ZNrVq1KnGcr6+vfH2rzwxzX3o+AQAAnKJCPZ8+Pj7q0aOHFi1aZG6zWq1atGiR4uLiSuzfrl07bdiwQYmJiebXFVdcoUGDBikxMdHtt9PLi9nuAAAAzlGhnk9JmjBhgsaMGaOePXuqV69eevnll3Xq1CmNHTtWkjR69Gg1btxY06dPl5+fnzp16mRzfEhIiCSV2F6dFY35ZMIRAABAZVQ4fI4cOVJHjhzRlClTlJKSoq5du2revHnmJKT9+/fLw6N2LZzk71PQ82k1pPQzuQr293ZziwAAAGomi2EYhrsbUZaMjAwFBwcrPT1dQUFBbmlD7LRflJqRre/G91VMVIhb2gAAAFBdlTev1a4uyirUMLBgAtSxU9lubgkAAEDNRfgsJx/PgkuVl1/tO4oBAACqLcJnOXlYLJIka/UfpQAAAFBtET7LycOjIHzmU20JAADgvBE+y8mTnk8AAIBKI3yWU2H1KMInAADA+SN8llPhmM98K+ETAADgfBE+y6lowpGbGwIAAFCDET7LyfPshCMr6RMAAOC8ET7L6Y+dRyVJ36875OaWAAAA1FyEz3LKziuosbTsbAgFAABAxRE+AQAA4DKETwAAALgM4RMAAAAuQ/gEAACAyxA+y+nm2KbubgIAAECNR/gspz93HXN3EwAAAGo8wmc5+XhyqQAAACqLRFVOPl5cKgAAgMoiUZWTt6fFfJyXb3VjSwAAAGouwmc5eViKhU/WdwcAADgvhM9yuqpbY/NxVm6+G1sCAABQcxE+y+mGi6LMx2//ttuNLQEAAKi5CJ/l5FVstvuqvcfd2BIAAICai/B5HtbsO+HuJgAAANRIhE8AAAC4DOETAAAALkP4BAAAgMsQPgEAAOAyhM8K8PSwlL0TAAAAHCJ8VkDxJTYBAABQcV7ubkBN4u3hoSyxrjsAAKgBEhKk7dul6GgpNtbdrTHR81kB3l5cLgAAcB4SEqRPPy34rytMmiT17i2NHl3w30mTXHPeciBNVUDHyCDzcW4+PaAAAKAcXB0EExKkWbNst82a5brgWwbCZwVMHt7efDx7VZIbWwIAQC3j6p5BV53PHUFw+/aKbXcxwmcFBAd4m48PHD/txpYAAFDFXBkGXd0z6MrzuSMIRkdXbLuLET4rwNNSNNvdg7JLAIDaypXhzNU9g64+nzuCYGysNHGiEhpLn3aREhqr4P9hNZl0RPisAH8fT/NxTh5jPgEAtZCrw5mrewZdfb6zQdCGC4LgpHip9zhp9DUF/5002KjS81UEpZYqINi/6LZ7k3r+bmwJAOCC46qyOaWFs6o4r6t7Bt3REzlzphIuidb2XasU3eoixQ69rerOJSnhQIJm/Wn7B8SsP2fpmvbXKLaJ+3s/6fmsoL91aeTuJgAA3K02l81xdThz9S1iN/RETlo4Sb1X3K7RR95R7xW3a9LCqh3Tuv2Y/T8gHG13NXo+K8jXq+DWeza33QHgwjRpku1t6YkTpZkzq+58jm6DX3NN1QSmwjD431na3kCKPibF/qOKw1m8NCug6PuJfQxV4RV1aU+kO3ohoxvY/0PB0XZXo+ezgo6czJYkzfh5q9JP57q5NQAAl7pAyua4crygo3CWcKDqrqkreyLd0QsZ2yRWE/vY9u5O6jupWtxylwifFbZ0+xHz8fvLdruxJQAAl7sAyua4Ogy6Opy5+v25qxdy5qUzteK2Ffrkqk+04rYVmhE/o0rPVxGEzwry8Sy6ZLuPnHJjSwAAJleNwTwb+GzGJxbbXiVcPCbS1WHQ1eHM1e/Pnb2QsU1iNSpmVLXp8SzEmM8Kev2mbrrj0zWSpLQzOW5uDQBUU66amS25dgxmbKwmPdZLs7xXFp0uN1YzXVA2x1VjIl0dBgvDWfHeyKoMZ+7oiZx56Uxd0/4abT+2XdENoqtdGHQ1i2EY1afwkwMZGRkKDg5Wenq6goKCyj6gCm0+lKHLXv3dZtvqx+MVWte31ON2HzmpRsH+NrVCXen4qRzVr+PjlnMDqAZqaxhMSCiY/X2uFSuq5H0mHEhQ7w9Knm/FbSuqLFC445yTFk4qEQar+rZtwoEEl4Uzd7y/C0F58xo9nxVksbOwUc9nf9HeGZc7PGbNvuO69q3lat4gQEseHlSFrbPv9cU79MKC7Zp2dWfdFNvU5ecH4GauDoOunJnt4pqUpd2yrarA5I5zuqOnLrZJrMt6BOmJdC/CZwXl5le8xNIP65IlSXuPuWc9+BcWFPzD9eg3G1wWPnPyrEo7naOwID+XnA+ocVzVE+mmMJjQWEVleg6qyguUlzhfFY3BdMctW3dNWHFlGHSH2v7+qjMmHFVQ24hAu9vf/m2Xi1tScfZ6bavKiNeWqde0RdqRmum6k1ZDe46e0i0frdSqvcfd3RRUJ64sGF4sDNpMkKmq2dnR0SXL9MSrSguUT3qsl+35HoutskDvjskj1b1sDlBRhM8K8vXy1NZnhpXYPuPnreU6/kxOvr5cnaRjZ+uFupKHC9PntrOh86cNyS47Z3X0z//+pSXbjui6t5e7uymoLlxdJ9LFYTChsTSrn+22Wf2KhV5nn+9Ags3kH0ma5Z1QpTUi3VHCpjqXzQEqivB5Hvy87U8aWrApxXz884ZkPThnnbJy83Uo7Yy5/ekfN+vhr9brHx+stPcSTvHL5lSt2XdC8zel2AwTON/ouWbfCb22aIfyzmPIwYXu4An3DLXAeXBVqR4X90S6Ogy6uoyNu5YRdEcJm+paNgeoKMZ8OtEdn67R6LhmGj+ote7+71+SpAWbU5SZlWfu8/nK/ZKkLckZysu3ysvTQ/lWQy8u2KaYqBBFBPmpS5NgWSrYS7l0+xH9vDFZl3YI1+2frDa3TxzW1nx8vh2f1771pySpXh0f/aN3M4f7ZeXmOwzmQIW4cma25NoJOWd7IosHwonLpJlV1BPp6skqrh6fWN2XEQRQEj2fTvbJ8n2KnbbI/L548DxX68d+1qG0M5o2d4veXLJLd366Rle+8YdaTJ6r9QfSKnTe0R+u1Ocrk3Trx6tttn/0x17zcUUC7aG0M3r4y3XafCjD3GavqP6ZnHylZmTpp/XJavfEPH26fK/N85bz7m+tHSr6RwTk2vGQknkb3KYnsgpvg7u6J9JdNRuLq8rxiYyHBGoeej7drM+MxXa3X/H6H5p//wCbCU75VkM7DmfqxQXb9eCQaLWLKKihlXTc8a3dI5lFY0s97OQgwzCUnWeVn7enDMPQ7zuOKjo8UPfNXqtVe0/oyzUHSj2+38zFOnaqqNj+E99t0qi45ub35c1eWbn58vb0kKe9k7hZakaWwi/0WfsunpltM3O5KmdmS9L27fZ7ImtJqR5XF/CWXF/GhrI5QM1yXuHzjTfe0PPPP6+UlBTFxMTotddeU69evezu+9577+mTTz7Rxo0bJUk9evTQtGnTHO5/Icq2bFOux0F5WxvL1yi6TT705aVqHVZXP9zTT1+uSdL0uVt1JjdfkrRwc6pZW7T/rF/LdZ6sXKvSz+Qq2N/b3PbgnHX6eu1Bzbu/v45kZmv0h47HonqcDYYnTuUo8UCaBrRpaBM87XlpYcEv2nsHt3G4z+mcPF307C9qFVZX39/Tz+F+7vDu0l2aNnerHhoSrXsucfweXC1h/gfavmuVoltdpNiht1XtySZNUsJ/ZxWFwZur8Ja0i4OgJCWE5djtibwmLEdVccYLZXUVV5exoWwOUHNUOHx+8cUXmjBhgt5++23Fxsbq5Zdf1tChQ7Vt2zaFhYWV2H/JkiW68cYb1adPH/n5+WnmzJkaMmSINm3apMaNq+g+kwtEh9fV9tSTlX6dE14fKcP7f+b3QbnXql7eWPP7nYdPasHmFE35blOJY19btENZefkVOl/MUwv0xaAUM7h8vTZCkjTs5d/LOFJKTs/S9tRMDfn3UknSM1d2tLvfuYtmvbRwe6nhc/XeEzqVk6/1B9LL+zYcOp2TJ39vT608uNLmF+3J7DzV8fEs121wq9Uwg/a0uQVVDF5YsF33XNJGhzOzFBZYshc0Yf4Hmrt8saKa9tDtt06o9PsozaTHY4tm9x55RxN/f1czn62iSTIJCZr01yzNGle0aeKyWZqZUDU9ka4OgpK0PcL+yl/bI3yq5Jzu6IksPC/hDEB1UOHlNWNjY3XRRRfp9ddflyRZrVZFRUXpX//6lx555JEyj8/Pz1e9evX0+uuva/To0eU6Z3VaXrPQobQzmr1yvz76Y68ysx2P6yxNtmWbUvweLLE9IutFmx5QZ4o+NEELWxXd9rt0V7S2R750Xq/VLiJQW1NK1vGMbx+mX7YcNr9vemyBxnXLNnvp8vKtWn8wXSnpWaoX4COrYejm9wvC0+5pl5nBrzjDMHQ6J191fG3/Xtp79JTunb1Wt/dvqR7N6qnvjMUKDp+j9RmfmPuM7nSvEhKH6YaLmurJKzoqOy9fO1JPKt9qqGNkkLw8PbRi9zG9MH+bTmbnKTMrT/MfGKC6vl5q/shP5us8NCRaLyzYridHdNAtfVuY223CoKSJub3MMNj16QVKO51b0NZSVsEqr4T5H6j3ittLbF/R+/0q6QFNeO9J9T70VMnzRU5V7LgnnX6+T9d9qtHflvx34ZOrPtGomFFOP5/knqULC8/LbWIAtUmVLK+Zk5OjNWvWaPLkyeY2Dw8PxcfHa/ny8tUxPH36tHJzc1W/fn2H+2RnZys7u2isYkZGhsN93SUyxF8ThrTVjxuSlXnk/MJnZPpqpdgZShiZvlrHgpwfPpseW2ATPCVpYavt6n9ggfY3GFLh1ztw4ozd7cWDZ2HY/f2IzF66en3/Y1OUf/YdRb/4s/Os8vfx1OHMLH38x16ln8nVkcxs7Tt2WttSM/XidTGKa9VAoXV9ZbFIA19YIkm69/O1BcdbttkET0n6ZOOrishrpo//tGpETCNd+1bRZ3VU72a6Kbapbnh3hc0xc9cn6/qLomy2Fa4U9eQPm83wueDbt+3UGFypa+Z/UCVhcPuuVXa3b925smrO10DSIfvba8staXoiAcC1KjTb/ejRo8rPz1d4eLjN9vDwcKWkpDg4ytakSZMUGRmp+Ph4h/tMnz5dwcHB5ldUVJTDfd3Naq1Qx7GNNscqtr2yAnLsT3RwtL0sJ8vo8bUXdmd5r9Tcr99yeEz/WYs1YU6iej23SG8u2aX/JuzXgs2pZtH6B79cpz4zFuv6d5broJ3wG5m+usS24tuLB09J+nTFPg1/peSQg1xr6TVNT5197y99853d57furJo6rtGtLrK7ffbmOpr89QannGP63C265aOVyrcaiu413H47im3Py7dq79GSlRDOh7tmLlPAGwBcx6WllmbMmKHZs2frm2++kZ+f49nDkydPVnp6uvmVlJTkwlZWTH7FRi3YyPLtqYnLbLdN+r1ge1U47WO/98jR9soqb9j9a/8J8/HRkzn6+q+DZb52YlKanvyh5DhYZwX6fKtR6qpVHafO1/xNKQ6v3QeJfvpg2R6bQlMrdh/Tb9uPaN+xigW1Yyez9dEfe3TiVI5ih96mibm2k/XGHIvRlsDBZg3Zsrzx605d/urvysjKtfv8O0t3a8m2I1q+61i5wuA9n63VwBeW6Ju1B859qXLbdChdV76+TH/sPOq2IEgBbwBwjQqFz9DQUHl6eio1NdVme2pqqiIiIko99oUXXtCMGTO0YMECdenSpdR9fX19FRQUZPNVXeXnn3/4TIxsq+aZ12rFe9InX0sr3pOanvy7EiOrZrzn/gZDdOku27B06a6253XLvTzKG3Znzdt2Xq+/ZNuREtucFehz8w2boQH23PnpmlKv6TM/btaJ00UB74Z3V2jMhyt18fNLdOJUjuZtTNGN765QcvqZs+e06mBayd7cu/5vjZ76YbPu+yJRkjTjmRUadeYR/ePIlVoe+766X/lliWMMw9CvWw/rcEZWieeen79Nmw5l6Pb/rNagF5ZotYN15wtXxyorDM47u7LXW0scX6/MrNwSE9GKG/vRKq07kG6O/SUIAkDtVaExnz4+PurRo4cWLVqkq666SlLBhKNFixbpnnvucXjcrFmz9Nxzz2n+/Pnq2bNqevXc5Ym/ddDd//1LwztFyMvTQ/uPndKGg+la9OBAfbv2oNYmpelfl7TW1O82aXNyybGrMweOVddDfdTi+EH9L6ZxlQXPQtsjX1L/AwsUkLNdp32itT2yaoKnVBh2550zwaltlZ4zMbKthm6/Vive+59ZGmhNk79rdgWva0WWEj2fa3rNW39qz9lb1f1m/qrNTw/VbR+v1rKdR3VbvxYa2jFCrRrWUYO6vlq1t6BneOn2grB9JDNbS9VPqttPr/a/VHe9uMR83T1HT+nZHzdr0dbDNufrGhWiN2/urgCfohWoVu4pCJ3/+CBBW58peXvdUFFYjG0Sqx6NLpKXp+O/V8+t/nAw7YwS96fpSGaWnvxhs0b2jNLMv9v+4fn1XwcUVT9Ax4uV7Oo/a7GeubKTBrYtWT2jPHLyrBr1QYJ6Nq+nh4e2O6/XAABUnQrPdv/iiy80ZswYvfPOO+rVq5defvllzZkzR1u3blV4eLhGjx6txo0ba/r06ZKkmTNnasqUKfrss8/Ut29f83Xq1q2runXrluuc1XG2e3HHTmarfh0fs4xPdl6+fL1sl5k8nZOnDlPmu6N5btf0WFEwq6pe1nN1PbRNLY4f1J76VR/oncFR6a4m9fxtJnZ9cmsvpZ/J1b/OTrBa+ehg9Sq2olbDQF+bhQXO1b1piP7an1Zi+78uaa11B9LVqmEdm1Wx3h/dU/EdwpV0/LQuf/V3jbwoSqN6N9eA539Vp8ZB+n58P7V8dK65/7SrOyu+fZj+2p+mf/53jc4dEl18xv/Gg+n622sF3dTenhblnnMX4XyrA/y4/pDu+WxtpV5DkvYfO60AX0+F1vU979cAgAtJefNahcOnJL3++utmkfmuXbvq1VdfVezZmn8DBw5U8+bN9fHHH0uSmjdvrn379pV4jalTp+rJJ5906pup7tLP5Krb0wtsfiE/dUVHTf2+5NjF8ppwabRZyB0XHkdh0pk+vKWnFm89rP9bUXJM6cRhbSs8bCKuZQON7dtcd3y6xtxmL3zumX7ZeS1P+l3iQd03O1FS+cPnjtRMTZu7RffFR6trVIiOZGbroud+KddrZGbl6rftR3RJuzAF+LBoHIALV5WGT1erLeFTKhhH5+Vh0dIdR+XlYVHf1qG68o0/tC4pTZI0smeUvlhtf4JVgI+n3ripu8Z+XFRuZ+EDA3Tp2aLvlRHo63Xe9UpR+zVvEKC9xxwv41pZPp4eyjlnqMOuaZc5XG41L9+qJ3/YpNgWDTQiJtLmuZ/WJ2v8Z39Jchwcj53MVr7VUNjZZVP7zVxs9jDvnXG5lu04qn98kGB+n52Xr02HMhTTJKREm275aKWWbDuiq7s11r9Hdq3YGweAWqS8ec2ls90heXt6yGKx6OLohurbOlSS9N6oHro/vo1WTB4sT8+iX2wjYiL1w9nlJh8aEq3NTw/ToHZhGtyuaCxcoJ+3nGHDU0M1Oq6ZU14LtU9VBk9JJYKnVFBxwJFvEw/p/1bs178+X6s1+2wnTHkV+xmy97e11Wqox7O/qNe0RTqdk6fjp3JK1Kw9N/NOmLNO17z5p974dWeJ1yuc+PbN2rKrNAAAznNtdzhXWJCf7o8vmDEd26K+PksouL35+OXtFR7kV6L35v0xPfX9ukPKyzcUEey4ZNU7o3rozmK3Nu35+p991KSevyTp0cva65PlJYdIAO6QbzW0NSVDby3ZpSu7RsrDYtGANg3l4WFRcrGqANe+tVytGtbRl3f10WuLd6h43szOs8rPu2D8tWEY2ngwQ03rB5jPL9iUqvvPVhEorvgqW1aroZ/WJ0uS3vltl+4d3Eb/W3NAdXy9NKxT6VU+AAAlET6rmStiImWxWBTTJFjhQfaDpcVi0ZVdG5vfL3looLnST6G6vl4KLLYU5axru2jS1+sV26K+Vuwu6Cn6ZcIAtQ4LNPfx8/YsV2A9V7C/t9LP2K8ZWVn16/hoaMcImxqWCx4YYK4vX1kzr+2sSf9zTnF2OFf7KfPMx98lFi2z9I/eTXUqO99m311HTqn7MwtLvEZWbr5GvrtCDev6KjUjSxsOpts8by94SpJHsbGmu48WTQQzJCWnn9GDX64reG7aZTbHJew+ptiWDXQmJ1/+xSoL7Dt2SiH+Pvph/SE9/u1GBfl5KXHKEJuQm5Wbr+w8q4L9nXM3AwCqK8JnNWOxWHTFOWPYytI8tI5+nzhI/Wf9Kkn67eGBCg/yk5eHRbEt6qtlw7q6/qIoXdWtsXy8PGQYhk2PUHGD2oape9MQdWkSov9bsU95xW59fj6utyJD/HT0ZLa+WJWkOasLiornFrtl2jY8UFd3b6z6dXz05q879eqN3fTAF4nadaSgrFDrsLraebjkrG57eresr1dv7KaGdX11WecIjfqgYNWgsmYft2pYxzxfWUZe1FQdI4P1jw8SzDXYz0e3piFae87EH39vT53Jzbd/AM6bvYlPjnR9umQgLcvMeVvVJqyoEkfxCYFWw1BqRlE1gRves12WdWSxZVqv7tZY3ZvVU7eoEHNWf6GMrDxN+X6jHhrSVl+tOaBZ87aZQw8igvz0yg1d9eu2I8rMytXouOZq1bBOqWWuAKAmYcJRLVG8lNPO54Y75RfVjJ+36u3fdumyzhF6eWQ3+XgVvWbhbOA6Pp6aOqKjJv5vvf45sJUmDitZV/HhL9fpyzUFQXXdlCF6dfEOfbBsj80+YYG+OpyZrY6RQbpvcBtJ0pCORbc0i5fl2fDkEPWZvticIPX837vo4a/Wm/vumX6Zvll7UBPmrDO3xbcP1y9bbBdHkGwnpIz/7C/l5Fm1cHPJ/Urz8NC2Gj+otXo994sOFytz1LtlUS8zLlxR9f2VdLzk4gEVcUVMpMb1b6l2jQLlTQgFUE2VN6/R81lLBPh4afGDF8vTw+K0HpIHh0Tr4uiG6tY0xCZ4SgX1JFc+Olh1/bwU4OOlgW0bqmGg/R7JsKCi7cEB3nribx3M8Dnnzjg1DPRVVD1/JadnKSzIt0SNVMl28om3p4fN7coRMZE24dNiseia7k3UvWk9czjC6zd1U7snim7j2vPGTd0lSc0f+UmSdHu/Fnr/bDsHtwtTVP0AdW9WT4ZhmKV8Fj94sVo2LOgl+/KuOKVmZOv6d4rWj3/lhq7mvuWx+vF49Xz2l3Lvj+qvssFTkr5fd0jfrysYejC0Y7jeGWV/sY7DGVmavzlVV3drrDX7Tqh5gwA1a1Cn0uc/XwfTzuh0dp7ahAeWvTOACwbhsxYpDEHO4u3pobhWDRw+H1ZsTGqYg/GpknT3wNbaf/yMLu/cyNz22GXttfvoKV3UvJ5ZyzGq2ESQc+UX66D38fRQdHhdc+UfP29PDWzbsMRym81D6+h/d8epQR1f+Xl7ave0y/TSwu1qE15XH/6xV5e2t7+Czoe39NSP65P1wKXRatmwrv7YdVQvXR9jE4r7tg7VsZM5Nte8WYM6Nr/oDUO6smtj/bAu2ex17du6ge6+uLVZxufKrpFK2H1cKWeXwQyt66tAPy9lZpUsexVV31/dourpxOkcXRzdUM/+tEWStOmpobr4+SU6etJxcfmy9G8Tqt93HD3v4wu9dH2M1iWlaURMpFbtPaGZ87aW2KdT4yBtPFhytS+Uz/xNqcrNt5o9oHn5Vi3beVQ9mtXTyHdXaM/RU/rPn3vN4S3lqXW6au9xRdULUGLSCbWNCFKL0JKBNd9qaN+xU2oRWqfc9Vf7zlgsSVrzeLwaOKlY/+HMLK3cc1xDO0bQCwzUUIRPVLm6vl567cZuNtvGDWhZodco3vPp4WHRKzd006x5WzW2bwtJUr/WoVqy7Yjq+tp+pHs0q29z3ENDC1Y7Kj5h61yXtAvXJe3CJUk3xTbVTbFNS+wTWte3zLGnhS3u17qBGT5fvK6rIoL99OVdcZq9MkmPXtZO17293Oa4+fcP0IJNKYoOD1TDQF+ln8lVh8ggmwLmWbn5+nF9svq0aqA6vl5aNmmQsvOsuui5X5STVzB2sK6vl04Wq916fc8mmnltF1ksFt03e63NJJ5pV3fW9e8sV3J6QQgOC/TV33s0Ud/WoeZ66+caEN3QXPJTku68uKWu6d5E13RvIknq2by+Nh1K149nZ4oXemdUTzOU4Pzk5RtKTDquX7ak6tu1B5Waka0ezeqZS7Y6Gledk2fVyHeXK6ZJiJ68oqN2pGbq0W82mH/IFVo2aZCa1AtQ+ulcjXh9ma7qGqk9x07rh3WHNPPazhp5UcmfiSOZ2dqcnCGr1dCBE6c1Kq65+dzeY6edFj6veO0PpWRkadKwdrp7YCunvCYA1yJ8okZofU6vbmSIv16+oSjQjunTXKF1fRXbsv65h7pcrxb1tXLPcd18NrT+o3cz1fH1UmyLBmZprIua19dFzQvammu1rXEZGeKvW86Gakf8vD317fi+Nt/7eXvqx3/101tLCsoB5Vut+r8V+/Xxn3slFczgLuyxeuWGbnrlhm6aPneLMrPzFFU/QPPuG6BdR08qOjxQAd6eNkMbCu2dcbk+XbFPHpaCcYhzNySb1QI87fSGzbi2i/q0CtWj3xTs0yasrhqH+JvP33tJa90fH635m1J093//KvU9o0hOvrXEHy1r9p2wu++N767Q6LhmysjKlb+Pl9buT9Pa/Wm655LWDheouOqNP7RowkDd8/lf2n/8tF5dXFTf9M0luzSsUyP5nh2K8/VfB7Vq7/ESdU47NwkxHycdP60uTYL1+44jahcRpMiznwGr1dDBtDNqUs+/zN7UnDyrHvtmg3mX4JctqQ7DZ1ZuvnaknlSnxkHntUoWgKrFhCPUGAfTzijA21P16vi4uymlys7L196jpxUdXrdcv/jipi8yexwrsxa5I/9euF2frdyv78b3NX/pV8QXq/Zr0v826I2buuvyLo1KPD/2o5X6ddsR/fbwQIfjCz9L2K/n52/Vf27tpS5NQsxxtU9f2VGjz/aQ/br1sM3qXXCsKsublZe9VamKq+PjqVM5RdUe7o9vo5d/2SFJalLPX3cPbKUtyRn6vxX79fLIrrqqW8HdiFPZedp5+KS6NAm2qcrxWcJ+84+YQoPbhWnS8HY6cSpHvVrUN3/eRn2QoN93HNW0qzvbvXMBoGqwvCZQQyTsPqZRH67U45e3N4OYsxmGUakeoKzcfLuluQpf+1ROfokhD6W14aUF27Ro62HNuTNOdc4eZxiGRn+4ssyxp75eHvp94iD1mrbIZvv0azpr8teOa7Y+//cuSknP0osLt5f6+qga3p4W5eY7/nVzz6DWWrA5RYczs5V2OldBfl7KyMpTv9ahuqVPc93+yepSX3/K3zrobzGNNO2nLfr27JCSdhGBmnf/AKe+D0ey8/LtTpYELiSET6AGKT6BBAW9Xx2nFpQOa98oSJ0ig8xyXcH+3lo3dYjZe/rjv/qpU+Ngm/XYC9UL8NaJs/VbFz4wQI3r+ZslyQo5oxTSv0fG6IEv1pXYfj6LNsB52oYHav4DVR8+3/99t579aYs+ubWXBkQ3PK/XMAxDv2w5rI6RQed1hwKoDljbHahBCJ626hTrRfXysOj562LM7wvHGi6ffIkWPDBAnRoHS5LiWjVQfPtw3XtJa7MXtnvTeppzZ5xev6mb2oQH2kzakgpWy/rfXX3s3pq9qmvRYg+PX97eYVv/fOQSXd2tifq1DrXZHlrXR/Htw8v7lk1v3ty9wsfAPms5+1YMw9CT32/SB8v26HROyUoTZSmsPPHQlyX/ACmvH9cna9wnqzXg7GIhQG3GbzwA1VrhxKe3/9FdjUP89c6oHpKkRsH+ii5WP9LTw6L3x/TUhCFt9d09fXXHgJaa+fcu6tWivv7WpShIvnVzdwX7e+vT23opOjxQYUF+mnZ1Z219ZpjNeZ+6opP5eFinCH03vuA1C4UH+WpMXDOzl6p3sclu/709VrPv6C1PD4ueuqKjgvy89P7onlry0EC9ckNXm/PMvLazLo5uqIggP215epgu69xI658cosHtwtQ6rK7m3z9A258dru3PDq/klbzwGCoYMnKulPQsFb/pl5iUpo//3KtnftysDlPm6/DZSU3nc77icvOtds9vT2HliOKryp2v3FLG4gLVAbPdAVRrDc5OMBvWqZGGdSo54cmeVg3r6tHL7PdWDu/cSMM6RZQYA+vn7akHL43Wiwu3a86dcfLzKfrbPC/fUExUiGKiQjS2b3N5eXgotK6PzWvc0reFdh05pcs6N1LfYr2gY/o016jezcwQXXx9+cJJXNf3jFK+1TAXiAjy89YHt1xUrvcKx3YePqn2U+bpwUujdc8lbbQlOUN3frpG+4+fVuMQfzWu56+BbRuan7FCP6xP1tg+zZV+Jlf16vgo/Uyuflx/SJd1amROeDycmaXE/WmKiQoxjyvMs7uOnNT8TSn6cNle5eZbtfKxwfpt2xE1ruevjpHB+mrNAaVmZOmfA1uZn6GKRM4f1h3S2v1pevzy9ubn6ucNyfL38VSQv7dueGeFHhwSrTsvphQVqifGfAKoln7ekKwP/9ijl2/oZlOeqaoVn1x156erlX4mV5/d3ttu6anzsWTbYd3yUcGs/opWNygc54qKW/no4BKT1BwJC/RV16gQLdicqs/H9dYHy/boly2p6tOqgT4b11uS1PPZhTp6MsfmuNC6vro/vo0e/3ajw9d++x89dNf/FYwDDvTz0vz7B+izhP1KTErTsp0Fk+3m3ttfx05lq3+bovGjM37equ8TD+qF62J009nauzFRIfr2n3105GS2ej1X8N6iw+tqe2rpCwxk5eZr+a5j6t2ygfx9mCQF52HCEQBUQ1aroUn/W6/2jYJ0a7/S67meqzLhc8EDA3Q0M9sMLoWu69HEnMxVlnsvaW1T81OSAn29tOThgbrz0zVa7aDWaHGhdX30zqieahzir97TyxcGq5uezeqV672WR2lls67v2UQ9mtWTn7enw2V6L+0QrgFtQvXEd5tKPLft2WG65s0/1aVJiKZf09ncPumr9fpidZKuiInUq+csAAJUBhOOAKAa8jg7gaqiwdORVY/F6z+39tLmp4fabN/0VNH31/VooujwQHVtGmKzzy8TLraZzFXckyM6aMlDA/XMVQVjXx+Ij9YDl0aX2O/ZqzupQV1ffXV3nxI9bRMujVZIgLfNtpWPxqtHs3rmggvnw8/bQ5+f7YF0B2cFT0ml1muds/qAJv1vg8PgKUkLN6faDZ6S9ML8bdp0KEOfr9yvuOmLdOJUjtYfSNMXq5MkSd+vO6Tfdxyxe6xUMBErO6/0MavrD6RpR2qm8vKtysu3ytn9WVm5+cpjDGutw5hPAKihujQJVsNAX10cWHB7dunDg3TdO3/q1r4tVMfXSz/f11/fJh7UPwe2llSwylUhR+ut39q3hRrU9dHouOby8LCoeWgdjerdzHy+d8v6WrH7uPl98clckvT+6J567dedevG6LmodFqg7L26pH9cl68GzM8GLD19oFxGorSmZdt9bWKCvDmdm233uzZu7K65Vg1KPh/Te73vMx8npWer2zMIS+4z6YKU+H9dbca0alHhu0v/W6+u/DmrJwwPVpF6A3v99tzKy8jTh7B8hW5IzdMXrf0gquN2flWtVk3r+5tCEysrKzVePZxYqMsRfCydcXOq+2Xn52nP0lNqGB8pisSgrN1/rD6Sre9MQcyw1qg/CJwDUELP+3kUTv1qvKX/roCu6RirY37ZXsWmDAK2YPNicxNK+UZDaNyq69eXn7alnr+qknDyrTfAMreujoydzFODjqSkjOpTahs9u762Wj86VJD14abQ8zxkLG98hXPEdikpM+Xp5ql2jQNkz7/4B+nJ1kh7+ar2C/b319x5NFOjnpdM5+ZpwabT2Hz+t6PBA5eZb1eaxn83jLmlX8Prf3dNXqenZOp2bp2Ev/24+b7EUTf5B2VbuOW43fM5ZXTAc4+M/9uqR4e3MklLX9WiiqPoBWrW36I+QwnGm+4+fVk6eVQdOnFbLYssi5+ZblZ1nLXMxiuI2HkzXqZx87Th8Upe98rs2J2fomas62fwxVOj2/6zW7zuO6vm/d9F1PaP06Ncb9PXag7pnUGs9NLStktPP6Ob3E/TsVZ3Up1XBhMD5m1KUlZuvK7s2ZpEAFyN8AkANcX3PKA3rFKEgP2+H+5S1ktU/7Pzi/r/bY/X8vG2aMKTkbfVzeXhY9M+BrTR3Q3K5V+TqGBmsV27oard4+nU9o3Rx24ZqWNe3RNsLS2l5e3qYK1g9WSwc+3p5qmmDAEnSvy5prdfOjkeNaRKixKS0crUNBY6dzNbGQxny8/LQzxtT9MjwduZzFotslkpdsfuYdh45qSkObvcPf2Wpdh05pRsuitLpnHz965LWuvTfS83nv7ijt5rUD1D9AB/5+3hq5rytemvJLn009iINahsmqeSqbJuTMyRJT3y7UcdOZmv/8dN6/u8xys236vcdR82V0R7+ar22pmTq67UHJUmv/7pT9er46JkfN0uSbnovQXtnXC6r1TAXgMjNN/TQl+v05IgOuqWvc4bDoHRMOAIA1AjpZ3JL9PYWt2TbYQX5e2vF7mOaNW+bC1tWu43r30K39muhuOmLnf7a564CtvKxwebM/bI8d3UnrU9KN8ewltfUER10Y6+mavfEvBLPLXrwYsW/9JtG9W6mTo2DNaxThJZsO6LwQF/FtizZOwxbzHYHAFyQcvKsin7857J3dDKLRRoT11xpp3PM9eXtCQv01W8PD9Lz87fpwz/2ONwPVadlwzrafeRUhY4prTRaTp5V17z1h9LP5Orn+wbYHV6wYFOKlu08qif+1uG8VrXbfeSkktOzbOoIF1q0JVVTvtukF6+PUauGdRXo52WWjHOl8uY1brsDAGoVHy8PrXosXpf++zelnS6YTX5u2Pj5vv7aefikHv5qnbJyrSWOj2vZQL+dXXXos9tjS5SoKtwvJ6/o2AX3D1Cbs0MFouoHmMMAznVVt8byPzu+tnfL+vp9x1EdSjujpg0CtOfoKZ3MypOXp8VmYhecq6LBUyoImGlncrQ+KV1dooIVFugnwzC099hpLd56WBsPFgwNePqHTZr196IqEmv2ndA9n/2l5PSClbPaRgTq5ljb4S87D2fquZ+2aPo1XRxWgrjkxd8kSa/d2E1/69JIyelZenDOOt3ar4XGfbJaknTDuyskSXV8PLVu6pBqO9mKnk8AQK1ktRp65qfNWrTlsH68t5+en7dNn67YJ6moF2tdUpqufKNgxna3piHq06qBruneRJHB/lqbdEK9mteXl6eHZs7bqqTjp/XqDd109GS2tqeeVM/m9fR94iGFBvrIz9vTnMhS6Ny6rAsfGKCdh0/qkvZhZU5uMQxDs1clqW1EoM7k5OuRr9cr6fgZ8/mQAG8zWEcG++naHk3MsPu/u+N043sJNsH4oSHR+mt/mhZvPXw+lxKSYlvUV8Keoj8IPrrlIq3Zd0Kv/1ryj4wOjYJ0S9/m6tGsngafDY2F7h3cRnNWJSklI0ujejfT+EGtbWrefjYuVt2b1tNbS3YprlUDeXta9MO6ZH38515zn+5NQ7T/+BkdPWm/IkShuff2V4dI1+UmbrsDACDbySuHM7MU5Odtc0vSMAxtOpShNuF1nTrjedXe4/p27UF5WCzy8fLQE38rvZJAWW75aKWWbDuijpFB+une/nrgi0St2ntc8+8foDq+XsrIypWXh0UBPl7Ktxrakpyhv722TPHtw/X+mJ7KyMrV94mHNKRDuO75fK1W7qFntbYL8vPS2ilDSlSlqCqETwAAapHMrFwt2XZEg9uHKcCnYNSc1WqUuvTr8VM5CvH3trvP6Zw8/efPfZo5b2uVtbnQj//qp8VbD+ulhdur/Fyw9cJ1Mfp7jyYuORcrHAEAUIsE+nlrREykGTwllRo8Jal+HR+H+wT4eOnuga30/N+7lHnuj265SLeXc1WuqPr+GtS2aF36/m1C1aFRkO4d3Eaf3NrL4XHXdi8ISIPaNlS3c1bjqozGdkp8XUi2pWS4uwklMOEIAIAL2HU9o/TwV+vtPtco2E//HNRag9qF6aIW9bXpUIaa1g/Q+EGtFRzgrTs+WW2Og7z3ktYaN6ClAv0Kyl39uu2IujcN0ae3xZqvNyC6of49MkaLtx7R83/vomOncjTitWW6sVeUHh7aTi9eXzBRJys3X3NWJ2lgdJhC6njrp/XJmvz1BknSLX2aa+We49qcnKE5d8bppvdWKM9adBN39ePxOnDijP635oD+dUlrhQT46MUF2/TO0t2SpD3TL9NXaw6oY2SwwoJ8devHq3R9zyglHT9t7lOof5tQs4ZoTdW5SYi7m1ACt90BALjADX/ld21JztDlXRrpqq6N9e+F2/XyDV3NQv+OFE7YuvPilpo8vL3NczsPn1TjEH/5+5Q+jrasoQPFHUo7o0bBfiUWJNh0KF2Xv7pMkuOSSK8v3qHgAB+7KyQVWrApRd5eHvpw2R6FB/nphetiNGdVkib+z344r26uiInU9+tsy3ztfG64y2a9M+YTAACUy+HMLM3bmKKrujUudQUte7Jy891SU/Jcry7aoYhgP13fM8rpr52ZlavOTy6w+9yXd8Xp8W82KjTQR3/sPKaxfZvroz/2lvp6258dblOL9p5Bre3Omi90e78Wen/ZHt07uI0GtW2oI5nZqlfHRz2b1dOc1Ul6d+luvT/mIjVvEKDlu46ZpcHuG9xGD1xa9splzkL4BAAAcJKYpxYo/Uyuzbbd0y6z22trGIashjT1+43q0iREA6MbKivXqsVbU3VDr6by8/Y0S3H1bxOqT2+Ltem9laQNTw5RYLE/BM5dcrQ0mVm5Wr3vhPq1Dj2vgvbni/AJAADgJHNWJ2ni2bGxc+/tr7YRgZUqYZRvNbTuQJq6NA42b4vvSM3Upf9eKslxsK3OWOEIAADASa7vGaV+rUPtjjk9H54eFnVvWs9mW+uwurqxV5TCAv1qXPCsCMInAABAOURWcdkmi8Wi6deUXfqqpqPOJwAAAFyG8AkAAACXIXwCAADAZQifAAAAcBnCJwAAAFyG8AkAAACXIXwCAADAZQifAAAAcBnCJwAAAFyG8AkAAACXIXwCAADAZQifAAAAcBnCJwAAAFyG8AkAAACX8XJ3A8rDMAxJUkZGhptbAgAAAHsKc1phbnOkRoTPzMxMSVJUVJSbWwIAAIDSZGZmKjg42OHzFqOseFoNWK1WHTp0SIGBgbJYLFV+voyMDEVFRSkpKUlBQUFVfr6aguviGNfGPq6LY1wb+7gujnFt7OO6OObqa2MYhjIzMxUZGSkPD8cjO2tEz6eHh4eaNGni8vMGBQXxQbaD6+IY18Y+rotjXBv7uC6OcW3s47o45sprU1qPZyEmHAEAAMBlCJ8AAABwGcKnHb6+vpo6dap8fX3d3ZRqheviGNfGPq6LY1wb+7gujnFt7OO6OFZdr02NmHAEAACA2oGeTwAAALgM4RMAAAAuQ/gEAACAyxA+AQAA4DKETwAAALgM4fMcb7zxhpo3by4/Pz/FxsZq5cqV7m5SlXryySdlsVhsvtq1a2c+n5WVpfHjx6tBgwaqW7eurr32WqWmptq8xv79+3X55ZcrICBAYWFhevjhh5WXl+fqt1JpS5cu1YgRIxQZGSmLxaJvv/3W5nnDMDRlyhQ1atRI/v7+io+P144dO2z2OX78uG6++WYFBQUpJCREt912m06ePGmzz/r169W/f3/5+fkpKipKs2bNquq3VillXZdbbrmlxGdo2LBhNvvUxusyffp0XXTRRQoMDFRYWJiuuuoqbdu2zWYfZ/38LFmyRN27d5evr69at26tjz/+uKrfXqWU59oMHDiwxOfmrrvustmntl2bt956S126dDFXm4mLi9PPP/9sPn+hfl6ksq/Nhfh5sWfGjBmyWCy6//77zW018nNjwDR79mzDx8fH+PDDD41NmzYZ48aNM0JCQozU1FR3N63KTJ061ejYsaORnJxsfh05csR8/q677jKioqKMRYsWGatXrzZ69+5t9OnTx3w+Ly/P6NSpkxEfH2+sXbvWmDt3rhEaGmpMnjzZHW+nUubOnWs89thjxtdff21IMr755hub52fMmGEEBwcb3377rbFu3TrjiiuuMFq0aGGcOXPG3GfYsGFGTEyMsWLFCuP33383Wrdubdx4443m8+np6UZ4eLhx8803Gxs3bjQ+//xzw9/f33jnnXdc9TYrrKzrMmbMGGPYsGE2n6Hjx4/b7FMbr8vQoUONjz76yNi4caORmJhoXHbZZUbTpk2NkydPmvs44+dn9+7dRkBAgDFhwgRj8+bNxmuvvWZ4enoa8+bNc+n7rYjyXJuLL77YGDdunM3nJj093Xy+Nl6b77//3vjpp5+M7du3G9u2bTMeffRRw9vb29i4caNhGBfu58Uwyr42F+Ln5VwrV640mjdvbnTp0sW47777zO018XND+CymV69exvjx483v8/PzjcjISGP69OlubFXVmjp1qhETE2P3ubS0NMPb29v48ssvzW1btmwxJBnLly83DKMgmHh4eBgpKSnmPm+99ZYRFBRkZGdnV2nbq9K5IctqtRoRERHG888/b25LS0szfH19jc8//9wwDMPYvHmzIclYtWqVuc/PP/9sWCwW4+DBg4ZhGMabb75p1KtXz+baTJo0yWjbtm0VvyPncBQ+r7zySofHXAjXxTAM4/Dhw4Yk47fffjMMw3k/PxMnTjQ6duxoc66RI0caQ4cOreq35DTnXhvDKAgTxX+BnutCuTb16tUz3n//fT4vdhReG8Pg85KZmWm0adPGWLhwoc21qKmfG267n5WTk6M1a9YoPj7e3Obh4aH4+HgtX77cjS2rejt27FBkZKRatmypm2++Wfv375ckrVmzRrm5uTbXpF27dmratKl5TZYvX67OnTsrPDzc3Gfo0KHKyMjQpk2bXPtGqtCePXuUkpJicy2Cg4MVGxtrcy1CQkLUs2dPc5/4+Hh5eHgoISHB3GfAgAHy8fEx9xk6dKi2bdumEydOuOjdON+SJUsUFhamtm3b6u6779axY8fM5y6U65Keni5Jql+/viTn/fwsX77c5jUK96lJ/y6de20K/fe//1VoaKg6deqkyZMn6/Tp0+Zztf3a5Ofna/bs2Tp16pTi4uL4vBRz7rUpdCF/XsaPH6/LL7+8RPtr6ufGq0petQY6evSo8vPzbf7nSFJ4eLi2bt3qplZVvdjYWH388cdq27atkpOT9dRTT6l///7auHGjUlJS5OPjo5CQEJtjwsPDlZKSIklKSUmxe80Kn6stCt+Lvfda/FqEhYXZPO/l5aX69evb7NOiRYsSr1H4XL169aqk/VVp2LBhuuaaa9SiRQvt2rVLjz76qIYPH67ly5fL09PzgrguVqtV999/v/r27atOnTpJktN+fhztk5GRoTNnzsjf378q3pLT2Ls2knTTTTepWbNmioyM1Pr16zVp0iRt27ZNX3/9taTae202bNiguLg4ZWVlqW7duvrmm2/UoUMHJSYmXvCfF0fXRrpwPy+SNHv2bP31119atWpViedq6r8zhM8L3PDhw83HXbp0UWxsrJo1a6Y5c+ZU2x9EVC833HCD+bhz587q0qWLWrVqpSVLlmjw4MFubJnrjB8/Xhs3btSyZcvc3ZRqx9G1ueOOO8zHnTt3VqNGjTR48GDt2rVLrVq1cnUzXaZt27ZKTExUenq6vvrqK40ZM0a//fabu5tVLTi6Nh06dLhgPy9JSUm67777tHDhQvn5+bm7OU7DbfezQkND5enpWWKGWGpqqiIiItzUKtcLCQlRdHS0du7cqYiICOXk5CgtLc1mn+LXJCIiwu41K3yutih8L6V9PiIiInT48GGb5/Py8nT8+PEL6nq1bNlSoaGh2rlzp6Taf13uuece/fjjj/r111/VpEkTc7uzfn4c7RMUFFTt/0B0dG3siY2NlSSbz01tvDY+Pj5q3bq1evTooenTpysmJkavvPIKnxc5vjb2XCiflzVr1ujw4cPq3r27vLy85OXlpd9++02vvvqqvLy8FB4eXiM/N4TPs3x8fNSjRw8tWrTI3Ga1WrVo0SKbMSe13cmTJ7Vr1y41atRIPXr0kLe3t8012bZtm/bv329ek7i4OG3YsMEmXCxcuFBBQUHm7ZLaoEWLFoqIiLC5FhkZGUpISLC5FmlpaVqzZo25z+LFi2W1Ws1/KOPi4rR06VLl5uaa+yxcuFBt27at9reWy+vAgQM6duyYGjVqJKn2XhfDMHTPPffom2++0eLFi0sMG3DWz09cXJzNaxTuU53/XSrr2tiTmJgoSTafm9p4bc5ltVqVnZ19QX9eHCm8NvZcKJ+XwYMHa8OGDUpMTDS/evbsqZtvvtl8XCM/N1UyjamGmj17tuHr62t8/PHHxubNm4077rjDCAkJsZkhVts8+OCDxpIlS4w9e/YYf/zxhxEfH2+EhoYahw8fNgyjoIRD06ZNjcWLFxurV6824uLijLi4OPP4whIOQ4YMMRITE4158+YZDRs2rJGlljIzM421a9caa9euNSQZL730krF27Vpj3759hmEUlFoKCQkxvvvuO2P9+vXGlVdeabfUUrdu3YyEhARj2bJlRps2bWxKCqWlpRnh4eHGqFGjjI0bNxqzZ882AgICqnVJodKuS2ZmpvHQQw8Zy5cvN/bs2WP88ssvRvfu3Y02bdoYWVlZ5mvUxuty9913G8HBwcaSJUtsyr+cPn3a3McZPz+FJVAefvhhY8uWLcYbb7xR7cvDlHVtdu7caTz99NPG6tWrjT179hjfffed0bJlS2PAgAHma9TGa/PII48Yv/32m7Fnzx5j/fr1xiOPPGJYLBZjwYIFhmFcuJ8Xwyj92lyonxdHzp35XxM/N4TPc7z22mtG06ZNDR8fH6NXr17GihUr3N2kKjVy5EijUaNGho+Pj9G4cWNj5MiRxs6dO83nz5w5Y/zzn/806tWrZwQEBBhXX321kZycbPMae/fuNYYPH274+/sboaGhxoMPPmjk5ua6+q1U2q+//mpIKvE1ZswYwzAKyi098cQTRnh4uOHr62sMHjzY2LZtm81rHDt2zLjxxhuNunXrGkFBQcbYsWONzMxMm33WrVtn9OvXz/D19TUaN25szJgxw1Vv8byUdl1Onz5tDBkyxGjYsKHh7e1tNGvWzBg3blyJP9hq43Wxd00kGR999JG5j7N+fn799Veja9euho+Pj9GyZUubc1RHZV2b/fv3GwMGDDDq169v+Pr6Gq1btzYefvhhm7qNhlH7rs2tt95qNGvWzPDx8TEaNmxoDB482AyehnHhfl4Mo/Rrc6F+Xhw5N3zWxM+NxTAMo2r6VAEAAABbjPkEAACAyxA+AQAA4DKETwAAALgM4RMAAAAuQ/gEAACAyxA+AQAA4DKETwAAALgM4RMAAAAuQ/gEAACAyxA+AQAA4DKETwAAALjM/wPk2dSUjq6pYgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the model"
      ],
      "metadata": {
        "id": "PG2PlHeuirzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {\n",
        "    'model': reg_model.state_dict()\n",
        "}\n",
        "torch.save(checkpoint, MODEL_PATH/\"MLP_SMALL_V2.pt\")"
      ],
      "metadata": {
        "id": "gR67fZbQivTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert False, \"stop here\""
      ],
      "metadata": {
        "id": "C4FZn5534ptw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "633ab4da-edb3-4b88-e030-77d6a4bf4dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "stop here",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-f68b657fa1b2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stop here\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: stop here"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "# Validation"
      ],
      "metadata": {
        "id": "xpm06Cs0XcDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reg_model.eval()"
      ],
      "metadata": {
        "id": "r4F8C6wz-q0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference_train_loader = train_loader"
      ],
      "metadata": {
        "id": "vVQL_LHdqwZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions = []\n",
        "train_targets = []\n",
        "used_train_eras = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(inference_train_loader):\n",
        "        features, targets, _, eras = batch\n",
        "        features = features.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=dtype):\n",
        "            t_predictions, _ = reg_model(features)\n",
        "\n",
        "        predictions = t_predictions.squeeze(-1).float().cpu().numpy()\n",
        "        targets = targets.numpy()\n",
        "\n",
        "        train_predictions.append(predictions)\n",
        "        train_targets.append(targets)\n",
        "        used_train_eras.append(eras)\n",
        "\n",
        "train_predictions = np.concatenate(train_predictions, axis=0)\n",
        "train_targets = np.concatenate(train_targets, axis=0)\n",
        "used_train_eras = np.concatenate(used_train_eras, axis=0)"
      ],
      "metadata": {
        "id": "o-9aEZz6q3ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_preds = pd.DataFrame(\n",
        "    data = {\n",
        "        'era': [str(era).zfill(4) for era in used_train_eras],\n",
        "        'target': train_targets,\n",
        "        MODEL_NAME: train_predictions\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "Fy6bwxjatn4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_preds[MODEL_NAME].describe()"
      ],
      "metadata": {
        "id": "bfp1PiUft_iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style=\"darkgrid\", context=\"talk\", palette=\"rainbow\", font_scale=.7)\n",
        "sns.histplot(data=df_train_preds, x=MODEL_NAME, bins=200)\n",
        "\n",
        "# Customize the plot to match your original settings\n",
        "plt.title(\"Train predictions\")\n",
        "plt.xlabel('Observation')   # Remove x-axis label\n",
        "plt.ylabel('')  # Optional: Set y-axis label\n",
        "plt.legend([], [], frameon=False)  # Remove legend\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4FPlDwoUsNfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zox0obM3Kh41"
      },
      "outputs": [],
      "source": [
        "per_era_corr = df_train_preds.groupby(\"era\").apply(\n",
        "    lambda x: numerai_corr(x[[MODEL_NAME]].dropna(), x['target'].dropna()),\n",
        "    include_groups=False\n",
        ")\n",
        "\n",
        "sns.set(style=\"darkgrid\", context=\"talk\", palette=\"rainbow\", font_scale=.7)\n",
        "sns.barplot(data=per_era_corr, x='era', y=MODEL_NAME)\n",
        "\n",
        "# Customize the plot to match your original settings\n",
        "plt.title(\"Train CORR\")\n",
        "plt.xticks([])  # Remove x-tick labels\n",
        "plt.xlabel('era')   # Remove x-axis label\n",
        "plt.ylabel('')  # Optional: Set y-axis label\n",
        "plt.legend([], [], frameon=False)  # Remove legend\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style=\"darkgrid\", context=\"talk\", palette=\"rainbow\", font_scale=.7)\n",
        "sns.lineplot(data=per_era_corr.cumsum(), x='era', y=MODEL_NAME)\n",
        "\n",
        "# Customize the plot to match your original settings\n",
        "plt.title(\"Cumulative Train CORR\")\n",
        "plt.xticks([])  # Remove x-tick labels\n",
        "plt.xlabel('era')   # Remove x-axis label\n",
        "plt.ylabel('')  # Optional: Set y-axis label\n",
        "plt.legend([], [], frameon=False)  # Remove legend\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jwZlqthRh4DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "DwfIv089h6fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inference_valid_loader = valid_loader"
      ],
      "metadata": {
        "id": "qb3kduWAo5l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_predictions = []\n",
        "valid_targets = []\n",
        "used_valid_eras = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(inference_valid_loader):\n",
        "        features, targets, _, eras = batch\n",
        "        features = features.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=dtype):\n",
        "            t_predictions, _ = reg_model(features)\n",
        "\n",
        "        predictions = t_predictions.squeeze(-1).float().cpu().numpy()\n",
        "        targets = targets.numpy()\n",
        "\n",
        "        valid_predictions.append(predictions)\n",
        "        valid_targets.append(targets)\n",
        "        used_valid_eras.append(eras)\n",
        "\n",
        "valid_predictions = np.concatenate(valid_predictions, axis=0)\n",
        "valid_targets = np.concatenate(valid_targets, axis=0)\n",
        "used_valid_eras = np.concatenate(used_valid_eras, axis=0)"
      ],
      "metadata": {
        "id": "R9-_LaW5pS2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_validation_preds = pd.DataFrame(\n",
        "    data = {\n",
        "        'era': [str(era).zfill(4) for era in used_valid_eras],\n",
        "        'target': valid_targets,\n",
        "        MODEL_NAME: valid_predictions\n",
        "    }\n",
        ").dropna()"
      ],
      "metadata": {
        "id": "JFxyx66VvSlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_validation_preds[MODEL_NAME].describe()"
      ],
      "metadata": {
        "id": "0aib3f5qJKSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9hlkxJyKh44"
      },
      "outputs": [],
      "source": [
        "# Compute the per-era corr between our predictions and the target values\n",
        "per_era_corr = df_validation_preds.groupby(\"era\").apply(\n",
        "    lambda x: numerai_corr(x[[MODEL_NAME]].dropna(), x['target'].dropna()),\n",
        "    include_groups=False\n",
        ")\n",
        "\n",
        "sns.set(style=\"darkgrid\", context=\"talk\", palette=\"rainbow\", font_scale=.7)\n",
        "sns.barplot(data=per_era_corr, x='era', y=MODEL_NAME)\n",
        "\n",
        "# Customize the plot to match your original settings\n",
        "plt.title(\"Validation CORR\")\n",
        "plt.xticks([])  # Remove x-tick labels\n",
        "plt.xlabel('era')   # Remove x-axis label\n",
        "plt.ylabel('')  # Optional: Set y-axis label\n",
        "plt.legend([], [], frameon=False)  # Remove legend\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4btBvX0bKh46"
      },
      "outputs": [],
      "source": [
        "sns.set(style=\"darkgrid\", context=\"talk\", palette=\"rainbow\", font_scale=.7)\n",
        "sns.lineplot(data=per_era_corr.cumsum(), x='era', y=MODEL_NAME)\n",
        "\n",
        "# Customize the plot to match your original settings\n",
        "plt.title(\"Cumulative Validation CORR\")\n",
        "plt.xticks([])  # Remove x-tick labels\n",
        "plt.xlabel('era')   # Remove x-axis label\n",
        "plt.ylabel('')  # Optional: Set y-axis label\n",
        "plt.legend([], [], frameon=False)  # Remove legend\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn-jbD9GKh5C"
      },
      "outputs": [],
      "source": [
        "# Compute performance metrics\n",
        "corr_mean = per_era_corr.mean()\n",
        "corr_std = per_era_corr.std(ddof=0)\n",
        "corr_sharpe = corr_mean / corr_std\n",
        "corr_max_drawdown = (per_era_corr.cumsum().expanding(min_periods=1).max() - per_era_corr.cumsum()).max()\n",
        "\n",
        "pd.DataFrame({\n",
        "    \"mean\": [corr_mean.values[0]],\n",
        "    \"std\": [corr_std.values[0]],\n",
        "    \"sharpe\": [corr_sharpe.values[0]],\n",
        "    \"max_drawdown\": [corr_max_drawdown.values[0]]\n",
        "}, index=[\"CORR\"]).T"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b2DfL7ZKo8B-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}