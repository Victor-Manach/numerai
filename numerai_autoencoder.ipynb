{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1RAUwnCbdGmlW8xV83vxgxj3wFMGu-0gF",
      "authorship_tag": "ABX9TyNz5ZKSKD/1t9zsETLznJdb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Victor-Manach/numerai/blob/main/numerai_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q numerapi"
      ],
      "metadata": {
        "id": "_jyKOP6w9uJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --no-deps numerai-tools"
      ],
      "metadata": {
        "id": "p7QFmq9M9oMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Any, Literal, Optional, Tuple, Callable\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "import math\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset, Sampler\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from numerai_tools.scoring import numerai_corr, correlation_contribution\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import h5py\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 5)"
      ],
      "metadata": {
        "id": "967lWY0-Wj05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_drive = Path('/content/drive/My Drive')"
      ],
      "metadata": {
        "id": "A0WsbcEFWjdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"{device=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v63MVMfibQDK",
        "outputId": "01196910-bb27-48ce-f9a6-a26d5ba640c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device='cuda'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQQNsJzFWcG7"
      },
      "outputs": [],
      "source": [
        "DATA_VERSION = \"v5.0\"\n",
        "MODEL_NAME = \"MLP_RECONSTRUCT\"\n",
        "PATH_TO_DATA = path_to_drive / f\"numerai/data/{DATA_VERSION}\"\n",
        "SKIP_TRAINING = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_path = PATH_TO_DATA / \"features.json\"\n",
        "TRAIN_PATH = PATH_TO_DATA / \"train/train_v2.h5\"\n",
        "VALID_PATH = PATH_TO_DATA / \"valid/valid_v2.h5\"\n",
        "\n",
        "MODEL_PATH = path_to_drive / f\"numerai/models\""
      ],
      "metadata": {
        "id": "puFZg1nlWtsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_metadata = json.load(open(features_path))\n",
        "all_target_cols = feature_metadata[\"targets\"]\n",
        "feature_sets = feature_metadata[\"feature_sets\"]\n",
        "\n",
        "sm_feature_set = feature_sets[\"small\"]\n",
        "med_feature_set = feature_sets[\"medium\"]\n",
        "all_feature_set = feature_sets[\"all\"]\n",
        "\n",
        "ERA_COL = \"era\"\n",
        "MAIN_TARGET = \"target\"\n",
        "AUXILIARY_TARGETS = [\n",
        "  \"target_victor_20\",\n",
        "  \"target_xerxes_20\",\n",
        "  \"target_teager2b_20\",\n",
        "  \"target_caroline_20\",\n",
        "  \"target_sam_20\",\n",
        "  \"target_echo_20\",\n",
        "  \"target_alpha_20\",\n",
        "  \"target_jeremy_20\"\n",
        "]\n",
        "\n",
        "feature_set = med_feature_set"
      ],
      "metadata": {
        "id": "UqWtdRgZWz7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_features_id(features_to_use, h5_file):\n",
        "    feature_to_id_file = h5_file.parent / \"feature_to_id.json\"\n",
        "    with open(feature_to_id_file, 'r') as f:\n",
        "        feature_to_id = json.load(f)\n",
        "    features_id = [feature_to_id[f] for f in features_to_use]\n",
        "\n",
        "    return features_id\n",
        "\n",
        "def load_targets_id(targets_to_use, h5_file):\n",
        "    target_to_id_file = h5_file.parent / \"target_to_id.json\"\n",
        "    with open(target_to_id_file, 'r') as f:\n",
        "        target_to_id = json.load(f)\n",
        "    targets_id = [target_to_id[t] for t in targets_to_use]\n",
        "\n",
        "    return targets_id\n",
        "\n",
        "def load_target_id(target_to_use, h5_file):\n",
        "    target_to_id_file = h5_file.parent / \"target_to_id.json\"\n",
        "    with open(target_to_id_file, 'r') as f:\n",
        "        target_to_id = json.load(f)\n",
        "    target_id = target_to_id[target_to_use]\n",
        "\n",
        "    return target_id\n",
        "\n",
        "def load_eras_data(keep_eras, h5_file):\n",
        "    era_vector = []\n",
        "    with h5py.File(h5_file, 'r') as f:\n",
        "        era_ix = f['era_index']\n",
        "        idx_list = []\n",
        "        if keep_eras is not None:\n",
        "            for era in keep_eras:\n",
        "                era_key = str(era).zfill(4)\n",
        "                if era_key in era_ix:\n",
        "                    idx_list.append(era_ix[era_key][:])\n",
        "                    era_vector.append([era]*len(era_ix[era_key]))\n",
        "            if idx_list:\n",
        "                indices = np.concatenate(idx_list)\n",
        "            else:\n",
        "                indices = np.array([], dtype=np.int64)\n",
        "            features = f['features'][indices, :]\n",
        "        else:\n",
        "            features = f['features'][:]\n",
        "\n",
        "    era_vector = np.concatenate(era_vector)\n",
        "    return features, era_vector\n",
        "\n",
        "def load_first_n_eras_data(n, h5_file):\n",
        "    era_vector = []\n",
        "    with h5py.File(h5_file, 'r') as f:\n",
        "        era_ix = f['era_index']\n",
        "        era_keys = sorted(list(era_ix.keys()))\n",
        "        first_n_keys = era_keys[:n]\n",
        "        idx_list = []\n",
        "        for era_key in first_n_keys:\n",
        "            idx_list.append(era_ix[era_key][:])\n",
        "            era_vector.append([int(era_key)]*len(era_ix[era_key]))\n",
        "        if idx_list:\n",
        "            indices = np.concatenate(idx_list)\n",
        "        else:\n",
        "            indices = np.array([], dtype=np.int64)\n",
        "        features = f['features'][indices, :]\n",
        "\n",
        "    era_vector = np.concatenate(era_vector)\n",
        "    return features, era_vector, [int(e) for e in first_n_keys]\n",
        "\n",
        "def load_eras_data_and_filter_columns(keep_eras, h5_file, features_to_use):\n",
        "    features, era_vector = load_eras_data(keep_eras, h5_file)\n",
        "    features_id = load_features_id(features_to_use, h5_file)\n",
        "\n",
        "    return features[:, features_id], era_vector\n",
        "\n",
        "def load_first_n_eras_data_and_filter_columns(n, h5_file, features_to_use):\n",
        "    features, era_vector, first_n_eras = load_first_n_eras_data(n, h5_file)\n",
        "    features_id = load_features_id(features_to_use, h5_file)\n",
        "\n",
        "    return features[:, features_id], era_vector, first_n_eras"
      ],
      "metadata": {
        "id": "H9s3DqTG98OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_eras = [i for i in range(500, 900)]\n",
        "x_train_np, train_era_vector_np = load_eras_data_and_filter_columns(train_eras, TRAIN_PATH, feature_set)"
      ],
      "metadata": {
        "id": "lBdEXYOEbKJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.tensor(x_train_np, dtype=torch.float32)\n",
        "train_era_vector_pt = torch.tensor(train_era_vector_np, dtype=torch.int32)\n",
        "\n",
        "x_train.shape, train_era_vector_pt.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsQZ25oyXYAW",
        "outputId": "a600b4b8-4286-44ce-b1b2-d7b32210072b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2317942, 705]), torch.Size([2317942]))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_era_vector_pt.min(), train_era_vector_pt.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qs29MUZNDVvZ",
        "outputId": "74e98047-a6e0-41d9-da72-a86bb1b432d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(500, dtype=torch.int32), tensor(899, dtype=torch.int32))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = (x_train - 2.0) / 2.0"
      ],
      "metadata": {
        "id": "FXKCUuFvfggZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.float()"
      ],
      "metadata": {
        "id": "I9Ud7zgpv5YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 80\n",
        "x_valid_np, valid_era_vector_np, valid_eras = load_first_n_eras_data_and_filter_columns(n, VALID_PATH, feature_set)"
      ],
      "metadata": {
        "id": "lI8VQ3FcFwkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_valid = torch.tensor(x_valid_np, dtype=torch.float32)\n",
        "valid_era_vector_pt = torch.tensor(valid_era_vector_np, dtype=torch.int32)\n",
        "\n",
        "x_valid.shape, valid_era_vector_pt.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXFblsSvfGl4",
        "outputId": "dfaf9e94-fa7a-4d87-a99a-db642bef3e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([527296, 705]), torch.Size([527296]))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_era_vector_pt.min(), valid_era_vector_pt.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A878ZC0DdLe",
        "outputId": "1ed09e2c-a5fd-4f19-db76-ff03a44caebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(975, dtype=torch.int32), tensor(1054, dtype=torch.int32))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_valid = (x_valid - 2.0) / 2.0"
      ],
      "metadata": {
        "id": "oPed6Toxfmd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_valid = x_valid.float()"
      ],
      "metadata": {
        "id": "_fG_qz48fJsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, device=None, dtype=None):\n",
        "        super().__init__()\n",
        "\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        self.ln = nn.LayerNorm(input_dim, **factory_kwargs)\n",
        "        self.c_fc = nn.Linear(input_dim, input_dim, bias=True, **factory_kwargs)\n",
        "        self.c_proj = nn.Linear(input_dim, output_dim, bias=True, **factory_kwargs)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ln(x)\n",
        "        x = x + self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, config, device=None, dtype=None):\n",
        "        super().__init__()\n",
        "\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        hidden_dims = [config.n_embd//2**i for i in range(config.n_layers+1)]\n",
        "        self.embedding = nn.Linear(config.input_dim, config.n_embd, bias=True, **factory_kwargs)\n",
        "        self.encoder_layers = nn.ModuleList([FFN(hidden_dims[i], hidden_dims[i+1], **factory_kwargs) for i in range(len(hidden_dims)-1)])\n",
        "        self.decoder_layers = nn.ModuleList([FFN(hidden_dims[i], hidden_dims[i-1], **factory_kwargs) for i in range(len(hidden_dims)-1, 0, -1)])\n",
        "        self.decoder = nn.Linear(config.n_embd, config.input_dim, bias=True, **factory_kwargs)\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.embedding(x)\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def decode(self, z):\n",
        "        for layer in self.decoder_layers:\n",
        "            z = layer(z)\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, mask = self.apply_mask(x)\n",
        "        z = self.encode(x)\n",
        "        return self.decode(z), mask\n",
        "\n",
        "    def apply_mask(self, x):\n",
        "        mask = torch.rand_like(x) < self.config.mask_ratio\n",
        "        noise = torch.randn_like(x) * self.config.noise_std\n",
        "        x_masked = x.clone()\n",
        "        x_masked[mask] = noise[mask]\n",
        "        return x_masked, mask\n",
        "\n",
        "    def configure_optimizer(self, weight_decay, learning_rate, betas, device_type):\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2] # all weight tensors in matmuls + embeddings decay\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2] # all biases and layernorms\n",
        "\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "\n",
        "        use_fused = (device_type == 'cuda')\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer"
      ],
      "metadata": {
        "id": "cKEp06usg7su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MlpBlock(nn.Module):\n",
        "    def __init__(self, config, device=None, dtype=None):\n",
        "        super().__init__()\n",
        "\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        self.ln = nn.LayerNorm(config.n_embd, **factory_kwargs)\n",
        "        self.c_fc = nn.Linear(config.n_embd, config.n_embd, bias=config.bias, **factory_kwargs)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias, **factory_kwargs)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.ln_pre_activation = x.clone()\n",
        "        x = self.ln(x)\n",
        "        self.ln_activation = x.clone()\n",
        "        x = self.c_fc(x)\n",
        "        self.gelu_pre_activation = x.clone()\n",
        "        x = self.gelu(x)\n",
        "        self.gelu_activation = x.clone()\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class MlpReconstruct(nn.Module):\n",
        "    def __init__(self, config, device=None, dtype=None):\n",
        "        super().__init__()\n",
        "\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        self.embedding = nn.Linear(config.input_dim, config.n_embd, bias=True, **factory_kwargs)\n",
        "        self.layers = nn.ModuleList([MlpBlock(config, **factory_kwargs) for _ in range(config.n_layers)])\n",
        "\n",
        "        self.main_proj = nn.Linear(config.n_embd, config.input_dim, bias=True, **factory_kwargs)\n",
        "        self.aux_proj = nn.Identity()\n",
        "\n",
        "        self.config = config\n",
        "        self.stats = {}\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, mask = self.apply_mask(x)\n",
        "        x = self.embedding(x)\n",
        "        self.post_embedding = x.clone()\n",
        "        for i,layer in enumerate(self.layers):\n",
        "            x_ffn = layer(x)\n",
        "            self.stats[f\"block{i}_ffn\"] = (x_ffn.mean(), x_ffn.std(), x_ffn.min(), x_ffn.max())\n",
        "            self.stats[f\"block{i}_res\"] = (x.mean(), x.std(), x.min(), x.max())\n",
        "            x = x + x_ffn\n",
        "\n",
        "        self.post_core = x.clone()\n",
        "\n",
        "        return self.main_proj(x), self.aux_proj(x), mask\n",
        "\n",
        "    def apply_mask(self, x):\n",
        "        mask = torch.rand_like(x) < self.config.mask_ratio\n",
        "        noise = torch.randn_like(x) * self.config.noise_std\n",
        "        x_masked = x.clone()\n",
        "        x_masked[mask] = noise[mask]\n",
        "        return x_masked, mask\n",
        "\n",
        "    def configure_optimizer(self, weight_decay, learning_rate, betas, device_type):\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2] # all weight tensors in matmuls + embeddings decay\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2] # all biases and layernorms\n",
        "\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "\n",
        "        use_fused = (device_type == 'cuda')\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer"
      ],
      "metadata": {
        "id": "G4YnN-N5ILmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class AutoEncoderConfig:\n",
        "    input_dim: int\n",
        "    latent_dim: int\n",
        "    n_layers: int\n",
        "    n_embd: int\n",
        "    dropout: float\n",
        "    bias: bool\n",
        "    mask_ratio: float\n",
        "    noise_std: float"
      ],
      "metadata": {
        "id": "T-Xdr4n9aPhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class MlpReconstructConfig:\n",
        "    input_dim: int\n",
        "    main_output_dim: int\n",
        "    aux_output_dim: int\n",
        "    n_layers: int\n",
        "    n_embd: int\n",
        "    dropout: float\n",
        "    bias: bool\n",
        "    mask_ratio: float\n",
        "    noise_std: float"
      ],
      "metadata": {
        "id": "UHC70SRJIVlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# config = AutoEncoderConfig(\n",
        "#     input_dim=len(feature_set),\n",
        "#     latent_dim=64,\n",
        "#     n_layers=2,\n",
        "#     n_embd=256,\n",
        "#     dropout=0.0,\n",
        "#     bias=True,\n",
        "#     mask_ratio=0.3,\n",
        "#     noise_std=0.1\n",
        "# )\n",
        "\n",
        "config = MlpReconstructConfig(\n",
        "    input_dim=len(feature_set),\n",
        "    main_output_dim=1,\n",
        "    aux_output_dim=len(AUXILIARY_TARGETS),\n",
        "    n_layers=4,\n",
        "    n_embd=256,\n",
        "    dropout=0.0,\n",
        "    bias=True,\n",
        "    mask_ratio=0.4,\n",
        "    noise_std=0.2\n",
        ")"
      ],
      "metadata": {
        "id": "1Nj_wBrUalqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 512\n",
        "total_batch_size = 721920\n",
        "assert total_batch_size % (batch_size*config.input_dim) == 0\n",
        "grad_accum_steps = total_batch_size // (batch_size*config.input_dim)\n",
        "print(f\"Total desired batch size: {total_batch_size}\")\n",
        "print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")"
      ],
      "metadata": {
        "id": "srEo7sJKcVO2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc32ce5e-3162-4dc9-de7d-06985faa5ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total desired batch size: 721920\n",
            "=> calculated gradient accumulation steps: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MlpReconstruct(config).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.main_proj.weight *= 3.0\n",
        "\n",
        "model = torch.compile(model)"
      ],
      "metadata": {
        "id": "9FpY-yiEbCNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Number of parameters: {n_params:,}\")"
      ],
      "metadata": {
        "id": "vi0WTnQ3cxzS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27b9323f-a418-4fdb-d839-bbdb1294aebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 890,305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 10000\n",
        "val_loss_n_steps = 200\n",
        "learning_rate = 6e-4\n",
        "decay_lr = False\n",
        "warmup_iters = 400\n",
        "lr_decay_iters = max_steps # should be ~= max_iters\n",
        "min_lr = 6e-5 # should be ~= learning_rate/10\n",
        "weight_decay = 1e-1\n",
        "betas = (0.9, 0.999)\n",
        "epsilon = 1e-8\n",
        "lambda_coef = 10.0\n",
        "desired_variance = 0.05\n",
        "gamma = 3.0\n",
        "\n",
        "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "np.random.seed(1337)\n",
        "torch.manual_seed(1337)\n",
        "torch.cuda.manual_seed_all(1337)"
      ],
      "metadata": {
        "id": "5lBDjdCgcdU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(x_train, train_era_vector_pt)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "imVhskatZdqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataset = TensorDataset(x_valid, valid_era_vector_pt)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "kZtfiXbDB5EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset), len(valid_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjsSZp42HolI",
        "outputId": "db508f98-1243-4973-9eb3-f895ad93c9ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2317942, 527296)"
            ]
          },
          "metadata": {},
          "execution_count": 303
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader), len(valid_loader)"
      ],
      "metadata": {
        "id": "7VQByG3aB5EU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0087465e-ed2a-4dbf-8647-e83533ac3e47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4528, 1030)"
            ]
          },
          "metadata": {},
          "execution_count": 304
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = model.configure_optimizer(weight_decay, learning_rate, betas, device)"
      ],
      "metadata": {
        "id": "gWQ1-uAFcslH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "241a9195-c193-4fa4-a50a-1925490007d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 10, with 885,248 parameters\n",
            "num non-decayed parameter tensors: 18, with 5,057 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr(it, warmup_iters, learning_rate, lr_decay_iters, min_lr):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * (it + 1) / (warmup_iters + 1)\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "get_lr_with_default_values = lambda it: get_lr(it, warmup_iters, learning_rate, lr_decay_iters, min_lr)"
      ],
      "metadata": {
        "id": "YXrkMSHJcxIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_mse_loss(pred, target, mask):\n",
        "    return ((pred - target)**2 * mask.float()).sum() / mask.float().sum().clamp_min(1e-8)"
      ],
      "metadata": {
        "id": "7Ru1XSWrm7tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = masked_mse_loss"
      ],
      "metadata": {
        "id": "4fWJaHEcmflX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# debug cell\n",
        "# train_losses, valid_losses = [], []\n",
        "\n",
        "# train_loader_iter = iter(train_loader)\n",
        "# for step in range(max_steps):\n",
        "#     model.train(True)\n",
        "#     lr = get_lr_with_default_values(step) if decay_lr else learning_rate\n",
        "#     for param_group in optimizer.param_groups:\n",
        "#         param_group['lr'] = lr\n",
        "\n",
        "#     optimizer.zero_grad()\n",
        "#     xb, _ = next(train_loader_iter)\n",
        "#     xb = xb.to(device)\n",
        "\n",
        "#     with torch.autocast(device_type=device, dtype=dtype):\n",
        "#         xb_hat, _, mask = model(xb)\n",
        "#         loss = loss_fn(xb_hat, xb, mask)\n",
        "\n",
        "#     loss.backward()\n",
        "#     norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "#     optimizer.step()\n",
        "\n",
        "#     break"
      ],
      "metadata": {
        "id": "2NxKpLeHjMld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assert False, \"stop debug here\""
      ],
      "metadata": {
        "id": "-yRMrkotjcdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, valid_losses = [], []\n",
        "\n",
        "train_loader_iter = iter(train_loader)\n",
        "for step in range(max_steps):\n",
        "    t0 = time.time()\n",
        "    last_step = (step == max_steps - 1)\n",
        "\n",
        "    if step % val_loss_n_steps == 0 or last_step:\n",
        "        model.eval()\n",
        "        valid_loader_iter = iter(valid_loader)\n",
        "        with torch.no_grad():\n",
        "            val_loss_accum = 0.0\n",
        "            val_loss_steps = len(valid_loader)\n",
        "            for _ in range(val_loss_steps):\n",
        "                xb, _ = next(valid_loader_iter)\n",
        "                xb = xb.to(device)\n",
        "                with torch.autocast(device_type=device, dtype=dtype):\n",
        "                    xb_hat, _, mask = model(xb)\n",
        "                    loss = loss_fn(xb_hat, xb, mask)\n",
        "                loss = loss / val_loss_steps\n",
        "                val_loss_accum += loss.detach()\n",
        "        print(f\"step {step+1:3d}/{max_steps:3d} | val_loss={val_loss_accum.item():.4f}\")\n",
        "        valid_losses.append(val_loss_accum.item())\n",
        "\n",
        "    model.train(True)\n",
        "    lr = get_lr_with_default_values(step) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss_accum = 0.0\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "        try:\n",
        "            xb, _ = next(train_loader_iter)\n",
        "        except StopIteration:\n",
        "            train_loader_iter = iter(train_loader)\n",
        "            xb, _ = next(train_loader_iter)\n",
        "\n",
        "        xb = xb.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=dtype):\n",
        "            xb_hat, _, mask = model(xb)\n",
        "            loss = loss_fn(xb_hat, xb, mask)\n",
        "\n",
        "        loss = loss / grad_accum_steps\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    if step % 50 == 0 or last_step:\n",
        "        print(f\"step {step+1:3d}/{max_steps:3d} | loss={loss_accum.item():.6f} | lr={lr:.6e} | norm={norm:.4f} | dt={dt*1000:.2f}ms\")\n",
        "\n",
        "    train_losses.append(loss_accum.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIq8WG8Vb773",
        "outputId": "b869797e-50cf-4645-b41f-b4081e5ab706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step   1/10000 | val_loss=1.3127\n",
            "step   1/10000 | loss=1.315024 | lr=1.496259e-06 | norm=0.8468 | dt=7972.38ms\n",
            "step  51/10000 | loss=0.922302 | lr=7.630923e-05 | norm=0.3333 | dt=48.18ms\n",
            "step 101/10000 | loss=0.627861 | lr=1.511222e-04 | norm=0.1773 | dt=42.66ms\n",
            "step 151/10000 | loss=0.472197 | lr=2.259352e-04 | norm=0.1185 | dt=43.51ms\n",
            "step 201/10000 | val_loss=0.3846\n",
            "step 201/10000 | loss=0.381074 | lr=3.007481e-04 | norm=0.0919 | dt=8115.40ms\n",
            "step 251/10000 | loss=0.329336 | lr=3.755611e-04 | norm=0.1038 | dt=41.92ms\n",
            "step 301/10000 | loss=0.304285 | lr=4.503741e-04 | norm=0.1396 | dt=41.84ms\n",
            "step 351/10000 | loss=0.284179 | lr=5.251870e-04 | norm=0.1525 | dt=42.94ms\n",
            "step 401/10000 | val_loss=0.2752\n",
            "step 401/10000 | loss=0.270028 | lr=6.000000e-04 | norm=0.1558 | dt=7796.67ms\n",
            "step 451/10000 | loss=0.260854 | lr=5.999639e-04 | norm=0.1502 | dt=42.65ms\n",
            "step 501/10000 | loss=0.251706 | lr=5.998554e-04 | norm=0.1390 | dt=42.04ms\n",
            "step 551/10000 | loss=0.248780 | lr=5.996748e-04 | norm=0.1425 | dt=47.46ms\n",
            "step 601/10000 | val_loss=0.2461\n",
            "step 601/10000 | loss=0.242120 | lr=5.994219e-04 | norm=0.1320 | dt=7671.89ms\n",
            "step 651/10000 | loss=0.236486 | lr=5.990969e-04 | norm=0.1446 | dt=48.68ms\n",
            "step 701/10000 | loss=0.230194 | lr=5.986999e-04 | norm=0.1421 | dt=41.75ms\n",
            "step 751/10000 | loss=0.232027 | lr=5.982309e-04 | norm=0.1370 | dt=42.68ms\n",
            "step 801/10000 | val_loss=0.2327\n",
            "step 801/10000 | loss=0.226999 | lr=5.976901e-04 | norm=0.1344 | dt=7444.75ms\n",
            "step 851/10000 | loss=0.224860 | lr=5.970777e-04 | norm=0.1344 | dt=42.29ms\n",
            "step 901/10000 | loss=0.220145 | lr=5.963937e-04 | norm=0.1156 | dt=41.81ms\n",
            "step 951/10000 | loss=0.219970 | lr=5.956384e-04 | norm=0.1189 | dt=46.71ms\n",
            "step 1001/10000 | val_loss=0.2245\n",
            "step 1001/10000 | loss=0.217504 | lr=5.948120e-04 | norm=0.1224 | dt=7766.85ms\n",
            "step 1051/10000 | loss=0.218417 | lr=5.939147e-04 | norm=0.1200 | dt=43.52ms\n",
            "step 1101/10000 | loss=0.216640 | lr=5.929468e-04 | norm=0.1078 | dt=46.88ms\n",
            "step 1151/10000 | loss=0.214552 | lr=5.919084e-04 | norm=0.1022 | dt=42.93ms\n",
            "step 1201/10000 | val_loss=0.2185\n",
            "step 1201/10000 | loss=0.211660 | lr=5.908000e-04 | norm=0.0976 | dt=8023.97ms\n",
            "step 1251/10000 | loss=0.214545 | lr=5.896217e-04 | norm=0.1079 | dt=42.34ms\n",
            "step 1301/10000 | loss=0.209924 | lr=5.883739e-04 | norm=0.1015 | dt=41.84ms\n",
            "step 1351/10000 | loss=0.211829 | lr=5.870569e-04 | norm=0.0901 | dt=41.75ms\n",
            "step 1401/10000 | val_loss=0.2144\n",
            "step 1401/10000 | loss=0.206434 | lr=5.856711e-04 | norm=0.0948 | dt=7751.26ms\n",
            "step 1451/10000 | loss=0.207951 | lr=5.842169e-04 | norm=0.0934 | dt=44.03ms\n",
            "step 1501/10000 | loss=0.207241 | lr=5.826946e-04 | norm=0.0904 | dt=42.99ms\n",
            "step 1551/10000 | loss=0.205508 | lr=5.811047e-04 | norm=0.0824 | dt=43.64ms\n",
            "step 1601/10000 | val_loss=0.2107\n",
            "step 1601/10000 | loss=0.205799 | lr=5.794475e-04 | norm=0.0858 | dt=7696.35ms\n",
            "step 1651/10000 | loss=0.203174 | lr=5.777235e-04 | norm=0.0815 | dt=44.08ms\n",
            "step 1701/10000 | loss=0.201879 | lr=5.759332e-04 | norm=0.0814 | dt=43.27ms\n",
            "step 1751/10000 | loss=0.201701 | lr=5.740771e-04 | norm=0.0782 | dt=41.61ms\n",
            "step 1801/10000 | val_loss=0.2078\n",
            "step 1801/10000 | loss=0.202801 | lr=5.721556e-04 | norm=0.0822 | dt=8176.41ms\n",
            "step 1851/10000 | loss=0.201564 | lr=5.701693e-04 | norm=0.0769 | dt=42.36ms\n",
            "step 1901/10000 | loss=0.201042 | lr=5.681187e-04 | norm=0.0749 | dt=42.41ms\n",
            "step 1951/10000 | loss=0.199571 | lr=5.660044e-04 | norm=0.0751 | dt=50.08ms\n",
            "step 2001/10000 | val_loss=0.2053\n",
            "step 2001/10000 | loss=0.200051 | lr=5.638269e-04 | norm=0.0698 | dt=7714.32ms\n",
            "step 2051/10000 | loss=0.198556 | lr=5.615867e-04 | norm=0.0731 | dt=45.66ms\n",
            "step 2101/10000 | loss=0.197995 | lr=5.592846e-04 | norm=0.0695 | dt=42.15ms\n",
            "step 2151/10000 | loss=0.198265 | lr=5.569211e-04 | norm=0.0648 | dt=45.71ms\n",
            "step 2201/10000 | val_loss=0.2030\n",
            "step 2201/10000 | loss=0.196808 | lr=5.544968e-04 | norm=0.0657 | dt=7402.29ms\n",
            "step 2251/10000 | loss=0.196588 | lr=5.520124e-04 | norm=0.0660 | dt=41.65ms\n",
            "step 2301/10000 | loss=0.195883 | lr=5.494686e-04 | norm=0.0645 | dt=41.47ms\n",
            "step 2351/10000 | loss=0.196865 | lr=5.468660e-04 | norm=0.0642 | dt=42.03ms\n",
            "step 2401/10000 | val_loss=0.2012\n",
            "step 2401/10000 | loss=0.195368 | lr=5.442054e-04 | norm=0.0635 | dt=7738.73ms\n",
            "step 2451/10000 | loss=0.194290 | lr=5.414874e-04 | norm=0.0643 | dt=42.44ms\n",
            "step 2501/10000 | loss=0.196007 | lr=5.387128e-04 | norm=0.0628 | dt=44.46ms\n",
            "step 2551/10000 | loss=0.193779 | lr=5.358823e-04 | norm=0.0603 | dt=45.04ms\n",
            "step 2601/10000 | val_loss=0.1998\n",
            "step 2601/10000 | loss=0.192409 | lr=5.329967e-04 | norm=0.0608 | dt=7734.68ms\n",
            "step 2651/10000 | loss=0.192250 | lr=5.300568e-04 | norm=0.0574 | dt=44.18ms\n",
            "step 2701/10000 | loss=0.192696 | lr=5.270633e-04 | norm=0.0578 | dt=43.84ms\n",
            "step 2751/10000 | loss=0.194696 | lr=5.240170e-04 | norm=0.0567 | dt=43.57ms\n",
            "step 2801/10000 | val_loss=0.1984\n",
            "step 2801/10000 | loss=0.191686 | lr=5.209188e-04 | norm=0.0581 | dt=7724.51ms\n",
            "step 2851/10000 | loss=0.191121 | lr=5.177695e-04 | norm=0.0543 | dt=41.83ms\n",
            "step 2901/10000 | loss=0.188054 | lr=5.145699e-04 | norm=0.0527 | dt=42.64ms\n",
            "step 2951/10000 | loss=0.191326 | lr=5.113209e-04 | norm=0.0561 | dt=44.62ms\n",
            "step 3001/10000 | val_loss=0.1970\n",
            "step 3001/10000 | loss=0.191806 | lr=5.080234e-04 | norm=0.0540 | dt=7782.37ms\n",
            "step 3051/10000 | loss=0.188254 | lr=5.046782e-04 | norm=0.0555 | dt=42.62ms\n",
            "step 3101/10000 | loss=0.188919 | lr=5.012862e-04 | norm=0.0523 | dt=43.60ms\n",
            "step 3151/10000 | loss=0.189681 | lr=4.978484e-04 | norm=0.0522 | dt=42.39ms\n",
            "step 3201/10000 | val_loss=0.1959\n",
            "step 3201/10000 | loss=0.187346 | lr=4.943656e-04 | norm=0.0524 | dt=7708.87ms\n",
            "step 3251/10000 | loss=0.188292 | lr=4.908388e-04 | norm=0.0484 | dt=43.93ms\n",
            "step 3301/10000 | loss=0.189182 | lr=4.872690e-04 | norm=0.0511 | dt=42.16ms\n",
            "step 3351/10000 | loss=0.190915 | lr=4.836570e-04 | norm=0.0505 | dt=42.35ms\n",
            "step 3401/10000 | val_loss=0.1948\n",
            "step 3401/10000 | loss=0.190269 | lr=4.800040e-04 | norm=0.0488 | dt=7742.10ms\n",
            "step 3451/10000 | loss=0.187290 | lr=4.763107e-04 | norm=0.0488 | dt=43.08ms\n",
            "step 3501/10000 | loss=0.188340 | lr=4.725783e-04 | norm=0.0496 | dt=41.95ms\n",
            "step 3551/10000 | loss=0.186512 | lr=4.688077e-04 | norm=0.0509 | dt=43.76ms\n",
            "step 3601/10000 | val_loss=0.1940\n",
            "step 3601/10000 | loss=0.184341 | lr=4.650000e-04 | norm=0.0483 | dt=7720.79ms\n",
            "step 3651/10000 | loss=0.188363 | lr=4.611561e-04 | norm=0.0478 | dt=42.08ms\n",
            "step 3701/10000 | loss=0.187729 | lr=4.572771e-04 | norm=0.0482 | dt=42.23ms\n",
            "step 3751/10000 | loss=0.185422 | lr=4.533640e-04 | norm=0.0464 | dt=45.51ms\n",
            "step 3801/10000 | val_loss=0.1931\n",
            "step 3801/10000 | loss=0.186772 | lr=4.494179e-04 | norm=0.0463 | dt=7670.49ms\n",
            "step 3851/10000 | loss=0.187151 | lr=4.454399e-04 | norm=0.0455 | dt=45.26ms\n",
            "step 3901/10000 | loss=0.185049 | lr=4.414309e-04 | norm=0.0456 | dt=43.26ms\n",
            "step 3951/10000 | loss=0.185686 | lr=4.373921e-04 | norm=0.0448 | dt=42.64ms\n",
            "step 4001/10000 | val_loss=0.1923\n",
            "step 4001/10000 | loss=0.186556 | lr=4.333245e-04 | norm=0.0452 | dt=7406.70ms\n",
            "step 4051/10000 | loss=0.186529 | lr=4.292293e-04 | norm=0.0457 | dt=42.73ms\n",
            "step 4101/10000 | loss=0.185689 | lr=4.251075e-04 | norm=0.0435 | dt=42.89ms\n",
            "step 4151/10000 | loss=0.185640 | lr=4.209603e-04 | norm=0.0443 | dt=42.58ms\n",
            "step 4201/10000 | val_loss=0.1916\n",
            "step 4201/10000 | loss=0.183651 | lr=4.167887e-04 | norm=0.0432 | dt=7715.25ms\n",
            "step 4251/10000 | loss=0.183812 | lr=4.125938e-04 | norm=0.0439 | dt=41.75ms\n",
            "step 4301/10000 | loss=0.185479 | lr=4.083769e-04 | norm=0.0442 | dt=46.12ms\n",
            "step 4351/10000 | loss=0.181586 | lr=4.041389e-04 | norm=0.0434 | dt=42.10ms\n",
            "step 4401/10000 | val_loss=0.1909\n",
            "step 4401/10000 | loss=0.184303 | lr=3.998811e-04 | norm=0.0428 | dt=7742.01ms\n",
            "step 4451/10000 | loss=0.184613 | lr=3.956046e-04 | norm=0.0431 | dt=41.98ms\n",
            "step 4501/10000 | loss=0.182504 | lr=3.913106e-04 | norm=0.0431 | dt=43.67ms\n",
            "step 4551/10000 | loss=0.181106 | lr=3.870001e-04 | norm=0.0428 | dt=41.98ms\n",
            "step 4601/10000 | val_loss=0.1903\n",
            "step 4601/10000 | loss=0.182529 | lr=3.826744e-04 | norm=0.0424 | dt=7741.89ms\n",
            "step 4651/10000 | loss=0.183539 | lr=3.783346e-04 | norm=0.0417 | dt=41.86ms\n",
            "step 4701/10000 | loss=0.182616 | lr=3.739818e-04 | norm=0.0429 | dt=45.71ms\n",
            "step 4751/10000 | loss=0.183242 | lr=3.696172e-04 | norm=0.0431 | dt=41.97ms\n",
            "step 4801/10000 | val_loss=0.1897\n",
            "step 4801/10000 | loss=0.182554 | lr=3.652421e-04 | norm=0.0417 | dt=7675.19ms\n",
            "step 4851/10000 | loss=0.181359 | lr=3.608575e-04 | norm=0.0410 | dt=42.42ms\n",
            "step 4901/10000 | loss=0.180798 | lr=3.564646e-04 | norm=0.0404 | dt=42.52ms\n",
            "step 4951/10000 | loss=0.181288 | lr=3.520647e-04 | norm=0.0409 | dt=41.65ms\n",
            "step 5001/10000 | val_loss=0.1892\n",
            "step 5001/10000 | loss=0.181465 | lr=3.476588e-04 | norm=0.0412 | dt=7432.84ms\n",
            "step 5051/10000 | loss=0.181575 | lr=3.432483e-04 | norm=0.0408 | dt=43.70ms\n",
            "step 5101/10000 | loss=0.181994 | lr=3.388342e-04 | norm=0.0412 | dt=42.47ms\n",
            "step 5151/10000 | loss=0.181982 | lr=3.344177e-04 | norm=0.0412 | dt=43.03ms\n",
            "step 5201/10000 | val_loss=0.1887\n",
            "step 5201/10000 | loss=0.181273 | lr=3.300000e-04 | norm=0.0409 | dt=7338.27ms\n",
            "step 5251/10000 | loss=0.181806 | lr=3.255823e-04 | norm=0.0415 | dt=46.39ms\n",
            "step 5301/10000 | loss=0.180033 | lr=3.211658e-04 | norm=0.0406 | dt=41.78ms\n",
            "step 5351/10000 | loss=0.180641 | lr=3.167517e-04 | norm=0.0405 | dt=42.86ms\n",
            "step 5401/10000 | val_loss=0.1882\n",
            "step 5401/10000 | loss=0.179861 | lr=3.123412e-04 | norm=0.0407 | dt=7723.13ms\n",
            "step 5451/10000 | loss=0.180458 | lr=3.079353e-04 | norm=0.0399 | dt=42.35ms\n",
            "step 5501/10000 | loss=0.180399 | lr=3.035354e-04 | norm=0.0392 | dt=42.07ms\n",
            "step 5551/10000 | loss=0.179941 | lr=2.991425e-04 | norm=0.0398 | dt=43.25ms\n",
            "step 5601/10000 | val_loss=0.1878\n",
            "step 5601/10000 | loss=0.179044 | lr=2.947579e-04 | norm=0.0403 | dt=7817.92ms\n",
            "step 5651/10000 | loss=0.181442 | lr=2.903828e-04 | norm=0.0397 | dt=41.28ms\n",
            "step 5701/10000 | loss=0.180064 | lr=2.860182e-04 | norm=0.0398 | dt=43.43ms\n",
            "step 5751/10000 | loss=0.180658 | lr=2.816654e-04 | norm=0.0393 | dt=42.97ms\n",
            "step 5801/10000 | val_loss=0.1874\n",
            "step 5801/10000 | loss=0.177224 | lr=2.773256e-04 | norm=0.0398 | dt=8057.44ms\n",
            "step 5851/10000 | loss=0.178225 | lr=2.729999e-04 | norm=0.0395 | dt=42.38ms\n",
            "step 5901/10000 | loss=0.181954 | lr=2.686894e-04 | norm=0.0400 | dt=43.31ms\n",
            "step 5951/10000 | loss=0.179914 | lr=2.643954e-04 | norm=0.0396 | dt=43.17ms\n",
            "step 6001/10000 | val_loss=0.1870\n",
            "step 6001/10000 | loss=0.179238 | lr=2.601189e-04 | norm=0.0398 | dt=7714.68ms\n",
            "step 6051/10000 | loss=0.179920 | lr=2.558611e-04 | norm=0.0394 | dt=41.87ms\n",
            "step 6101/10000 | loss=0.179668 | lr=2.516231e-04 | norm=0.0393 | dt=42.23ms\n",
            "step 6151/10000 | loss=0.178636 | lr=2.474062e-04 | norm=0.0394 | dt=42.34ms\n",
            "step 6201/10000 | val_loss=0.1866\n",
            "step 6201/10000 | loss=0.178997 | lr=2.432113e-04 | norm=0.0389 | dt=7721.09ms\n",
            "step 6251/10000 | loss=0.179627 | lr=2.390397e-04 | norm=0.0394 | dt=41.99ms\n",
            "step 6301/10000 | loss=0.181866 | lr=2.348925e-04 | norm=0.0395 | dt=41.85ms\n",
            "step 6351/10000 | loss=0.177454 | lr=2.307707e-04 | norm=0.0394 | dt=41.61ms\n",
            "step 6401/10000 | val_loss=0.1863\n",
            "step 6401/10000 | loss=0.178033 | lr=2.266755e-04 | norm=0.0389 | dt=7695.62ms\n",
            "step 6451/10000 | loss=0.177570 | lr=2.226079e-04 | norm=0.0389 | dt=43.35ms\n",
            "step 6501/10000 | loss=0.178029 | lr=2.185691e-04 | norm=0.0387 | dt=47.41ms\n",
            "step 6551/10000 | loss=0.178792 | lr=2.145601e-04 | norm=0.0389 | dt=41.83ms\n",
            "step 6601/10000 | val_loss=0.1859\n",
            "step 6601/10000 | loss=0.180103 | lr=2.105821e-04 | norm=0.0391 | dt=7651.15ms\n",
            "step 6651/10000 | loss=0.179426 | lr=2.066360e-04 | norm=0.0391 | dt=42.93ms\n",
            "step 6701/10000 | loss=0.181038 | lr=2.027229e-04 | norm=0.0390 | dt=41.79ms\n",
            "step 6751/10000 | loss=0.179045 | lr=1.988439e-04 | norm=0.0390 | dt=44.62ms\n",
            "step 6801/10000 | val_loss=0.1857\n",
            "step 6801/10000 | loss=0.176578 | lr=1.950000e-04 | norm=0.0385 | dt=7723.84ms\n",
            "step 6851/10000 | loss=0.177749 | lr=1.911923e-04 | norm=0.0390 | dt=43.80ms\n",
            "step 6901/10000 | loss=0.177063 | lr=1.874217e-04 | norm=0.0384 | dt=43.36ms\n",
            "step 6951/10000 | loss=0.179860 | lr=1.836893e-04 | norm=0.0387 | dt=45.28ms\n",
            "step 7001/10000 | val_loss=0.1854\n",
            "step 7001/10000 | loss=0.177450 | lr=1.799960e-04 | norm=0.0382 | dt=7665.56ms\n",
            "step 7051/10000 | loss=0.176014 | lr=1.763430e-04 | norm=0.0380 | dt=44.17ms\n",
            "step 7101/10000 | loss=0.175871 | lr=1.727310e-04 | norm=0.0381 | dt=41.97ms\n",
            "step 7151/10000 | loss=0.176410 | lr=1.691612e-04 | norm=0.0384 | dt=43.27ms\n",
            "step 7201/10000 | val_loss=0.1851\n",
            "step 7201/10000 | loss=0.177878 | lr=1.656344e-04 | norm=0.0383 | dt=8098.42ms\n",
            "step 7251/10000 | loss=0.177349 | lr=1.621516e-04 | norm=0.0384 | dt=42.33ms\n",
            "step 7301/10000 | loss=0.176427 | lr=1.587138e-04 | norm=0.0393 | dt=42.42ms\n",
            "step 7351/10000 | loss=0.177336 | lr=1.553218e-04 | norm=0.0386 | dt=42.20ms\n",
            "step 7401/10000 | val_loss=0.1848\n",
            "step 7401/10000 | loss=0.175567 | lr=1.519766e-04 | norm=0.0383 | dt=7743.42ms\n",
            "step 7451/10000 | loss=0.178252 | lr=1.486791e-04 | norm=0.0392 | dt=41.92ms\n",
            "step 7501/10000 | loss=0.176796 | lr=1.454301e-04 | norm=0.0381 | dt=42.88ms\n",
            "step 7551/10000 | loss=0.177028 | lr=1.422305e-04 | norm=0.0384 | dt=41.05ms\n",
            "step 7601/10000 | val_loss=0.1846\n",
            "step 7601/10000 | loss=0.177710 | lr=1.390812e-04 | norm=0.0387 | dt=7680.66ms\n",
            "step 7651/10000 | loss=0.176684 | lr=1.359830e-04 | norm=0.0382 | dt=42.20ms\n",
            "step 7701/10000 | loss=0.176874 | lr=1.329367e-04 | norm=0.0382 | dt=42.41ms\n",
            "step 7751/10000 | loss=0.177876 | lr=1.299432e-04 | norm=0.0385 | dt=47.02ms\n",
            "step 7801/10000 | val_loss=0.1846\n",
            "step 7801/10000 | loss=0.176869 | lr=1.270033e-04 | norm=0.0389 | dt=7438.46ms\n",
            "step 7851/10000 | loss=0.177361 | lr=1.241177e-04 | norm=0.0387 | dt=43.71ms\n",
            "step 7901/10000 | loss=0.175396 | lr=1.212872e-04 | norm=0.0384 | dt=42.83ms\n",
            "step 7951/10000 | loss=0.177609 | lr=1.185126e-04 | norm=0.0382 | dt=42.56ms\n",
            "step 8001/10000 | val_loss=0.1842\n",
            "step 8001/10000 | loss=0.175895 | lr=1.157946e-04 | norm=0.0382 | dt=7780.37ms\n",
            "step 8051/10000 | loss=0.176745 | lr=1.131340e-04 | norm=0.0387 | dt=42.86ms\n",
            "step 8101/10000 | loss=0.176549 | lr=1.105314e-04 | norm=0.0392 | dt=48.25ms\n",
            "step 8151/10000 | loss=0.176555 | lr=1.079876e-04 | norm=0.0382 | dt=41.37ms\n",
            "step 8201/10000 | val_loss=0.1841\n",
            "step 8201/10000 | loss=0.175811 | lr=1.055032e-04 | norm=0.0381 | dt=7644.59ms\n",
            "step 8251/10000 | loss=0.175822 | lr=1.030789e-04 | norm=0.0387 | dt=41.64ms\n",
            "step 8301/10000 | loss=0.175478 | lr=1.007154e-04 | norm=0.0382 | dt=41.95ms\n",
            "step 8351/10000 | loss=0.175634 | lr=9.841328e-05 | norm=0.0385 | dt=44.00ms\n",
            "step 8401/10000 | val_loss=0.1840\n",
            "step 8401/10000 | loss=0.175516 | lr=9.617314e-05 | norm=0.0381 | dt=7736.55ms\n",
            "step 8451/10000 | loss=0.177790 | lr=9.399561e-05 | norm=0.0389 | dt=44.83ms\n",
            "step 8501/10000 | loss=0.177140 | lr=9.188126e-05 | norm=0.0381 | dt=42.65ms\n",
            "step 8551/10000 | loss=0.177797 | lr=8.983066e-05 | norm=0.0388 | dt=43.26ms\n",
            "step 8601/10000 | val_loss=0.1838\n",
            "step 8601/10000 | loss=0.176113 | lr=8.784436e-05 | norm=0.0386 | dt=7376.29ms\n",
            "step 8651/10000 | loss=0.176212 | lr=8.592289e-05 | norm=0.0387 | dt=42.55ms\n",
            "step 8701/10000 | loss=0.177887 | lr=8.406677e-05 | norm=0.0388 | dt=45.84ms\n",
            "step 8751/10000 | loss=0.174114 | lr=8.227649e-05 | norm=0.0377 | dt=42.97ms\n",
            "step 8801/10000 | val_loss=0.1838\n",
            "step 8801/10000 | loss=0.175575 | lr=8.055253e-05 | norm=0.0387 | dt=7466.13ms\n",
            "step 8851/10000 | loss=0.175877 | lr=7.889535e-05 | norm=0.0391 | dt=41.73ms\n",
            "step 8901/10000 | loss=0.175188 | lr=7.730540e-05 | norm=0.0388 | dt=42.07ms\n",
            "step 8951/10000 | loss=0.176329 | lr=7.578310e-05 | norm=0.0385 | dt=42.42ms\n",
            "step 9001/10000 | val_loss=0.1836\n",
            "step 9001/10000 | loss=0.174983 | lr=7.432887e-05 | norm=0.0386 | dt=7719.17ms\n",
            "step 9051/10000 | loss=0.174881 | lr=7.294308e-05 | norm=0.0380 | dt=42.25ms\n",
            "step 9101/10000 | loss=0.174684 | lr=7.162611e-05 | norm=0.0381 | dt=43.11ms\n",
            "step 9151/10000 | loss=0.176242 | lr=7.037831e-05 | norm=0.0388 | dt=43.57ms\n",
            "step 9201/10000 | val_loss=0.1835\n",
            "step 9201/10000 | loss=0.175326 | lr=6.920003e-05 | norm=0.0382 | dt=7677.47ms\n",
            "step 9251/10000 | loss=0.175581 | lr=6.809156e-05 | norm=0.0389 | dt=44.03ms\n",
            "step 9301/10000 | loss=0.176946 | lr=6.705322e-05 | norm=0.0390 | dt=42.10ms\n",
            "step 9351/10000 | loss=0.173358 | lr=6.608527e-05 | norm=0.0389 | dt=42.47ms\n",
            "step 9401/10000 | val_loss=0.1834\n",
            "step 9401/10000 | loss=0.175461 | lr=6.518797e-05 | norm=0.0385 | dt=7747.23ms\n",
            "step 9451/10000 | loss=0.174353 | lr=6.436158e-05 | norm=0.0381 | dt=43.40ms\n",
            "step 9501/10000 | loss=0.177111 | lr=6.360630e-05 | norm=0.0386 | dt=42.19ms\n",
            "step 9551/10000 | loss=0.176687 | lr=6.292234e-05 | norm=0.0384 | dt=42.37ms\n",
            "step 9601/10000 | val_loss=0.1833\n",
            "step 9601/10000 | loss=0.176384 | lr=6.230989e-05 | norm=0.0392 | dt=7731.87ms\n",
            "step 9651/10000 | loss=0.175736 | lr=6.176910e-05 | norm=0.0385 | dt=42.82ms\n",
            "step 9701/10000 | loss=0.175896 | lr=6.130012e-05 | norm=0.0386 | dt=43.22ms\n",
            "step 9751/10000 | loss=0.175138 | lr=6.090309e-05 | norm=0.0386 | dt=42.08ms\n",
            "step 9801/10000 | val_loss=0.1833\n",
            "step 9801/10000 | loss=0.176143 | lr=6.057809e-05 | norm=0.0385 | dt=7699.48ms\n",
            "step 9851/10000 | loss=0.177201 | lr=6.032523e-05 | norm=0.0388 | dt=42.16ms\n",
            "step 9901/10000 | loss=0.175110 | lr=6.014456e-05 | norm=0.0388 | dt=42.82ms\n",
            "step 9951/10000 | loss=0.173982 | lr=6.003614e-05 | norm=0.0381 | dt=41.79ms\n",
            "step 10000/10000 | val_loss=0.1832\n",
            "step 10000/10000 | loss=0.174398 | lr=6.000001e-05 | norm=0.0384 | dt=7749.43ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.arange(len(train_losses)), train_losses)\n",
        "plt.scatter(np.arange(len(valid_losses))*val_loss_n_steps, valid_losses, color='red', zorder=3, s=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xwIpu32bdJFY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "b529e1ef-48f1-486b-a4f2-1e67475157a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the model"
      ],
      "metadata": {
        "id": "PG2PlHeuirzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {\n",
        "    'model': model._orig_mod.state_dict(),\n",
        "    'config': config\n",
        "}\n",
        "# torch.save(checkpoint, MODEL_PATH/f\"{MODEL_NAME}.pt\")"
      ],
      "metadata": {
        "id": "gR67fZbQivTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert False, \"stop here\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "6x64oFbB4IXZ",
        "outputId": "d7f79bc6-b474-4f85-bdee-9a61ffa89539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "stop here",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-f68b657fa1b2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stop here\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: stop here"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "# Validation"
      ],
      "metadata": {
        "id": "xpm06Cs0XcDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "r4F8C6wz-q0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inference_train_loader = DataLoader(\n",
        "#     train_dataset,\n",
        "#     batch_size=batch_size,\n",
        "#     shuffle=False,\n",
        "#     num_workers=8,\n",
        "#     pin_memory=True\n",
        "# )\n",
        "\n",
        "inference_train_loader = train_loader"
      ],
      "metadata": {
        "id": "vVQL_LHdqwZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions = []\n",
        "train_targets = []\n",
        "used_train_eras = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(inference_train_loader):\n",
        "        features, _, targets, _, eras, _ = batch\n",
        "        features = features.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=dtype):\n",
        "            t_predictions, _ = model(features)\n",
        "\n",
        "        predictions = t_predictions.squeeze(-1).float().cpu().numpy()\n",
        "        targets = targets.numpy()\n",
        "\n",
        "        train_predictions.append(predictions)\n",
        "        train_targets.append(targets)\n",
        "        used_train_eras.append(eras)\n",
        "\n",
        "train_predictions = np.concatenate(train_predictions, axis=0)\n",
        "train_targets = np.concatenate(train_targets, axis=0)\n",
        "used_train_eras = np.concatenate(used_train_eras, axis=0)"
      ],
      "metadata": {
        "id": "o-9aEZz6q3ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_preds = pd.DataFrame(\n",
        "    data = {\n",
        "        'era': [str(era).zfill(4) for era in used_train_eras],\n",
        "        'target': train_targets,\n",
        "        MODEL_NAME: train_predictions\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "Fy6bwxjatn4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_preds[MODEL_NAME].describe()"
      ],
      "metadata": {
        "id": "bfp1PiUft_iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style=\"darkgrid\", context=\"talk\", palette=\"rainbow\", font_scale=.7)\n",
        "sns.histplot(data=df_train_preds, x=MODEL_NAME, bins=200)\n",
        "\n",
        "# Customize the plot to match your original settings\n",
        "plt.title(\"Train predictions\")\n",
        "plt.xlabel('Observation')   # Remove x-axis label\n",
        "plt.ylabel('')  # Optional: Set y-axis label\n",
        "plt.legend([], [], frameon=False)  # Remove legend\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4FPlDwoUsNfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zox0obM3Kh41"
      },
      "outputs": [],
      "source": [
        "per_era_corr = df_train_preds.groupby(\"era\").apply(\n",
        "    lambda x: numerai_corr(x[[MODEL_NAME]].dropna(), x['target'].dropna()),\n",
        "    include_groups=False\n",
        ")\n",
        "\n",
        "sns.set(style=\"darkgrid\", context=\"talk\", palette=\"rainbow\", font_scale=.7)\n",
        "sns.barplot(data=per_era_corr, x='era', y=MODEL_NAME)\n",
        "\n",
        "# Customize the plot to match your original settings\n",
        "plt.title(\"Train CORR\")\n",
        "plt.xticks([])  # Remove x-tick labels\n",
        "plt.xlabel('era')   # Remove x-axis label\n",
        "plt.ylabel('')  # Optional: Set y-axis label\n",
        "plt.legend([], [], frameon=False)  # Remove legend\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style=\"darkgrid\", context=\"talk\", palette=\"rainbow\", font_scale=.7)\n",
        "sns.lineplot(data=per_era_corr.cumsum(), x='era', y=MODEL_NAME)\n",
        "\n",
        "# Customize the plot to match your original settings\n",
        "plt.title(\"Cumulative Train CORR\")\n",
        "plt.xticks([])  # Remove x-tick labels\n",
        "plt.xlabel('era')   # Remove x-axis label\n",
        "plt.ylabel('')  # Optional: Set y-axis label\n",
        "plt.legend([], [], frameon=False)  # Remove legend\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jwZlqthRh4DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "DwfIv089h6fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inference_valid_loader = DataLoader(\n",
        "#     valid_dataset,\n",
        "#     batch_size=batch_size,\n",
        "#     shuffle=False,\n",
        "#     num_workers=8,\n",
        "#     pin_memory=True\n",
        "# )\n",
        "\n",
        "inference_valid_loader = valid_loader"
      ],
      "metadata": {
        "id": "qb3kduWAo5l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_predictions = []\n",
        "valid_targets = []\n",
        "used_valid_eras = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(inference_valid_loader):\n",
        "        features, _, targets, _, eras, _ = batch\n",
        "        features = features.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=dtype):\n",
        "            t_predictions, _ = model(features)\n",
        "\n",
        "        predictions = t_predictions.squeeze(-1).float().cpu().numpy()\n",
        "        targets = targets.numpy()\n",
        "\n",
        "        valid_predictions.append(predictions)\n",
        "        valid_targets.append(targets)\n",
        "        used_valid_eras.append(eras)\n",
        "\n",
        "valid_predictions = np.concatenate(valid_predictions, axis=0)\n",
        "valid_targets = np.concatenate(valid_targets, axis=0)\n",
        "used_valid_eras = np.concatenate(used_valid_eras, axis=0)"
      ],
      "metadata": {
        "id": "R9-_LaW5pS2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_validation_preds = pd.DataFrame(\n",
        "    data = {\n",
        "        'era': [str(era).zfill(4) for era in used_valid_eras],\n",
        "        'target': valid_targets,\n",
        "        MODEL_NAME: valid_predictions\n",
        "    }\n",
        ").dropna()"
      ],
      "metadata": {
        "id": "JFxyx66VvSlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_validation_preds[MODEL_NAME].describe()"
      ],
      "metadata": {
        "id": "0aib3f5qJKSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9hlkxJyKh44"
      },
      "outputs": [],
      "source": [
        "# Compute the per-era corr between our predictions and the target values\n",
        "per_era_corr = df_validation_preds.groupby(\"era\").apply(\n",
        "    lambda x: numerai_corr(x[[MODEL_NAME]].dropna(), x['target'].dropna()),\n",
        "    include_groups=False\n",
        ")\n",
        "\n",
        "sns.set(style=\"darkgrid\", context=\"talk\", palette=\"rainbow\", font_scale=.7)\n",
        "sns.barplot(data=per_era_corr, x='era', y=MODEL_NAME)\n",
        "\n",
        "# Customize the plot to match your original settings\n",
        "plt.title(\"Validation CORR\")\n",
        "plt.xticks([])  # Remove x-tick labels\n",
        "plt.xlabel('era')   # Remove x-axis label\n",
        "plt.ylabel('')  # Optional: Set y-axis label\n",
        "plt.legend([], [], frameon=False)  # Remove legend\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4btBvX0bKh46"
      },
      "outputs": [],
      "source": [
        "sns.set(style=\"darkgrid\", context=\"talk\", palette=\"rainbow\", font_scale=.7)\n",
        "sns.lineplot(data=per_era_corr.cumsum(), x='era', y=MODEL_NAME)\n",
        "\n",
        "# Customize the plot to match your original settings\n",
        "plt.title(\"Cumulative Validation CORR\")\n",
        "plt.xticks([])  # Remove x-tick labels\n",
        "plt.xlabel('era')   # Remove x-axis label\n",
        "plt.ylabel('')  # Optional: Set y-axis label\n",
        "plt.legend([], [], frameon=False)  # Remove legend\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn-jbD9GKh5C"
      },
      "outputs": [],
      "source": [
        "# Compute performance metrics\n",
        "corr_mean = per_era_corr.mean()\n",
        "corr_std = per_era_corr.std(ddof=0)\n",
        "corr_sharpe = corr_mean / corr_std\n",
        "corr_max_drawdown = (per_era_corr.cumsum().expanding(min_periods=1).max() - per_era_corr.cumsum()).max()\n",
        "\n",
        "pd.DataFrame({\n",
        "    \"mean\": [corr_mean.values[0]],\n",
        "    \"std\": [corr_std.values[0]],\n",
        "    \"sharpe\": [corr_sharpe.values[0]],\n",
        "    \"max_drawdown\": [corr_max_drawdown.values[0]]\n",
        "}, index=[\"CORR\"]).T"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SyJ1WHeFel1z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}